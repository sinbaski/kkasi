\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{embrechts:kluppelberg:mikosch:1997}
\citation{resnick:2007}
\citation{resnick:1987}
\@writefile{toc}{\contentsline {section}{\tocsection {}{1}{Estimation of the largest eigenvalues: an overview in the iid case}}{1}{section.1}}
\newlabel{sec:motivation}{{1}{1}{Estimation of the largest eigenvalues: an overview in the iid case}{section.1}{}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{1.1}{The light-tailed case}}{1}{subsection.1.1}}
\citation{muirhead}
\citation{anderson:1963}
\citation{johnstone:2001}
\citation{anderson:1963}
\citation{elkaroui:2003}
\citation{davis:mikosch:pfaffel:2015}
\citation{davis:pfaffel:stelzer:2014}
\citation{heiny:mikosch:2015:iid}
\citation{bai:silverstein:2010}
\citation{geman}
\newlabel{eq:gamma}{{1.1}{2}{The light-tailed case}{equation.1.1}{}}
\newlabel{eq:MP}{{1.2}{2}{The light-tailed case}{equation.1.2}{}}
\newlabel{eq:order}{{1.3}{2}{The light-tailed case}{equation.1.3}{}}
\citation{johnstone:2001}
\citation{tracy:widom:2012}
\citation{johansson}
\citation{tao09b}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Sample density function of the largest eigenvalue compared with the Tracy--Widom density function. The data matrix $ {\mathbf  X}$ has dimension $200 \times 1000$. An ensemble of 2000 matrices is simulated.}}{3}{figure.1}}
\newlabel{fig:normal-3point-TW}{{1}{3}{Sample density function of the largest eigenvalue compared with the Tracy--Widom density function. The data matrix $ \X $ has dimension $200 \times 1000$. An ensemble of 2000 matrices is simulated}{figure.1}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Standard normal entries}}}{3}{figure.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Entry distribution: $\mathbb {P}(X=\sqrt {3}) = \mathbb {P}(X=-\sqrt {3} ) = 1/6$, $\mathbb {P}(X=0)=2/3$. Note $\mathbb {E}X = 0$, { $\mathbb {E}[ X^2] = 1$, $\mathbb {E}[X^3] = 0$ and $\mathbb {E}[ X^4] = 3$, i.e., the first 4 moments of $X$ match those of the standard normal distribution.}}}}{3}{figure.1}}
\newlabel{eq:geman}{{1.4}{3}{The light-tailed case}{equation.1.4}{}}
\newlabel{eq:tc}{{1.5}{3}{The light-tailed case}{equation.1.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Sample distribution function of the largest eigenvalue $\lambda _{(1)}$ compared to the Fr\'{e}chet distribution (solid line) with $\alpha =1.6$. The data matrices have dimension $200 \times 1000$ and iid entries with infinite fourth moment. The results are based on 2000 replicates.}}{4}{figure.2}}
\newlabel{fig:MyDist-Frechet}{{2}{4}{Sample distribution function of the largest eigenvalue $\la _{(1)}$ compared to the \Frechet distribution (solid line) with $\alpha =1.6$. The data matrices have dimension $200 \times 1000$ and iid entries with infinite fourth moment. The results are based on 2000 replicates}{figure.2}{}}
\newlabel{subsec:1.2}{{1.2}{4}{The heavy-tailed case}{subsection.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{1.2}{The heavy-tailed case}}{4}{subsection.1.2}}
\citation{embrechts:kluppelberg:mikosch:1997}
\citation{dehaan:ferreira:2006}
\citation{baisilv}
\citation{feller}
\citation{belinschi:dembo:guionnet:2009}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Tail indices of log-returns of 478 time series\ from the S\&P 500 index. The values $(\mathaccentV {hat}05E\alpha _L,\mathaccentV {hat}05E\alpha _U)$ of the lower and upper tail indices are provided by Hill's estimator. We also draw the line $\mathaccentV {hat}05E\alpha _U=\mathaccentV {hat}05E\alpha _L$.}}{5}{figure.3}}
\newlabel{fig:SP500_tail_indices}{{3}{5}{Tail indices of log-returns of 478 \ts \ from the S\&P 500 index. The values $(\hat \alpha _L,\hat \alpha _U)$ of the lower and upper tail indices are provided by Hill's estimator. We also draw the line $\hat \alpha _U=\hat \alpha _L$}{figure.3}{}}
\newlabel{eq:wdfr}{{1.6}{5}{The heavy-tailed case}{equation.1.6}{}}
\newlabel{eq:regvar}{{1.7}{5}{The heavy-tailed case}{equation.1.7}{}}
\citation{arous:guionnet:2008}
\citation{bingham:goldie:teugels:1987}
\citation{soshnikov:2004}
\citation{soshnikov:2006}
\citation{resnick:2007}
\citation{resnick:1987}
\citation{resnick:2007}
\citation{resnick:1987}
\newlabel{eq:nn}{{1.8}{6}{The heavy-tailed case}{equation.1.8}{}}
\newlabel{eq:Gamma}{{1.9}{6}{The heavy-tailed case}{equation.1.9}{}}
\citation{auffinger:arous:peche:2009}
\citation{davis:pfaffel:stelzer:2014}
\citation{lam:yao}
\citation{heiny:mikosch:2015:iid}
\citation{soshnikov:2004}
\citation{soshnikov:2006}
\citation{auffinger:arous:peche:2009}
\citation{davis:mikosch:pfaffel:2015}
\citation{davis:pfaffel:stelzer:2014}
\citation{heiny:mikosch:2015:iid}
\citation{auffinger:arous:peche:2009}
\citation{davis:mikosch:pfaffel:2015}
\citation{davis:pfaffel:stelzer:2014}
\newlabel{eq:nna}{{1.10}{7}{The heavy-tailed case}{equation.1.10}{}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{1.3}{Overview}}{7}{subsection.1.3}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{2}{General growth rates for $p_n$ in the iid heavy-tailed case}}{7}{section.2}}
\newlabel{sec:2}{{2}{7}{General growth rates for $p_n$ in the iid heavy-tailed case}{section.2}{}}
\newlabel{subsec:pn}{{2}{7}{Growth conditions on $(p_n)$}{section*.1}{}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{}{Growth conditions on $(p_n)$}}{7}{section*.1}}
\newlabel{eq:p}{{{$C_p(\beta )$}}{7}{Growth conditions on $(p_n)$}{AMS.2}{}}
\citation{resnick:2007}
\citation{embrechts:veraverbeke:1982}
\citation{bhatia:1997}
\newlabel{thm:intro}{{2.1}{8}{}{lemma.2.1}{}}
\newlabel{Cbeta}{{{$\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle C$}\mathaccent "0365{C}_\beta (\alpha )$}}{8}{}{AMS.3}{}}
\newlabel{eq:2}{{2.1}{8}{}{equation.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\tocsubsubsection {}{}{A discussion of the case $\beta \in [0,1]$}}{8}{section*.4}}
\newlabel{prop:offdiagonal}{{2.2}{8}{}{lemma.2.2}{}}
\newlabel{eq:weyl}{{2.2}{8}{A discussion of the case $\beta \in [0,1]$}{equation.2.2}{}}
\citation{davis:mikosch:pfaffel:2015}
\citation{bingham:goldie:teugels:1987}
\citation{bhatia:1997}
\newlabel{cor:687}{{2.3}{9}{}{lemma.2.3}{}}
\newlabel{lem:pp}{{2.4}{9}{}{lemma.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\tocsubsubsection {}{}{Extension to general $\beta $}}{9}{section*.5}}
\newlabel{prop:offdiagonal1}{{2.5}{9}{}{lemma.2.5}{}}
\newlabel{cor:6871}{{2.6}{9}{}{lemma.2.6}{}}
\citation{davis:mikosch:pfaffel:2015}
\citation{heiny:mikosch:2015:iid}
\citation{heiny:mikosch:2016:noniid}
\citation{embrechts:kluppelberg:mikosch:1997}
\citation{davis:pfaffel:stelzer:2014}
\citation{davis:mikosch:pfaffel:2015}
\citation{lam:yao}
\@writefile{toc}{\contentsline {section}{\tocsection {}{3}{Introducing dependence between the rows and columns}}{10}{section.3}}
\newlabel{sec:model}{{3}{10}{Introducing dependence between the rows and columns}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{3.1}{The model}}{10}{subsection.3.1}}
\newlabel{eq:1}{{3.1}{10}{The model}{equation.3.1}{}}
\newlabel{eq:27}{{3.2}{10}{The model}{equation.3.2}{}}
\newlabel{eq:2a}{{3.3}{10}{The model}{equation.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{3.2}{Sample covariance and autocovariance matrices}}{10}{subsection.3.2}}
\newlabel{eq:26}{{3.4}{10}{Sample covariance and autocovariance matrices}{equation.3.4}{}}
\newlabel{eq:sample}{{3.5}{10}{Sample covariance and autocovariance matrices}{equation.3.5}{}}
\newlabel{eq:squares}{{3.6}{10}{Sample covariance and autocovariance matrices}{equation.3.6}{}}
\citation{belitski}
\citation{bhatia:1997}
\citation{horn}
\citation{shores}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{3.3}{More notation}}{11}{subsection.3.3}}
\newlabel{eq:zorder}{{3.7}{11}{More notation}{equation.3.7}{}}
\newlabel{eq:ll}{{3.8}{11}{More notation}{equation.3.8}{}}
\newlabel{eq:help6}{{3.9}{11}{More notation}{equation.3.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\tocsubsubsection {}{}{Matrix norms}}{11}{section*.6}}
\newlabel{specnorm}{{3.10}{11}{Matrix norms}{equation.3.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\tocsubsubsection {}{}{Singular values of the sample autocovariance matrices}}{11}{section*.7}}
\newlabel{eq:sigma}{{3.11}{11}{Singular values of the sample autocovariance matrices}{equation.3.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{\tocsubsubsection {}{}{The matrix ${\mathbf  M}(s)$}}{12}{section*.8}}
\newlabel{eq:m}{{3.12}{12}{The matrix $\M (s)$}{equation.3.12}{}}
\newlabel{eq:v1}{{3.13}{12}{The matrix $\M (s)$}{equation.3.13}{}}
\newlabel{eq:tracea}{{3.14}{12}{The matrix $\M (s)$}{equation.3.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{\tocsubsubsection {}{}{Normalizing sequence}}{12}{section*.9}}
\newlabel{subsec:defdelta}{{3.3}{12}{Approximations to singular values}{section*.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\tocsubsubsection {}{}{Approximations to singular values}}{12}{section*.10}}
\newlabel{sec:mainresult}{{3.4}{12}{Approximation of the singular values}{subsection.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{3.4}{Approximation of the singular values}}{12}{subsection.3.4}}
\newlabel{thm:mains}{{3.1}{12}{}{lemma.3.1}{}}
\citation{heiny:mikosch:2016:noniid}
\citation{davis:mikosch:pfaffel:2015}
\citation{davis:mikosch:pfaffel:2015}
\citation{davis:mikosch:pfaffel:2015}
\citation{resnick:1987}
\newlabel{eq:mains1}{{3.15}{13}{}{equation.3.15}{}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{3.5}{Point process convergence}}{13}{subsection.3.5}}
\newlabel{eq:ppdef}{{3.16}{13}{Point process \con }{equation.3.16}{}}
\newlabel{cor:1}{{3.4}{13}{}{lemma.3.4}{}}
\newlabel{eq:pp}{{3.17}{13}{}{equation.3.17}{}}
\newlabel{eq:wwa}{{3.18}{13}{Point process \con }{equation.3.18}{}}
\newlabel{eq:ppc}{{3.19}{13}{Point process \con }{equation.3.19}{}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{4}{Some applications}}{14}{section.4}}
\newlabel{sec:samplecov}{{4.1}{14}{Sample covariance matrices}{subsection.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{4.1}{Sample covariance matrices}}{14}{subsection.4.1}}
\newlabel{ex:xiao}{{4.1}{14}{}{lemma.4.1}{}}
\newlabel{eq:distrsim}{{4.1}{14}{}{equation.4.1}{}}
\citation{resnick:2007}
\citation{davis:mikosch:pfaffel:2015}
\citation{davis:mikosch:pfaffel:2015}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Density of the approximation errors for the eigenvalues of $a_{np}^{-2}{\mathbf  X}{\mathbf  X}'$. The entries of ${\mathbf  X}$ are iid with density \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:distrsim}\unskip \@@italiccorr )}} and $\alpha =1.6$.}}{15}{figure.4}}
\newlabel{fig:lambda_comparison}{{4}{15}{Density of the approximation errors for the eigenvalues of $a_{np}^{-2}\X \X '$. The entries of $\X $ are iid with density \eqref {eq:distrsim} and $\alpha =1.6$}{figure.4}{}}
\newlabel{eq:u}{{4.2}{15}{Sample covariance matrices}{equation.4.2}{}}
\newlabel{cor:1q}{{4.2}{15}{}{lemma.4.2}{}}
\newlabel{eq:limit}{{4.3}{15}{}{equation.4.3}{}}
\newlabel{rem:4.5}{{4.3}{15}{}{lemma.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Distribution function of $(\lambda _{(1)} - \lambda _{(2)})/\lambda _{(1)}$ for iid data (left) and data generated from the model \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {ex:4.6}\unskip \@@italiccorr )}} (right). In each graph we compare the empirical distribution\ function\ (dotted line, based on 1000 simulations of $200 \times 1000$ matrices with $Z$-distribution\ \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:distrsim}\unskip \@@italiccorr )}}) with the theoretical curve (solid line).}}{16}{figure.5}}
\newlabel{fig:ProbMass}{{5}{16}{Distribution function of $(\lambda _{(1)} - \lambda _{(2)})/\lambda _{(1)}$ for iid data (left) and data generated from the model \eqref {ex:4.6} (right). In each graph we compare the empirical \ds \ \fct \(dotted line, based on 1000 simulations of $200 \times 1000$ matrices with $Z$-\ds \ \eqref {eq:distrsim}) with the theoretical curve (solid line)}{figure.5}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {iid data}}}{16}{figure.5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {data from model \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {ex:4.6}\unskip \@@italiccorr )}}}}}{16}{figure.5}}
\newlabel{ex:4.6}{{4.4}{16}{}{equation.4.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Histogram based on 1000 replications of $\left (\lambda _{(2)}/\lambda _{(1)}\right )^{2/\alpha }$ from model \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {ex:4.6}\unskip \@@italiccorr )}}.}}{17}{figure.6}}
\newlabel{fig:1}{{6}{17}{Histogram based on 1000 replications of $\left (\lambda _{(2)}/\lambda _{(1)}\right )^{2/\alpha }$ from model \eqref {ex:4.6}}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Graph of $G(x)=\mathbb  {P}(\Gamma _1/\Gamma _2\le 2^{-\alpha }|\Gamma _1<(x/8)^{-\alpha /2})$ when $\alpha =1.5$.}}{18}{figure.7}}
\newlabel{fig:2}{{7}{18}{Graph of $G(x)=\P (\Gamma _1/\Gamma _2\le 2^{-\alpha }|\Gamma _1<(x/8)^{-\alpha /2})$ when $\alpha =1.5$}{figure.7}{}}
\newlabel{exam:spectralgap}{{4.5}{18}{}{lemma.4.5}{}}
\newlabel{exam:separable}{{4.6}{18}{}{lemma.4.6}{}}
\newlabel{sec:sp500}{{4.2}{19}{S\&P 500 data}{subsection.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{4.2}{S\&P 500 data}}{19}{subsection.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The logarithms of the ratios $\lambda _{(i+1)} / \lambda _{(i)}$ for the S\&P 500 series after rank transform. We also show the 1, 50 and 99\% quantiles (bottom, middle, top lines, respectively) of the variables $\qopname  \relax o{log}((\Gamma _i/ \Gamma _{i+1})^{2})$. }}{19}{figure.8}}
\newlabel{fig:EigenRatio}{{8}{19}{The logarithms of the ratios $\lambda _{(i+1)} / \lambda _{(i)}$ for the S\&P 500 series after rank transform. We also show the 1, 50 and 99\% quantiles (bottom, middle, top lines, respectively) of the variables $\log ((\Gamma _i/ \Gamma _{i+1})^{2})$}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The logarithms of the ratios $\lambda _{(i+1)} / \lambda _{(i)}$ for the original (non-rank transformed) S\&P 500 log-return data. We also show the 1, 50 and 99\% quantiles (bottom, middle, top lines, respectively) of the variables $\qopname  \relax o{log}((\Gamma _i/ \Gamma _{i+1})^{2/2.3})$; see also Figure\nonbreakingspace \ref  {fig:EigenRatio} for comparison.}}{20}{figure.9}}
\newlabel{fig:22}{{9}{20}{The logarithms of the ratios $\lambda _{(i+1)} / \lambda _{(i)}$ for the original (non-rank transformed) S\&P 500 log-return data. We also show the 1, 50 and 99\% quantiles (bottom, middle, top lines, respectively) of the variables $\log ((\Gamma _i/ \Gamma _{i+1})^{2/2.3})$; see also Figure~\ref {fig:EigenRatio} for comparison}{figure.9}{}}
\newlabel{eq:iid}{{4.5}{20}{S\&P 500 data}{equation.4.5}{}}
\newlabel{sec:possemidef}{{4.3}{20}{Sums of squares of sample autocovariance matrices}{subsection.4.3}{}}
\citation{lam:yao}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{4.3}{Sums of squares of sample autocovariance matrices}}{21}{subsection.4.3}}
\newlabel{thm:mainstr}{{4.7}{21}{}{lemma.4.7}{}}
\newlabel{eq:gdfg}{{4.6}{21}{Sums of squares of sample autocovariance matrices}{equation.4.6}{}}
\newlabel{eq:sumA}{{4.7}{22}{Sums of squares of sample autocovariance matrices}{equation.4.7}{}}
\newlabel{prop:sumsmal}{{4.8}{22}{}{lemma.4.8}{}}
\newlabel{exam:additive}{{4.9}{22}{}{lemma.4.9}{}}
\newlabel{fig:LamYao:a}{{10(a)}{22}{Subfigure 10(a)}{subfigure.10.1}{}}
\newlabel{sub@fig:LamYao:a}{{(a)}{22}{Subfigure 10(a)\relax }{subfigure.10.1}{}}
\newlabel{fig:LamYao:b}{{10(b)}{22}{Subfigure 10(b)}{subfigure.10.2}{}}
\newlabel{sub@fig:LamYao:b}{{(b)}{22}{Subfigure 10(b)\relax }{subfigure.10.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The largest eigenvalues of the sums of the squared autocovariance matrices compared with the sums of the largest eigenvalues of these matrices for the S\&P 500 data for different values $s_1$. The two values are surprisingly close to each other; mind the scale of the $y$-axis. We also show their ratios.}}{22}{figure.10}}
\newlabel{fig:LamYao}{{10}{22}{The largest eigenvalues of the sums of the squared autocovariance matrices compared with the sums of the largest eigenvalues of these matrices for the S\&P 500 data for different values $s_1$. The two values are surprisingly close to each other; mind the scale of the $y$-axis. We also show their ratios}{figure.10}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{22}{figure.10}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{22}{figure.10}}
\citation{embrechts:goldie:1980}
\newlabel{eq:drtgdfg}{{4.8}{23}{}{equation.4.8}{}}
\@writefile{toc}{\contentsline {section}{\tocsection {Appendix}{A}{Auxiliary results}}{23}{appendix.A}}
\newlabel{appendix:A}{{A}{23}{Auxiliary results}{appendix.A}{}}
\citation{nagaev:1979}
\citation{cline:hsing:1998}
\citation{denisov:dieker:shneer:2008}
\citation{resnick:1987}
\citation{resnick:2007}
\citation{resnick:1987}
\bibcite{anderson:1963}{1}
\bibcite{auffinger:arous:peche:2009}{2}
\bibcite{bai:silverstein:2010}{3}
\bibcite{baisilv}{4}
\bibcite{belinschi:dembo:guionnet:2009}{5}
\bibcite{belitski}{6}
\bibcite{arous:guionnet:2008}{7}
\bibcite{bhatia:1997}{8}
\bibcite{bingham:goldie:teugels:1987}{9}
\bibcite{cline:hsing:1998}{10}
\bibcite{davis:mikosch:pfaffel:2015}{11}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{A.1}{Large deviation results}}{24}{subsection.A.1}}
\newlabel{thm:nagaev}{{A.1}{24}{}{lemma.A.1}{}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{A.2}{A point process convergence result}}{24}{subsection.A.2}}
\newlabel{lem:ppr}{{A.2}{24}{}{lemma.A.2}{}}
\@writefile{toc}{\contentsline {section}{\tocsection {Appendix}{}{Acknowledgments}}{24}{section*.11}}
\@writefile{toc}{\contentsline {section}{\tocsection {Appendix}{}{References}}{24}{section*.12}}
\bibcite{davis:pfaffel:stelzer:2014}{12}
\bibcite{dehaan:ferreira:2006}{13}
\bibcite{denisov:dieker:shneer:2008}{14}
\bibcite{elkaroui:2003}{15}
\bibcite{embrechts:goldie:1980}{16}
\bibcite{embrechts:kluppelberg:mikosch:1997}{17}
\bibcite{embrechts:veraverbeke:1982}{18}
\bibcite{feller}{19}
\bibcite{geman}{20}
\bibcite{heiny:mikosch:2015:iid}{21}
\bibcite{heiny:mikosch:2016:noniid}{22}
\bibcite{horn}{23}
\bibcite{johansson}{24}
\bibcite{johnstone:2001}{25}
\bibcite{lam:yao}{26}
\bibcite{muirhead}{27}
\bibcite{nagaev:1979}{28}
\bibcite{resnick:2007}{29}
\bibcite{resnick:1987}{30}
\bibcite{shores}{31}
\bibcite{soshnikov:2004}{32}
\bibcite{soshnikov:2006}{33}
\bibcite{tao09b}{34}
\bibcite{tracy:widom:2012}{35}
\newlabel{tocindent-1}{0pt}
\newlabel{tocindent0}{64.17928pt}
\newlabel{tocindent1}{72.39178pt}
\newlabel{tocindent2}{35.40477pt}
\newlabel{tocindent3}{0pt}
