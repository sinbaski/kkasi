\documentclass{article}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage[bookmarks=true]{hyperref}
\usepackage{bookmark}
\usepackage{graphicx}

\newif\ifALL

\ALLtrue

\input{../physics_common}
\title{Eigenvalue Estimation of fixed-dimensional sample covariance
  matrices}
\author{Xie Xiaolei}
\date{\today}
\begin{document}
\maketitle

\ifALL
\section{iid sequences}\label{sec:iid}
Consider iid sequences $\{X_{it}\}$, $1 \leq i \leq p$, $1 \leq t \leq
n$. $p$ is fixed while $n \to \infty$. $\P(X_{it} > x) = p_+ L(x)
x^{-\alpha}$, $\P(X_{it} < -x) = p_- L(x) x^{-\alpha}$, $\alpha \in
(0,4)$ for a slowly varying function $L(x)$, and $p_+ + p_- = 1$. The
normalizing sequence $a_n$ is such that $\P(|X_{it}| > a_n) \sim {1
  \over n}$, implying $a_n \sim n^{1/\alpha} l(n)$, where $l(n)$ is a
slowing varying function. Moreover, we assume $\E X_{11} = 0$ if $\E
|X_{11}| < \infty$. The sample covariance matrix is $\mtx
{XX'}$.

For convenience, we let $c$ stand for any constant whose value is not
of importance in the rest of this report. We shall prove
\[
a_n^{-2}\|\mtx {XX'} - \diag(\mtx {XX'})\|_2 \xrightarrow{P} 0
\]

\begin{proof}
  Using the inequality
  \[
  \|\mtx A\|_2 \leq \|\mtx A\|_\infty
  \]
  for an arbitrary matrix $\mtx A$, we have
  \begin{align*}
    \P(\|\mtx {XX'} - \diag(\mtx {XX'})\|_2 \geq a_n^2 \epsilon) & \leq
    \P\left(\max_{1\leq i \leq p} \sum_{j=1, j\neq i}^p |\sum_{t=1}^n
    X_{it} X_{jt}| > a_n^2 \epsilon\right) \\
    &\leq \sum_{i=1}^p \sum_{j=1, j\neq i}^p \P\left(|\sum_{t=1}^n
    X_{1t} X_{2t}| > {a_n^2 \epsilon \over p-1}\right) \\
  \end{align*}
  Thus the large deviation result found in Cline and Hsing
  \cite{ClingHsing1998} and Nagaev \cite{nagaev1979} is
  applicable. With it we obtain
  \begin{align*}
    \P\left(|\sum_{t=1}^n X_{1t} X_{2t}| > {a_n^2 \epsilon \over p-1}\right)
    &\sim
    c n \P\left(|X_{11} X_{21}| > {a_n^2 \epsilon \over p-1 }\right)
  \end{align*}
  Now that $|X_{11}|$ and $|X_{21}|$ are iid and regularly varying
  with index $\alpha$, so is $|X_{11} X_{21}|$, according to Jessen
  and Mikosch \cite{JessenMikosch2006}. Thus we have
  \begin{eqnarray*}
    c n \P\left(|X_{11} X_{21}| > {a_n^2 \epsilon \over p-1 }\right)
    &\sim& c n L_{12}(n) (n^{2/\alpha})^{-\alpha} \to 0\text{,  }n\to\infty
  \end{eqnarray*}
  where $L_{12}$ is a slowly varying function.
\end{proof}
Now, using the result just proved and applying Weyl's perturbation
theorem we obtain
\[
a_n^{-2} \max_{i=1,\dots,p} |\lambda^C_{(i)} -
\lambda^{\text{diag}}_{(i)}| \leq  a_n^{-2}\|C - \diag{C}\| \to 0
\]
Therefore the eigenvalues of $XX'$ can be approximated by those of
$\diag(XX')$:
\begin{eqnarray*}
a_n^{-2}\lambda^{\text{diag}}_{i}  &=& a_n^{-2} \sum_{t=1}^n X_{it}^2
\end{eqnarray*}

\subsection{Upper Order Statistics of the eigenvalues}
When $\alpha \in (0,2)$, $\E X^2 = \infty$. Now that
$\P(X_{11}^2 > a_n^2) \sim 1/n$, the central limit theorem 
gives, according to Mikosch et al\cite{Embrechts1997} 
\[
a_n^{-2} \sum_{t=1}^n X_{it}^2 \xrightarrow{d} \xi_i \sim S_{\alpha/2}
\]
where $S_{\alpha/2}$ denotes an $\alpha/2$-stable distribution.
Since $(\xi_i)_{i=1}^p$ are iid, the joint density function of the $k$
upper order statistics of $\lambda_i$, the eigenvalues of $a_n^{-2} X X'$
follows as
\begin{eqnarray*}
  f_{\lambda_{(1)}, \dots, \lambda_{(k)}}(x_1, \dots, x_k) &=&
  p(p-1)\cdots(p-k+1) S_{\alpha/2}^{p-k}(x_k) \prod_{i=1}^k
  f_{\alpha/2}(x_i)
\end{eqnarray*}
where $f_{\alpha/2}(x) = {d S_{\alpha/2}(x) \over dx}$.

When $\alpha \in (2,4)$ the central limit theorem gives
\[
a_n^{-2} (\sum_{t=1}^n X_{it}^2 - n \E X^2)\xrightarrow{d}
\xi_i \sim S_{\alpha/2}
\]
Since $a_n \sim n^{1/\alpha} l(n)$, it is clear
\[
a_n^{-2} \sum_{t=1}^n X_{it}^2 \xrightarrow{a.s}\infty
\]

\section{Correlated Sequences}
\subsection{MA(1) process}
First consider the model
\[
X_{it} = Z_{it} + \theta Z_{i,t-1}
\]
We have
\begin{eqnarray*}
  a_n^{-2}(XX')_{ij} &=& a_n^{-2} \sum_{t=1}^n (Z_{it} + \theta
  Z_{i,t-1}) (Z_{jt} + \theta Z_{j,t-1}) \\
  &=& a_n^{-2} \sum_{t=1}^n Z_{it} Z_{jt} + a_n^{-2} \theta \sum_{t=1}^n
  Z_{it} Z_{j,t-1} \\
  && +\theta a_n^{-2} \sum_{t=1}^n Z_{i, t-1}
  Z_{j,t} + \theta^2 a_n^{-2} \sum_{t=1}^n Z_{i,t-1} Z_{j,t-1}
\end{eqnarray*}
Consider $\P(a_n^{-2} \sum_{t=1}^n Z_{1t} Z_{2t} > \epsilon)$,
$\forall \epsilon>0$. Since $Z_1 Z_2 \in \mathcal R_{-\alpha}$ (See
Mikosch and Jessen \cite{JessenMikosch2006}) and $\E Z_1 Z_2 = 0$ if
$\E|Z_1Z_2| < \infty$. By the large deviation result of Cline and
Hsing \cite{ClingHsing1998} and Nagaev \cite{nagaev1979}
\begin{eqnarray*}
  && \P(a_n^{-2} \sum_{t=1}^n Z_{1t} Z_{2t} > \epsilon)  \\
  &\sim& c n \P(|Z_1 Z_2| > a_n^{2} \epsilon)  \\
\end{eqnarray*}
Because $a_n \sim n^{1/\alpha} l(n)$, we have
\begin{eqnarray*}
  \P(|Z_1 Z_2| > a_n^{2} \epsilon) &\sim& n^{-2} L_{12}(n)
\end{eqnarray*}
where $L_{12}(n)$ is a slowly varying function. Thus
\[
\P(a_n^{-2} \sum_{t=1}^n Z_{1t} Z_{2t} > \epsilon) \to 0
\]

On the other hand $Z^2 \in \mathcal R_{-\alpha/2}$, hence
\begin{eqnarray*}
  \P(a_n^{-2} \sum_{t=1}^n Z_{1t}^2 > \epsilon) &\sim& c n
  \P(Z_{1t}^2 > a_n^2 \epsilon) \\
  &\sim& c n \P(|Z_{1t}| > a_n) \\
  &\sim& c L(n)
\end{eqnarray*}
Therefore
\begin{eqnarray*}
  \P(a_n^{-2} \sum_{t=1}^n Z_{it} Z_{jt}\I{i \neq j} > \epsilon) \to
  0 \\
  \P(a_n^{-2} \sum_{t=1}^n Z_{i,t-1} Z_{j,t-1} \I{i \neq j} >
  \epsilon) \to 0 \\
  \P(a_n^{-2} \sum_{t=1}^n Z_{it} Z_{j,t-1} > \epsilon) \to 0
  \\
  \P(a_n^{-2} \sum_{t=1}^n Z_{i, t-1} Z_{j,t} > \epsilon) \to 0
\end{eqnarray*}
Then it follows, for $i \neq j$, i.e. non-diagonal entries,
$\P(a_n^{-2} (XX')_{ij} > \epsilon) \to 0$; and for diagonal entries
\begin{eqnarray*}
  && \P\left[(XX')_{ii} - (1 + \theta^2)\sum_{t=1}^n
      Z_{it}^2 > a_n^{2} \epsilon \right] \\
  &=& \P\left[\theta^2 ( Z_{0}^2-Z_{n}^2) > a_n^{2} \epsilon
  \right] \to 0\\
\end{eqnarray*}
So for the eigenvalues of $a_n^{-2}XX'$, $\lambda_{(1)} > \lambda_{(2)} >
\dots > \lambda_{(p)}$, we have
\[
(\lambda_{(1)}, \lambda_{(2)}, \dots, \lambda_{(p)}) \xrightarrow{d}
(1+\theta^2)(\sum_{t=1}^n Z_{1t}^2, \sum_{t=1}^n Z_{2t}^2, \dots,
\sum_{t=1}^n Z_{pt}^2)
\]
As argued in the iid case in \S\ref{sec:iid}, if $\alpha \in (0,2)$,
$\sum_{t=1}^n Z_{it}^2 \xrightarrow{d} \xi_{i}$, where $\xi_{i}$ is an
$\alpha/2$-stable rv; if $\alpha \in (2,4)$, $\sum_{t=1}^n Z_{it}^2
\xrightarrow{a.s.} \infty$.

\subsection{Cross-correlation limited to order 1}\label{sec:B1}
Now we consider the model
\[
X_{it} = Z_{it} + \varphi Z_{i-1,t}
\]
\begin{eqnarray*}
  (XX')_{ij} &=& \sum_{t=1}^n (Z_{it} + \varphi Z_{i-1,t})(Z_{jt} +
  \varphi Z_{j-1,t}) \\
  &=& \sum_{t=1}^n (Z_{it}^2 + \varphi^2 Z_{i-1,t}^2)\I{i=j}
  + \sum_{t=1}^n (Z_{it}Z_{jt}+\varphi^2 Z_{i-1,t}Z_{j-1,t})\I{i\neq j}
  \\
  && + \varphi \sum_{t=1}^n Z_{it}^2 \I{i=j-1} 
  +\varphi\sum_{t=1}^n Z_{it} Z_{j-1,t}\I{i\neq j-1} \\
  && + \varphi \sum_{t=1}^n Z_{i-1,t}^2 \I{i=j+1} 
  +\varphi\sum_{t=1}^n Z_{jt} Z_{i-1,t}\I{i\neq j+1} \\
\end{eqnarray*}
As shown in previous sections, $\P(a_n^{-2} \sum_{t=1}^n Z_{1t}Z_{2t} >
\epsilon) \to 0$, as $n \to \infty$. Therefore
\[
\P\left(
  (XX')_{ij} - A_{ij}> a_n^2 \epsilon \right) \to 0\text{,  }n \to 0
\]
where
\[
A_{ij} = \sum_{t=1}^n (Z_{it}^2 + \varphi^2 Z_{i-1,t}^2)\I{i=j}
      + \varphi \sum_{t=1}^n Z_{it}^2 \I{i=j-1} 
      + \varphi \sum_{t=1}^n Z_{i-1,t}^2 \I{i=j+1}
\]
If $\alpha \in (0,2)$, $a_n^{-2}\sum_{t=1}^n Z_{it}^2 \xrightarrow{d}
\xi_i \sim S_{\alpha/2}$. So we have in this case
\[
A =
\begin{pmatrix}
  \xi_1 + \varphi^2 \xi_0 & \varphi \xi_1 & 0 & \cdots & 0 \\
  \varphi \xi_1 & \xi_2 + \varphi^2 \xi_1 & \varphi \xi_2 & \cdots & 0 \\
  0 & \varphi \xi_2 & \xi_3 + \varphi^2 \xi_2 & \cdots & 0 \\
  \vdots & \vdots & \vdots & \ddots & \vdots \\
  0 & 0 & 0 & \cdots & \xi_p + \varphi^2 \xi_{p-1}
\end{pmatrix}
\]

\subsection{cross- \& auto-correlation limited to order 1}
Here we consider the mode
\[
X_{it} = Z_{it} + \theta Z_{i, t-1} + \varphi Z_{i-1, t}
\]
The calculations are similar to those in \S\ref{sec:B1}. For clearity
we write $A_{ij} \sim B_{ij}$ if $\P(|A_{ij} - B_{ij}| > a_n^2 \epsilon)
\to 0$, $\forall \epsilon > 0$ and $n \to \infty$. Then we have
\begin{eqnarray*}
  (XX')_{ij} &\sim& \left[
    \sum_{t=1}^n Z_{it}^2 + \theta^2 \sum_{t=1}^n Z_{i,t-1}^2 +
    \varphi^2 \sum_{t=1}^n Z_{i-1,t}^2 
  \right]\I{i=j} \\
  && + \varphi \left[
    \sum_{t=1}^n Z_{it}^2 \I{i=j-1} + \sum_{t=1}^n Z_{i-1, t}^2 \I{i=j+1}
  \right]
\end{eqnarray*}
When $\alpha \in (0,2)$, the matrix $A$ comprising the entries on the
right-hand-side of the above expression is
\[
A =
\begin{pmatrix}
  (1 + \theta^2)\xi_1 + \varphi^2 \xi_0 & \varphi \xi_1 & 0 & \cdots & 0 \\
  \varphi \xi_1 & (1 + \theta^2)\xi_2 + \varphi^2 \xi_1 & \varphi \xi_2 & \cdots & 0 \\
  0 & \varphi \xi_2 & (1 + \theta^2)\xi_3 + \varphi^2 \xi_2 & \cdots & 0 \\
  \vdots & \vdots & \vdots & \ddots & \vdots \\
  0 & 0 & 0 & \cdots & (1 + \theta^2)\xi_p + \varphi^2 \xi_{p-1}
\end{pmatrix}
\]
where each $\xi_i \sim S_{\alpha/2}$.

\subsection{A more general model}\label{sec:GeneralModel}
Now we consider the model
\begin{eqnarray*}
  X_{it} &=& \sum_{k=0}^\infty \sum_{l=0}^\infty h_{kl} Z_{i-k,t-l}
\end{eqnarray*}
\subsubsection[alpha in (0,2)]{$\alpha \in (0,2)$}
In this case we have
\begin{eqnarray*}
  (XX')_{ij} &=& \sum_{t=1}^n \sum_{k=0}^\infty \sum_{k'=0}^\infty
  \sum_{l=0}^\infty \sum_{l'=0}^\infty h_{kl} h_{k'l'} Z_{i-k,t-l}
  Z_{j-k',t-l'} \\
  &=& \sum_{t=1}^n \sum_{k=0}^\infty \sum_{l=0}^\infty h_{kl}
  h_{j-i+k,l} Z_{i-k, t-l}^2 \\
  && + \sum_{t=1}^n \sum_{k=0}^\infty \sum_{i-k \neq j-k'}^\infty
  \sum_{l,l'=0}^\infty h_{kl} h_{k'l'} Z_{i-k,t-l} Z_{j-k',t-l'} \\
  && + \sum_{t=1}^n \sum_{k=0}^\infty \sum_{l\neq l'} h_{kl}
  h_{j-i+k, l'} Z_{i-k,t-l} Z_{i-k,t-l'} \\
  &=& I^{(1)}_{ij} + I^{(2)}_{ij} + I^{(3)}_{ij}
\end{eqnarray*}
\begin{eqnarray*}
  && \P(\left| I^{(2)}_{ij} \right| > 2 a_n^2 \epsilon) \\
  &\leq& \P\left(
    \sum_{k=0}^\infty \sum_{i-k \neq j-k'}^\infty
    \sum_{l,l'=0}^\infty | h_{kl} h_{k'l'}| \left|
      \sum_{t=1}^n  Z_{i-k,t-l} Z_{j-k',t-l'} \right| > 2 a_n^2
    \epsilon \right) \\
  &\leq& \P\left(
    \sum_{k=0}^\infty \sum_{i-k \neq j-k'}^\infty
    \sum_{l,l'=0}^\infty | h_{kl} h_{k'l'}| \left|
      \sum_{t=1}^n  Z_{i-k,t-l} Z_{j-k',t-l'} \right|
    \I{A_{kk'll'}} > a_n^2 \epsilon
  \right) \\
  && + \P\left(
    \sum_{k=0}^\infty \sum_{i-k \neq j-k'}^\infty
    \sum_{l,l'=0}^\infty | h_{kl} h_{k'l'}| \left|
      \sum_{t=1}^n  Z_{i-k,t-l} Z_{j-k',t-l'} \right|
    \I{A^c_{kk'll'}} > a_n^2 \epsilon
  \right) \\
  &=& P^{(2)}_1 + P^{(2)}_2
\end{eqnarray*}
where the events $A_{kk'll'}$ are
\begin{eqnarray*}
  A_{kk'll'} &=& \left\{
  k,k',l,l': | h_{kl} h_{k'l'}| \left| \sum_{t=1}^n  Z_{i-k,t-l} Z_{j-k',t-l'}
  \right| > a_n^2 \epsilon
  \right\}
\end{eqnarray*}
Then we have
\begin{eqnarray*}
  P^{(2)}_1 &\leq& \P\left(
    \bigcup_{\substack{k \geq 0 \\ i-k \neq j-k' \\ l,l'\geq0}}
    A_{kk'll'}
  \right) \\
  &\leq& \sum_{k=0}^\infty \sum_{i-k \neq j-k'}^\infty
  \sum_{l,l'=0}^\infty \P\left(
    \left| \sum_{t=1}^n  Z_{i-k,t-l} Z_{j-k',t-l'}
    \right| > | h_{kl} h_{k'l'}|^{-1} a_n^2 \epsilon 
  \right)
\end{eqnarray*}
Note $\E Z_1 Z_2 = (\E Z_1)^2 = 0$ if $\E|Z_1| < \infty$. Thus the
large deviation results of Cline and Hsing \cite{ClingHsing1998} and
Nagaev \cite{nagaev1979} give
\begin{eqnarray*}
  && \P\left(
    \left| \sum_{t=1}^n  Z_{i-k,t-l} Z_{j-k',t-l'}
    \right| > | h_{kl} h_{k'l'}|^{-1} a_n^2 \epsilon 
  \right) \\
  &\sim& c n \P(|Z_1 Z_2| > | h_{kl} h_{k'l'}|^{-1} a_n^2 \epsilon) \\
  &\sim& c n | h_{kl} h_{k'l'}|^{\alpha} a_n^{-2\alpha}
\end{eqnarray*}
\begin{eqnarray*}
  P^{(2)}_1 &\leq& c \left( \sum_{k=0}^\infty \sum_{i-k \neq j-k'}^\infty
  \sum_{l,l'=0}^\infty | h_{kl} h_{k'l'}|^{\alpha} \right) l(n) n^{-1}
\to 0
\end{eqnarray*}
As for $P^{(2)}_2$ we first apply Markov's inequality
\begin{eqnarray*}
  P^{(2)}_2 &\leq& a_n^{-2} \epsilon^{-1}
  \sum_{k=0}^\infty \sum_{i-k \neq j-k'}^\infty
  \sum_{l,l'=0}^\infty | h_{kl} h_{k'l'}| \E \left|
  \sum_{t=1}^n  Z_{i-k,t-l} Z_{j-k',t-l'} \right|
  \I{A^c_{kk'll'}}
\end{eqnarray*}
Since the large deviation results imply $\P(\sum_{t=1}^n  Z_{i-k,t-l}
Z_{j-k',t-l'} > x) \sim n \P(|Z_1 Z_2| > x)$, we have
\begin{eqnarray*}
  && | h_{kl} h_{k'l'}| \E \left| \sum_{t=1}^n  Z_{i-k,t-l} Z_{j-k',t-l'} \right|
  \I{A^c_{kk'll'}} \\
  &\leq& \left(
    | h_{kl} h_{k'l'}|^{\alpha + \delta} \E \left| \sum_{t=1}^n
      Z_{i-k,t-l} Z_{j-k',t-l'} \right|^{\alpha + \delta}
    \I{A^c_{kk'll'}} \right)^{1/(\alpha + \delta)} \\
    &\sim& \left[
      {\alpha \over \delta} \P\left(
      | h_{kl} h_{k'l'}| \left| \sum_{t=1}^n  Z_{i-k,t-l} Z_{j-k',t-l'}
      \right| > a_n^2 \epsilon \right) \right]^{1/(\alpha + \delta)} \\
  &\sim& \left[
    {\alpha \over \delta} | h_{kl} h_{k'l'}|^{\alpha}
    c n a_n^{-2\alpha} L_{12}(a_n)
    \right]^{1/(\alpha + \delta)}
\end{eqnarray*}
where $\delta$ is some constant such that $\delta > \max\{1-\alpha,
0\}$. Therefore
\begin{eqnarray*}
  P^{(2)}_2 &\leq& c  \left[\sum_{k=0}^\infty \sum_{i-k \neq j-k'}^\infty
  \sum_{l,l'=0}^\infty |h_{kl} h_{k'l'}|^{\alpha/(\alpha + \delta)}\right]
  n^{1/(\alpha + \delta)} a_n^{-2\alpha/(\alpha + \delta )-2} L_{12}(a_n)\\
  &\to&  0
\end{eqnarray*}
Next we show
\begin{eqnarray*}
  \P(|I^{(3)}_{ij}| > 2 a_n^2 \epsilon) &\to& 0
\end{eqnarray*}
\begin{eqnarray*}
  && \P(|I^{(3)}_{ij}| > a_n^2 \epsilon) \\
  &\leq& \P(\sum_{t=1}^n \sum_{\substack{l,l'=0 \\ l \neq l'}}^\infty
  \left|\sum_{k=0}^\infty h_{kl}h_{j-i+k,l'} Z_{i-k,t-l}Z_{i-k,t-l'}\right|
  \I{A_{tll'}} > a_n^2 \epsilon ) \\
  && + \P(\sum_{t=1}^n \sum_{\substack{l,l'=0 \\ l \neq l'}}^\infty
  \left|\sum_{k=0}^\infty h_{kl}h_{j-i+k,l'} Z_{i-k,t-l}Z_{i-k,t-l'}\right|
  \I{A^c_{tll'}} > a_n^2 \epsilon ) \\
  &=& P^{(3)}_1 + P^{(3)}_2
\end{eqnarray*}
where the events $A_{tll'}$ are
\begin{eqnarray*}
  A_{tll'} &=& \left\{t,l,l': \left|
      \sum_{k=0}^\infty h_{kl}h_{j-i+k,l'} Z_{i-k,t-l}Z_{i-k,t-l'}
    \right| > a_n^2 \epsilon \right\}
\end{eqnarray*}
\begin{eqnarray*}
  P^{(3)}_1 &\leq& n \sum_{\substack{l,l'=0 \\ l \neq l'}}^\infty \P\left(
    \left|\sum_{k=0}^\infty h_{kl}h_{j-i+k,l'} Z_{i-k,t-l}Z_{i-k,t-l'}\right| > a_n^2 \epsilon
  \right) \\
  &\sim& n \sum_{\substack{l,l'=0 \\ l \neq l'}}^\infty \sum_{k=0}^\infty |h_{kl}h_{j-i+k,l'}|^{\alpha} \P(|Z_1 Z_2| > a_n^2 \epsilon)
  \\
  &\sim& c n \sum_{\substack{l,l'=0 \\ l \neq l'}}^\infty
  \sum_{k=0}^\infty |h_{kl}h_{j-i+k,l'}|^{\alpha} a_n^{-2\alpha} L_{12}(a_n)\\  
  &\sim& c n^{-1} l(n)  \to 0
\end{eqnarray*}
where $L_{12}(\cdot)$ and $l(\cdot)$ are slowly varying functions. As
for $P^{(3)}_2$, Markov's inequality gives
\begin{eqnarray*}
  P^{(3)}_2 &\leq& c n a_n^{-2} \sum_{\substack{l,l'=0 \\ l \neq l'}}^\infty
  \E\left|\sum_{k=0}^\infty h_{kl}h_{j-i+k,l'}
    Z_{i-k,l}Z_{i-k,l'}\right| \I{A^c_{tll'}} \\
\end{eqnarray*}
Let
\begin{eqnarray*}
  Y_{tll'} &=& \left|\sum_{k=0}^\infty h_{kl}h_{j-i+k,l'}
              Z_{i-k,t-l}Z_{i-k,t-l'}\right|
\end{eqnarray*}
\begin{enumerate}[i)]
\item In case $\alpha < 1$. Karamata's theorem yields
  \begin{eqnarray*}
    && \E Y_{tll'}\I{A^c_{tll'}} \\
    &\sim& {\alpha \over 1 -\alpha} \P(Y_{tll'} > a_n^2 \epsilon) \\
    &\sim& c \sum_{k=0}^\infty |h_{kl}h_{j-i+k,l'}|^{\alpha}
           a_n^{-2\alpha} L_{12}(a_n) \\
  \end{eqnarray*}
  Thus
  \begin{eqnarray*}
    P^{(3)}_2 &\leq& c n^{-2(1/\alpha + 1)+1}l(n) \to 0
  \end{eqnarray*}

\item In case $\alpha > 1$. 
  \begin{eqnarray*}
    && \E Y_{tll'} \I{A^c_{tll'}} \leq \E Y_{tll'} = \text{constant}
  \end{eqnarray*}
  \begin{eqnarray*}
    P^{(3)}_2 &\leq& cna_n^{-2} \sim c n^{1-2/\alpha} l(n) \to 0
  \end{eqnarray*}
\item In case $\alpha=1$.
  Choose $M$ so large that $\P(Y_{tll'} > x) \leq {L(x) \over x}
  (1 + \epsilon)$ for a slowly varying function $L(x)$
  where $x > M$. Then we have
  \begin{eqnarray*}
    && \E Y_{tll'} \I{A^c_{tll'}} \\
    &\leq& \text{constant} + \int_{M}^{a_n^2 \epsilon}
           {L(x) \over x} dx
  \end{eqnarray*}
  According to Mikosch et al \cite{Embrechts1997},
  \begin{eqnarray*}
    \int_{M}^{a_n^2 \epsilon}
    {L(x) \over x} dx &=& L_1(a_n^2)
  \end{eqnarray*}
  where $L_1(\cdot)$ is a slowly varying function. So we have
  \begin{eqnarray*}
    P^{(3)}_2 &\leq& c n a_n^{-2} L_1(a_n)\\
    &\sim& c n^{-2/\alpha + 1} l(n) \to 0
  \end{eqnarray*}
\end{enumerate}
Now we have shown $I^{(2)}_{ij}, I^{(3)}_{ij} \xrightarrow{P} 0$ as $n
\to \infty$. Then it is clear $(XX')_{ij}$ has the following structure
as $n \to \infty$:
\begin{eqnarray*}
  a_n^{-2}(XX')_{ij} &=& a_n^{-2} \sum_{t=1}^n \sum_{k=0}^\infty \sum_{l=0}^\infty
  h_{kl} h_{k+j-i,l} Z_{i-k, t-l}^2 + o_P(1)
\end{eqnarray*}
When $j < i$, one may re-write
\begin{eqnarray*}
  a_n^{-2}(XX')_{ij} &=& a_n^{-2} \sum_{t=1}^n \sum_{k=j-i}^\infty \sum_{l=0}^\infty
  h_{k+|j-i|,l} h_{kl} Z_{j-k, t-l}^2 + o_P(1)
\end{eqnarray*}
Because $h_{kl} = 0$ if $\min\{k,l\} < 0$ we have
\begin{eqnarray*}
  a_n^{-2}(XX')_{ij} &=& a_n^{-2}\sum_{t=1}^n \sum_{k=0}^\infty \sum_{l=0}^\infty
  h_{k+|j-i|,l} h_{kl} Z_{j-k, t-l}^2 + o_P(1)
\end{eqnarray*}
In summary
\begin{eqnarray*}
  a_n^{-2}(XX')_{ij} &=& a_n^{-2} \sum_{t=1}^n \sum_{k=0}^\infty \sum_{l=0}^\infty
  h_{k+|j-i|,l} h_{kl} Z_{(i \wedge j) -k, t-l}^2 + o_P(1)
\end{eqnarray*}

\subsection[Approximating XX']{Approximating $XX'$}
Here we prove
\begin{eqnarray*}
  a_n^{-2}(XX')_{ij} &=& a_n^{-2}\sum_{t=1}^n \sum_{k=0}^\infty \sum_{l=0}^\infty
  h_{k+|j-i|,l} h_{kl} Z_{(i \wedge j) -k, t-l}^2 + o_P(1) \\
  &=& a_n^{-2} \sum_{t=1}^n \sum_{k=0}^M \sum_{l=0}^M
  h_{k+|j-i|,l} h_{kl} Z_{(i \wedge j) -k, t-l}^2 + o_P(1) \\
\end{eqnarray*}
where $M \to \infty$.
\begin{proof}
  \begin{eqnarray*}
    && \P\left(\left|a_n^{-2}(XX')_{ij} - a_n^{-2} \sum_{t=1}^n \sum_{k=0}^M \sum_{l=0}^M
    h_{k+|j-i|,l} h_{kl} Z_{(i \wedge j) -k, t-l}^2\right| > \epsilon \right) \\
   &=& \P\left(\left|a_n^{-2} \sum_{t=1}^n \sum_{k=M+1}^\infty \sum_{l=M+1}^\infty
    h_{k+|j-i|,l} h_{kl} Z_{(i \wedge j) -k, t-l}^2\right| > \epsilon \right)
  \end{eqnarray*}
  Since
  \begin{eqnarray*}
    a_n^{-2} \sum_{t=1}^n Z_{(i \wedge j) -k, t-l}^2 &\xrightarrow{d}&
    \xi_{(i \wedge j) - k} \sim S_{\alpha/2}
  \end{eqnarray*}
  We have
  \begin{eqnarray*}
   && \P\left(\left|a_n^{-2} \sum_{t=1}^n \sum_{k=M+1}^\infty \sum_{l=M+1}^\infty
    h_{k+|j-i|,l} h_{kl} Z_{(i \wedge j) -k, t-l}^2 \right| > \epsilon \right)
  \\
  &\leq& \P\left(\sum_{k=M+1}^\infty \sum_{l=M+1}^\infty \left|
    h_{k+|j-i|,l} h_{kl} \xi_{(i \wedge j) -k} \I{A_{kl}}\right| > \epsilon/2 \right)
  \\
  && + \P\left(\sum_{k=M+1}^\infty \sum_{l=M+1}^\infty \left|
    h_{k+|j-i|,l} h_{kl} \xi_{(i \wedge j) -k} \I{A^c_{kl}} \right| > \epsilon/2 \right)
  \\
  \end{eqnarray*}
  where
  \begin{eqnarray*}
    A_{kl} = \left\{
      k,l \geq M+1: \left| h_{k+|j-i|,l} h_{kl} \xi_{(i \wedge j) -k}
      \right| > \epsilon/2
    \right\}
  \end{eqnarray*}
  So 
  \begin{eqnarray*}
    && \P\left(\left|a_n^{-2} \sum_{t=1}^n \sum_{k=M+1}^\infty \sum_{l=M+1}^\infty
        h_{k+|j-i|,l} h_{kl} Z_{(i \wedge j) -k, t-l}^2\right| > \epsilon
    \right) \\
    &\leq& \sum_{k=M+1}^\infty \sum_{l=M+1}^\infty \P\left(
      \left| h_{k+|j-i|,l} h_{kl} \xi_{(i \wedge j) -k}\right| >
      \epsilon/2\right) \\
    && + 2 \epsilon^{-1}\sum_{k=M+1}^\infty \sum_{l=M+1}^\infty
    |h_{k+|j-i|,l} h_{kl}| \E \xi_{(i \wedge j) -k} \I{A^c_{kl}} \\
    &\leq& \sum_{k=M+1}^\infty \sum_{l=M+1}^\infty |h_{k+|j-i|,l}
    h_{kl}|^{\alpha/2} \\
    && + 2 \epsilon^{-1}\sum_{k=M+1}^\infty \sum_{l=M+1}^\infty
    |h_{k+|j-i|,l} h_{kl}|^{1+\alpha/2} {\alpha/2 \over 1 - \alpha/2} \\
    &\to& 0 \text{ as } M \to \infty
  \end{eqnarray*}
\end{proof}
Now it is clear
\begin{eqnarray*}
  a_n^{-2}(XX')_{ij} &=& a_n^{-2} \sum_{t=1}^n \sum_{k=0}^M \sum_{l=0}^M
  h_{k+|j-i|,l} h_{kl} Z_{(i \wedge j) -k, t-l}^2 + o_P(1) \\
  &=& \sum_{k=0}^M \left(\sum_{l=0}^M h_{k+|j-i|,l}
    h_{kl}\right)\xi_{(i \wedge j) -k}
\end{eqnarray*}
Explicitly, we have
\begin{eqnarray*}
  && a_n^{-2} XX' \\
  &=& \sum_{k=0}^\infty \sum_{l=0}^\infty
  h_{kl}
  \begin{pmatrix}
    h_{kl} \xi_{1-k} & 
     h_{k+1,l} \xi_{1-k} & 
     h_{k+2,l} \xi_{1-k} & \cdots & h_{k+p-1,l}
    \xi_{1-k}\\
     h_{k+1,l} \xi_{1-k} &
    h_{kl} \xi_{2-k} & 
     h_{k+1,l} \xi_{2-k} & \cdots & h_{k+p-2,l}
    \xi_{2-k}\\
     h_{k+2,l} \xi_{1-k} &
     h_{k+1,l} \xi_{2-k} & 
    h_{kl} \xi_{3-k} & \cdots &
    h_{k+p-3,l} \xi_{3-k}\\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    h_{k+p-1} \xi_{1-k} & h_{k+p-2} \xi_{2-k} &
    h_{k+p-3} \xi_{3-k} & \cdots & h_{kl} \xi_{p-k}
  \end{pmatrix}
\end{eqnarray*}
\subsection{When the matrix dimension tends to infinity}
Consider the 1-norm
\begin{eqnarray*}
  && \| a_n^{-2} XX' - a_n^{-2} \sum_{t=1}^n
     \sum_{k=0}^\infty \sum_{l=0}^\infty
     h_{k+|j-i|,l} h_{kl} Z_{(i \wedge j) -k, t-l}^2\|_1 \\
  &=& a_n^{-2} \sum_{i=1}^p (I^{(2)}_{ii} + I^{(3)}_{ii})
\end{eqnarray*}
Because $\P(|I^{(a)}_{ii}| > a_n^2 \epsilon) < l(n)
n^{-\delta(\alpha)}$ for $a = 2,3$ and $\delta(\alpha) > 0$, as is
proven in \S\ref{sec:GeneralModel}, we have
\begin{eqnarray*}
  \P(\sum_{i=1}^p a_n^{-2} I^{(a)}_{ii} > \epsilon) &\leq& p l(n)
  n^{-\delta(\alpha)}
\end{eqnarray*}
if $n^{-\delta(\alpha)} p(n) \to 0$, e.g. when $p(n)$ is slowly varying, the RHS
tends to 0.

\fi
\section{Stochastic volatility model}
Here we consider the model
$$
X_t = \{\sigma_{it} Z_{it}\}_{i=1,2,\dots,p;\; t=1,2\dots,n}
$$
where we fix $p$ and let $n \to \infty$. We assume $Z_{it} \in
\mathcal R_{-\alpha}$ are iid, $\sigma_{it}$ is strictly stationary
for all $i$ and independent of each and every $Z$. Moreover we assume
$\E \sigma^{\alpha + s} < \infty$ and $\E (\sigma_{i1}
\sigma_{j1})^{\alpha + s'} < \infty$ for some $s, s' > 0$ and $i \neq j$.

In subsection \ref{sec:SV1} we show that the eigenvalues of $a_n^{-2}
XX'$ are asymptotically equal to those of $a_n^{-2} \diag(XX')$, and
in section \ref{sec:SV2} we show that $i-th$ diagonal entry of
$a_n^{-2} XX'$ tends to a $S_{\alpha/2}$ rv.

In the rest of this section, we write $L(\cdot)$ for a slowly varying
function whose particular form is of no interest.

\subsection{Off-diagonal elements vanish in probability} \label{sec:SV1}
Here we show that the eigenvalues of $a_n^{-2}XX'$ are asymptotically
equal to those of $a_n^{-2} \diag(XX')$. Because $p$ is constant, it
suffices to show for all $i \neq j$
$$
a_n^{-2}|(XX')_{ij}| \overset{P}{\to} 0 \text{ as } n \to \infty
$$
 Let
\[
\sum_{k=1}^\infty \pop{\tilde P^{(i)}_{k,l}}
\]
be PRM($\mu_l$) where
\begin{eqnarray*}
\mu_l (dx) &=& \left\{
  \begin{array}{lr}
    p_0 \alpha x^{-\alpha - 1} \1{x > 0} dx + (1 - p_0)
    \alpha (-x)^{-\alpha - 1} \1{x < 0} dx & \text{ if } l=0 \\
    p_1 \alpha x^{-\alpha - 1} \1{x > 0} dx + (1 - p_1)
    \alpha (-x)^{-\alpha - 1} \1{x < 0} dx & \text{ if } l \geq 1
  \end{array} \right. \\
  p_1 &=& p_0^2 + (1- p_0)^2
\end{eqnarray*}
For fixed $1 \leq i \leq p$ and $1 \leq h \leq p-i$, let
\[
P^{(i)}_{k,0} = (\E\sigma_{i1}^\alpha)^{1/\alpha} \tilde P^{(i)}_{k,0}
\]
and
\[
P^{(i)}_{k,h} = [\E(\sigma_{i1} \sigma_{i+h, 1})^\alpha]^{1/\alpha} \tilde P^{(i)}_{k,h}
\]
These notations are meaningful due to the stationarity of
$\{\sigma_{it}\}_{i=1,\dots,p; t=1,\dots,n}$

By the same arguments as found in the proof of theorem 3.1 in Davis
and Mikosch \cite{davis2001} it may be shown
\begin{equation}
  \label{eq:pp_convergence}
N^{(i)}_n = \sum_{t=1}^n \pop{(0,\dots,0, a_n^{-1} X_{it}, b_n^{-1} X_{it} X_{i+1,
    t}, \dots, b_n^{-1} X_{it} X_{pt})} \overset{d}{\to}
N^{(i)} = \sum_{k=1}^\infty \sum_{j=0}^{p-i} \pop{P^{(i)}_{k,j} \vec{e}_{i+j}}
\end{equation}
where $\vec{e}_{i+j} \in \mathds R^{p}$ is the unit vector with $(i+j)$-th
component equal to 1 and the rest 0. $a_n$ and $b_n$ are defined such
that 
\begin{eqnarray*}
  \P(|Z| > a_n) &=& 1/n \\
  \P(|Z_1 Z_2| > b_n) &=& 1/n
\end{eqnarray*}
For $\delta > 0$, $1 \leq h \leq p-i$ , define mapping
$T^{(i)}_{\delta, h} $ from $M_p(\mathds R^p \setminus \{(0,
\dots, 0)\})$ to $\mathds R$:
\[
T^{(i)}_{\delta, h} \left(\sum_{t=1}^\infty \pop{(g_{1t}, g_{2t},
    \dots, g_{pt})}\right) = \sum_{t=1}^\infty g_{i+h, t}
\1{|g_{i+h,t}|> \delta}
\]
Using an argument similar to that given in section 3 of Resnick
\cite{Resnick1986}, it maybe shown that $T^{(i)}_{\delta, h}$ is a.s. continuous
with respect to the limit point process $N^{(i)}$ defined in
\eqref{eq:pp_convergence}. Thus by the continuous mapping theorem
\[
T^{(i)}_{\delta, h}(N^{(i)}_n) \overset{d}{\to} T^{(i)}_{\delta, h}(N^{(i)})
\]
i.e.
\[
\sum_{t=1}^n b_n^{-1} X_{it} X_{i+h, t}\1{|X_{it} X_{i+h,t}| > b_n \delta} \overset{d}{\to}
\sum_{k=1}^\infty P^{(i)}_{k,h}
\]
Choose $\delta = n^{-\beta/\alpha}$, $\beta > 1$ such that $b_n \delta
\to 0$. Then 
\[
\lim_{\delta \to 0} \sum_{t=1}^n b_n^{-1} X_{it} X_{i+h, t}
\1{|X_{it} X_{i+h,t}| > b_n \delta} \to \sum_{t=1}^\infty b_n^{-1}
X_{it} X_{i+h, t}
\]
So it remains to show
\[
\lim_{\delta \to 0} \limsup_{n \to \infty} \P\left[
\left|\sum_{t=1}^n b_n^{-1} X_{it} X_{i+h, t} \1{b_n^{-1} |X_{it} X_{i+h,
    t}| \leq \delta} \right| > \epsilon\right] = 0
\]
Using Markov's inequality
\begin{eqnarray*}
  && \P\left[\left|\sum_{t=1}^n b_n^{-1} X_{it} X_{i+h, t} \1{b_n^{-1} |X_{it}
        X_{i+h, t}| \leq \delta} \right|> \epsilon \right] \\
  &\leq& \epsilon^{-1} b_n^{-1} \sum_{t=1}^n \E \left|X_{it}
    X_{i+h, t}\1{b_n^{-1} |X_{it} X_{i+h, t} | \leq \delta} \right|
\end{eqnarray*}
Since $\E(\sigma_{it} \sigma_{i+h, t})^{\alpha} < \infty$ and
$Z_{it} Z_{i+h, t} \in \mathcal R_{-\alpha}$, $X_{it} X_{i+h,t} \in
\mathcal R_{-\alpha}$. By H\"older's inequality,
\begin{eqnarray*}
  && \E \left|X_{it} X_{i+h, t}\1{b_n^{-1} |X_{it} X_{i+h, t} | \leq
      \delta} \right|  \\
  &\leq& \left [\E \left|X_{it} X_{i+h, t} \right|^2\1{b_n^{-1} |X_{it} X_{i+h, t} | \leq
      \delta} \right]^{1/2}  \\
  &\sim& \left({\alpha \over 2 - \alpha}\right)^{1/2}  \delta^{1 -
    \alpha/2} b_n n^{-1/2}
\end{eqnarray*}
Since $0 < \alpha < 2$, and $\delta = n^{-\beta/\alpha}$, the above
expression tends to 0 as $n \to \infty$. Now applying theorem 4.2 in
Billingsley \cite{billingsley1968convergence}, we have
\[
\sum_{t=1}^n b_n^{-1} X_{it} X_{i+h, t} \overset{d}{\to}
\sum_{k=1}^\infty P^{(i)}_{k,h}
\]
Now let $\sum_{k=1}^\infty \pop{\tilde P^{(i)}_{k,h}}$ be the limit point
process of $\sum_{k=1}^n \pop{b_n^{-1} j_{k,h}}$ where $j_{k,h} \sim
\mu_h$
\begin{enumerate}[(i)]
\item if $0 < \alpha < 1$, by the central limit theorem
  \begin{eqnarray*}
    \sum_{k=1}^\infty P^{(i)}_{k,h} &=& [\E(\sigma_{it} \sigma_{i+h,
      t})^{\alpha}]^{1/\alpha} \lim_{n \to \infty}
    b_n^{-1}\sum_{k=1}^n j_{k,h} \overset{d}{=} \zeta_{i,h} \sim
    S_{\alpha}
  \end{eqnarray*}
  But $b_n/a_n^2 \sim n^{1/\alpha - 2/\alpha} L(n) \to 0$, so
  \[
  a_n^{-2} \sum_{t=1}^n X_{it} X_{i+h, t} \overset{P}{\to} 0
  \]

\item if $1 < \alpha < 2$, the central limit theorem gives
  \begin{eqnarray*}
    b_n^{-1} (\sum_{k=1}^n j_{k,h} - n \E j_{k,h}) &\overset{d}{\to}&
    \zeta_{i,h} \sim S_{\alpha} \\
    b_n a_n^{-2} \left(
      \sum_{k=1}^n b_n^{-1} j_{k,h} - b_n^{-1} n \E j_{k,h}
    \right) &\overset{P}{\to}& 0
  \end{eqnarray*}
  Since $n a_n^{-2} \sim n^{1 - 2/\alpha} L(n)\to 0$, it follows
  \[
  a_n^{-2} \sum_{t=1}^n X_{it} X_{i+h, t} \overset{P}{\to} 0
  \]
\end{enumerate}

\subsection[Diagonal Elements tend to alpha/2 stable rv]{Diagonal
  Elements tend to $\alpha/2$ stable rv}\label{sec:SV2}
By the same arguments as for mapping $T^{(i)}_{\delta, h}$, one can
show
\[
T^{(i)}_{\delta} \left(\sum_{t=1}^\infty \pop{(g_{1t}, g_{2t},
    \dots, g_{pt})}\right) = \sum_{t=1}^\infty g_{it}^2
\1{|g_{t}|> \delta}
\]
is a.s. continuous. Thus using \eqref{eq:pp_convergence} and
the continuous mapping theorem, it follows
\[
\sum_{t=1}^n a_n^{-2} X_{it}^2 \1{|X_{it}| > \delta a_n}
\overset{d}{\to} \sum_{k=1}^\infty P^{(i)}_{k0}
\]
Choose $\delta = n^{-1/(\alpha - \epsilon)}$. Then $\delta a_n \to 0$,
and
\[
\lim_{\delta \to 0}\sum_{t=1}^n a_n^{-2} X_{it}^2 \1{|X_{it}| > \delta
  a_n} = \sum_{t=1}^n a_n^{-2} X_{it}^2
\]
Using Markov's inequality and the Karamata theorem as in subsection
\ref{sec:SV1}, it is easily shown
\[
\P\left[
  \left|
\sum_{t=1}^n a_n^{-2} X_{it}^2 \1{|X_{it}| \leq \delta a_n}
\right|> \epsilon
\right] \leq {\alpha \over 2 - \alpha} \epsilon^{-1} \delta^{2-\alpha}
\to 0
\]
So by Billingsley \cite{billingsley1968convergence}, it follows
\[
a_n^{-2} \sum_{t=1}^n  X_{it}^2 \overset{d}{\to} \sum_{k=1}^\infty P^{(i)}_{k0}
\]
Choose $\sum_{k=1}^\infty \pop{\tilde P_{k0}}$ as $\lim_{n \to \infty}\sum_{k=1}^n \pop{a_n^{-2} j_{k0}}$, where $j_{k0} \sim
\mu_0$. Then the central limit theorem gives
\[
a_n^{-2} \sum_{t=1}^n  X_{it}^2 \overset{d}{\to} (\E
\sigma_{i1}^\alpha)^{1/\alpha} \xi_{i}
\]
where $\xi_i \sim S_{\alpha/2}$.
% we conclude
% \[
% a_n^{-2} \mtx{XX'} \overset{d}{\to} \diag(\xi_1, \xi_2, \dots, \xi_p)
% \]

\bibliographystyle{unsrt}
\bibliography{../thesis/econophysics}
\end{document}
