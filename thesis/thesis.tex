\documentclass{report}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{amssymb}
\usepackage{amsfonts, amsmath}
\usepackage{mathrsfs}
\usepackage{enumerate}
\usepackage[bookmarks=true]{hyperref}
\usepackage{bookmark}

\input{../physics_common.tex}
\DeclareGraphicsExtensions{.pdf,.png,.jpg}

% \title{Stocks' Cross-Correlations}
% \subtitle{}

\author{Xie Xiaolei}
\date{\today}
\begin{document}

\begin{titlepage}
\begin{center}

% Upper part of the page. The '~' is needed because \\
% only works if a paragraph has started.
\includegraphics[width=0.5\textwidth]{../pics/lund_uni-logo_s}~\\[1cm]

% \textsc{\LARGE Lund University}\\[1.5cm]

\textsc{\Large Master's Thesis Project}\\[0.5cm]

% Title
%\HRule \\[0.4cm]
{ \huge \bfseries Returns Models and Their Implications
  on Cross-Correlation Matrix \\[0.4cm] }

%\HRule \\[1.5cm]

% Author and supervisor
\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Author:}\\
\textsc{Xie} Xiaolei
\end{flushleft}
\end{minipage}
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Supervisor:} \\
Prof. Sven \textsc{\AA berg}
\end{flushright}
\end{minipage}

\vfill

% Bottom of the page
{\large \today}

\end{center}
\end{titlepage}
% \maketitle
\tableofcontents

\chapter{Introduction}
The mathematics of the financial market has always been a topic that
arouses interest and imagination, and with no doubt, has been
studied from many aspects and in many different ways.

Central to all these studies are the concepts of probability
distributions and correlations. The values of stocks, futures,
options, etc. are stochastic in nature and are governed by the laws of
stochastic processes, which are expressed in terms of probability
distributions and correlations.

Meanwhile, the observables of the market are prices, volumes,
turn-over (the amount of money paid in a trade), names of the brokers,
and time of the trades. These quantities don't make much sense
by themselves but do reveal the probabilistic dynamics of the market
when put together and turned into statistics.

Here by dynamics we mean how the value of an asset is affected by its
own history as well as by the histories and current values of other
assets in the market. The influence from the asset's own history is
termed autocorrelations, i.e. correlations in time, while
the influence from other assets are termed cross correlations.

Mathematical models that describe the aforementioned observables in
terms of probability distributions, autocorrelations and cross
correlations give predictive power and help determine a fair price of
a given asset. Hence they are a central topic of the mathematics of
the financial market.

Once a model has been tentatively constructed, it must be
verified. The verification again relies on the statistics of the
observables. Over the years, a number of phenomena have been
consistently reported and become generally accepted. These are referred
to as stylized facts, which are detailed in \S
\ref{sec:StylizedFacts} and constitute a standardized test suit for
model verification.

% Judged by the stylized facts, each of the models proposed in the
% literature has its own advantages and disadvantages. The most
% prominent and influential among them are arguably the GARCH
% models. Their first and simplest variant was proposed by Bollerslev in
% 1990 \cite{Bollerslev86} and has been developed tremendously since
% then.

% An apparent advantage of the GARCH models is their simplicity. They
% are linear models, where the conditional variance is modeled as a
% linear combination of past variances and past innovations squared. As
% a result, it is relatively easy to fit a model to a given set of data,
% i.e. to estimate the model parameters with respect to the
% data. Because of the linearity, the estimation of model parameters
% often give consistent results, regardless of the choice of initial
% parameter values or the choice of pre-sample data.

% In contrast, many of the other types of models are often specified in
% a complicated, although more advanced way, for example, by stochastic
% differential equations. This kind of specifications certainly have the
% advantage of directly linking to the theory of stochastic processes,
% but nonetheless are difficult to fit to data, one of the most tricky
% problems being the high sensitivity to the choice of initial parameter
% values.

% Another advantage of GARCH models is that their tail behavior is
% better understood than that of their competitors. The simplest variant
% of GARCH, namely the GARCH(1,1) models are proven to have regularly
% varying tails, or in common terms, power-law tails
% \cite{mikosch2000}. In particular, if the $\alpha_1$ and $\beta_1$
% parameters of the model sum up to 1, the return series generated by
% the model has a power-law tail with power exponent 2, and consequently
% the squared returns have a tail with power exponent 1 -- by the
% central limit theorem the sum of the squared returns will converge to
% the Cauchy distribution, a stable distribution with L\'evy index 1. We
% explore this fact in more details in chapter
% \ref{chp:CrossCorrelationFat} where the cross-correlation matrix is
% studied.

% The GARCH model with $\alpha_1 + \beta_1 = 1$ is more often referred
% to as an IGARCH model, aka. an integrated GARCH model. It is of
% particular interest because, very often, when fitting a GARCH(1,1)
% model to real financial data, one finds $\alpha_1 + \beta_1$ extremely
% close to 1, i.e. at values like 0.99, 0.995, etc.

% A third advantage of GARCH models is their flexibility. Namely one can
% choose any appropriate distribution to model the innovations. While
% the Gaussian and the student's t distributions are the most common,
% the use of other distributions such as asymmetric student's t and
% Johnson Su has also been studied in the literature (see
% \cite{Simonato2012}).

% However, GARCH models have their disadvantages too. The original
% GARCH model does not account for skewness in the distribution of
% the returns. As a result, the skewness, often attributed to
% price-volatility correlations, have to be added ``by hand''. This kind
% of treatment fixes the model on the one hand, but introduces some
% arbitrariness on the other. Put in another way, skewness does not
% follow naturally from the structure of the model but rather is added
% explicitly.

% Moreover, fitting a GARCH model often results in a large number of
% parameters; 16 to 30 are not uncommon when intra-day data are being
% dealt with. This raises the question of true fitness: the model
% specification may indeed fit the selected data set but very well fall
% short of an accurate description of the time series in question. In
% other words, such a model is unstable and hence unreliable: if one
% removes a small fraction of data from the beginning of the set and
% appends some to the end -- a necessary operation for the evolution of
% the model -- the model parameters change drastically and so do the
% predictions.

% As intra-day transaction data become more accessible, the volatility,
% which has been considered an unobservable quantity by models like
% GARCH, can be estimated by the so called realized volatility, which is
% the square root of the sum of squared returns that have been sampled
% at a higher frequency than are those that one intends to model. For
% example, it has been shown that the volatility of daily returns is
% best estimated using 17-minute intra-day returns \cite{Sahalia05}.

% Now that the volatility can be estimated, models that take advantage
% of this are expected to achieve significantly higher accuracy than
% those that do not. The model presented in this thesis is built indeed
% in this spirit.

% Like the GARCH models, our proposed model has the simple structure
% $r_t = \sigma_t z_t$, where $z_t$ has unit variance and is
% drawn from any appropriate distribution; $\sigma_t$ is the
% volatility. Unlike GARCH, which treats $\sigma_t$ as deterministic at
% time $t$, we treat $\sigma_t$ as random and model $\ln\sigma_t$ using
% time series analysis. This is possible because past values of
% $\ln\sigma_t$ can be estimated using past values of the returns at a
% higher frequency, as has just been discussed.

% Our model has a number of advantages too. Like the original
% GARCH model, it is simple. It has two random variables $z_t$ and
% $\ln\sigma_t$, the latter being the subject of time series
% modeling. The use of conventional time series analysis means the
% model can be not only more accurate but also more economic -- long
% memory of the volatility, which is the cause of a large number of
% parameters in a GARCH model, can be modeled by differencing or
% fractional differencing. As a result, much fewer parameters are needed
% for the model.

% Secondly, unlike GARCH models, skewness follows directly from the
% correlation between the two random variables $z_t$ and
% $\ln\sigma_t$. As shown in chapter \ref{chp:PriceModels},
% positive/negative correlation leads to positive/negative skewness,
% and skewness increases with the strength of correlation.

% Thirdly, our model has sub-exponential tails, as shown in chapter
% \ref{chp:PriceModels}. This type of tails are not as fat as regularly
% varying tails, but in our specific case, the tails do behave
% approximately as a power-law in the regions relevant to stock and
% index returns. After all, power-law behavior is an empirical result
% obtained in the returns data, and such data are not available to a
% statistically significant amount at extremely large values. In plain
% words, situations where one makes a 20\%+ profit on a single stock
% trade in a single day happen too rarely to allow any statistical
% statement.

Another important part of the thesis is about the cross-correlation
matrix, for which we study the distribution of its elements as well
as its eigenvalues. When the cross-correlation matrix is constructed
from returns with simple Gaussian distribution, the matrix is called a
Wishart Matrix and has been studied extensively in the literature. If
the returns have L\'evy distributions, the matrix is termed
Wishart-L\'evy and has been studied to some extent, particularly
regarding its eigenvalue distribution \cite{politi2010}.

However, it is understood that real stock/index returns are much more
complicated than a straight-forward Gaussian or L\'evy distribution can
describe - instead, one needs structured models for their description.

Therefore in this thesis we study how the elements and the eigenvalues
of the cross-correlation matrix are distributed when the matrix is
built from returns described by different returns' models, specifically
GARCH(1,1) and ARIMA-log-volatility models, whose the tail behavior is
understood. Moreover, we also study how auto-correlations in the
returns influence the aforementioned distributions -- such
auto-correlations, known as second-order auto-correlations decay
exponentially but may still leave footprints in the cross-correlation
matrix.

The rest of this report is organized as follows: in
\S\ref{sec:FundamentalConcepts} we present a few very important
concepts and notations that we will often refer to in later
chapters. Then chapter \ref{chp:PriceModels} reviews some of the most
influential returns' models, calibrates them to a few intra-day return
series and compares their performance. The unconditional distribution
functions of ARIMA-log-volatility models, which is not seen to be
given a full discussion in the literature, are also calculated and
discussed. Chapter \ref{chp:Gaussian} investigates the elements' and
eigenvalues' distributions of the Wishart matrix, where the influence
of auto-correlations in the constituent returns are studied in
detail. Chapter \ref{chp:CrossCorrelationFat} looks at the elements'
and eigenvalues' distributions 


\section{Fundamental Concepts \&
  Notations}\label{sec:FundamentalConcepts}
This section is a list of a few concepts that may be unfamiliar to the
reader and that we will often refer to later in the thesis:
\begin{itemize}
\item Return. Given a fixed time interval $[t - \Delta t, t]$, for example, a
  day, a week, a month, etc, and a particular asset, for example, a
  share in company ABC, the return of this asset over the time
  interval is defined as
  \begin{eqnarray*}
    r_{t, \Delta t} &=& \ln p_{t} - \ln p_{t - \Delta t} \\
    &=& \ln \left(1 + {p_{t} - p_{t - \Delta t} \over p_{t - \Delta t}}\right) \\
    &\approx& {p_{t} - p_{t - \Delta t} \over p_{t - \Delta t}}
  \end{eqnarray*}
  where $p_t$ is the price of the asset at time $t$. $\Delta t$ is
  sometimes called the time-lag of the return. Quite often, where
  confusion is not possible, we will just write $r_t$ to mean $r_{t,
    \Delta t}$, the time lag either does not matter or is clear from
  the context.

\item Autocorrelation. By autocorrelation, denoted $\rho_k$ here, we
  mean the correlation between two temporally separated observations
  of the same time series:
  \begin{eqnarray*}
    \rho_k &=& {
      \E\left[(a_t -\E(a_t))(a_{t-k} - \E(a_{t-k})\right]
      \over
      \sqrt{\text{var}(a_t)}\sqrt{\text{var}(a_{t-k})}
    }
  \end{eqnarray*}
  Here $\E(x)$ stands for the expectation value of $x$, and
  $\text{var}(x)$ stands for the variance of $x$. $k$ is called the
  time-lag and is the temporal seperation of the two observations
  measured by the number of observations in between. For example, the
  time-lag between the 1st and 3rd observation is 2.

\item Cross-Correlation Matrix. Correlations between the returns of a
  group of assets are described by the cross-correlation matrix. When
  the asset returns are described by a stable distribution law with
  L\'evy index $\alpha$, the empirical cross-correlation matrix is
  constructed as
  \begin{equation}
    \label{eq:cross-correlation}
    \begin{aligned}
      C_{ij} &= {1 \over T^{2/\alpha}} \sum_{t=1}^T [r_{i,t}-\E(r_i)]
      [r_{j,t}-\E(r_j)] \\
      &= {1 \over T^{2/\alpha}} RR'
    \end{aligned}
  \end{equation}
  where $r_{i,t}$ is the return of the i-th asset at time t and is
  placed at the entry (i, t) of matrix R; $R'$ denotes the
  transpose of R. In most practical situations, one has abundant data
  for each and every asset. Thus $T \geq N$ is assumed throughout this
  thesis.

\item Auto-regressive processes. A time series $r_t$ is called an
  auto-regressive process of order $p$ and denoted AR(p), if it can be
  written in the following form:
  \begin{eqnarray*}
    r_t &=& \sum_{i=1}^p \phi_i r_{t-i} + a_t
  \end{eqnarray*}
  where, for all $i$, $\phi_i \in (-1, 1)$, and the $a_t$'s are
  independent and identically distributed (iid.) random variables with
  zero mean. Obviously this implies the mean of $r_t$ is zero
  too. Apart from this, their distribution of $a_t$ is not restricted
  to any particular form.
  
  Of particular interest to this thesis is the AR(1) process:
  \begin{eqnarray*}
    r_t = \phi r_{t-1} + a_t
  \end{eqnarray*}
  Its autocorrelation function $\rho_k = \text{corr}(r_t, r_{t-k})$
  ($k = 0, 1, 2, \cdots$) can be easily shown to fall off
  exponentially:
  \begin{eqnarray*}
    \rho_k &=& {\E(r_tr_{t-k}) - \E(r_{t-k})\E(r_{t}) \over
      \sqrt{\var(r_t) \var(r_{t-k})}} \\
    &=& {\phi \E(r_{t-1}r_{t-k}) + \E(a_t r_{t-k})
      \over
      \sqrt{\var(r_t) \var(r_{t-k})}
    } \\
    &=& \phi \rho_{k-1}
  \end{eqnarray*}
  where $\E(a_t r_{t-k}) = 0$ follows from the fact that any return
  $r_{t1}$ must not depend on disturbances $a_{t2}$ that occur later
  in time. The last equation means $\rho_k$ is a geometric series. Since
  $\rho_0 = 1$, we have
  \begin{eqnarray*}
    \rho_k &=& \phi^k \\
    |\rho_k| &=& e^{k\ln|\phi|} \\
  \end{eqnarray*}
  Note that $|\phi| < 1$ and hence $\ln|\phi| < 0$.

  Although $k$ can only take integer values, it is still useful to
  define a correlation time $\tau$ such that $\phi^\tau = 1/2$. Such a
  quantity is more intuitive and constitutes a measure of
  autocorrelations that is universal and comparable among different
  time series' models. From the definition of $\tau$ we get
  \begin{equation}
    \label{eq:tau_def}
    \begin{aligned}
      \phi^\tau &= 1/2 \\
      \tau &= -{\ln 2 \over \ln\phi} \\
      \phi &= 2^{-1/\tau}
    \end{aligned}
  \end{equation}
  Figure \ref{fig:AR1-autocorrelation} shows the autocorrelation
  function of the AR(1) model. As proven above, this function decays
  exponentially.
  \begin{figure}[htb!]
    %\vspace{-15mm}
    \centering
    \includegraphics[scale=0.5, clip=true, trim=113 229 115
    139]{../pics/AR1-autocorrelation.pdf}
    \caption{\small \it Autocorrelation function of the AR(1) model
      with $\phi=1/\sqrt{2}$. Red circles: autocorrelations at $k=0, 1,
      2, \cdots$. Blue line: $e^{t\ln\phi}$.}
    \label{fig:AR1-autocorrelation}
  \end{figure}
  For more details about conventional time series' models, see
  \cite{BoxJenkins94}.
\end{itemize}

For purposes of later reference, we also list some notations that may
cause confusion to the reader:
\begin{itemize}
\item $\E(x)$ or $\mean{x}$: The expectation value of $x$.
\item $\var(x)$: the variance of $x$.
\item $\text{cov}(x,y)$: the covariance of $x$ and $y$.
\item $\text{corr}(x,y)$: the correlation between $x$ and $y$, i.e.
  \begin{equation*}
    \text{corr}(x,y) = {\text{cov}(x,y) \over \sqrt{\var(x)\var(y)}}
  \end{equation*}
\end{itemize}

\section{Stylized facts}\label{sec:StylizedFacts}
This section presents and explains a few ``stylized facts'',
i.e. phenomena that are widely observed and accepted as true.
\begin{itemize}
\item Fat tails. It has been consistently reported by various studies
  - for example \cite{Potters2003} and \cite{Mantegna2000} - that the
  probability density function (PDF) of stock/index returns are not
  Gaussian. Unlike the Gaussian PDF, which is symmetric and falls off
  very quickly as its argument moves from the center to the outskirts
  (tails), the PDF of stock/index returns are higher on the tails,
  i.e. the probability of large fluctuations is higher than is dictated
  by a Gaussian distribution - in fact, even higher than dictated by
  an exponential function. Empirical studies suggest the distribution
  function of the returns follows a power-law on the tails --- the
  exponent of the power depends on the specific stock/index.

  Figure \ref{fig:FatTail} illustrates this feature.
  \begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.6, clip=true, trim=92 229 110
    140]{../pics/FatTail.pdf}
    \caption{\small \it Fat tails of {\it Nordea Bank} 15-minute
      returns during the period 2013-10-10 and 2014-01-29. The returns
      are computed using minute-by-minute average prices.}
    \label{fig:FatTail}
  \end{figure}

\item Non-zero skewness. Apart from fat tails, the PDF of stock/index
  returns are often also skewed. If the skewness is positive
  (negative), the probability of very large positive (negative)
  returns is higher than that of very large negative (positive)
  returns, even though the mean of the returns is 0 or extremely
  close to 0.

  Table \ref{tab:EmpiricalSkewness} lists the skewness of a few
  Swedish stocks traded on the Stockholm OMX market.
  \begin{table}[htb!]
    \footnotesize
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|}
      \hline
      Nordea Bank & Volvo B & Boliden & ABB Ltd & H\&M & Scania
      B & Ericsson B \\
      \hline
      0.1362 & 0.0471 & 0.0567 & -0.0364 & -0.0168 &
      -1.1554 & 0.2086 \\
      \hline
    \end{tabular}
    \caption{\small \it Skewness of Stock Returns. All the returns
      have time-lag of 15 minutes and are computed using paid prices
      between 2013-10-10 and 2014-01-29. }
    \label{tab:EmpiricalSkewness}
  \end{table}

\item Higher-order autocorrelation. To the lowest order, return series
  are not auto-correlated --- if they are, the auto-correlations would
  present an obvious opportunity of making easy profit and be
  exploited and vanish as soon as they appear. However, the squared
  returns do have significant auto-correlations, as figure
  \ref{fig:nordea_15min_acf} illustrates. These auto-correlations
  suggest the variances of the returns at subsequent time steps are
  correlated. These are referred to as higher-order auto-correlations
  and make a subjet of returns' models.
  
\end{itemize}

\input{price_models}
\input{cross_correlation_gaussian}

\chapter{Covariance Matrix of GARCH(1,1) Returns}
\label{chp:CrossCorrelationFat}

% In this chapter we study the cross-correlation matrix $C$ constructed from
% returns that have a fat-tailed distribution, i.e.
% \begin{eqnarray*}
%   C &=& {1 \over T^{2/\alpha}} RR'
% \end{eqnarray*}
% where $R$ is an $N\times T$ returns' matrix whose entries are
% \begin{eqnarray}\label{eq:CrossCorrelationMatrix}
%   R_{it} &=& \left\{
%   \begin{array}{ll}
%     a_{it} & \text{if $t=1$} \\
%     \phi R_{i, t-1} + a_{it} & \text{if $t>1$} \\
%   \end{array} \right.
% \end{eqnarray}
% Here $a_{it}$ are random variables drawn from a stable distribution
% with l\'evy index $\alpha$. In \S\ref{sec:LevyWishart} we look at
% the case where the stable distribution is chosen to be Cauchy; in
% \S\ref{sec:XieWishart} we look at the case where $a_{it}$ is
% described by an ARIMA log-volatility model, which is detailed in
% \S\ref{sec:SLV_model}; finally in \S\ref{sec:GarchWishart} we
% investigate the case where $a_{it}$ is a GARCH(1, 1) process, which is
% proven to have regularly varying tails by Mikosch et al
% \cite{mikosch2000}.

% \section{Wishart-L\'evy Matrices}\label{sec:LevyWishart}
% In this section we study the case where the distribution of $a_{it}$
% as appear in equation \ref{eq:CrossCorrelationMatrix} is chosen to be
% Cauchy, a stable distribution with L\'evy index $\alpha =
% 1$. Politi et al derived the eigenvalue distribution of Wishart-L\'evy
% matrices in \cite{politi2010} using free probability theory. Here we
% show, qualitatively, how the distribution changes when
% auto-correlations are present among the return series. To be precise,
% the PDF of $a_{it}$ is
% \begin{eqnarray*}
%   f_a(x) &=& {\gamma \over \pi} {1 \over \gamma^2 + x^2}
% \end{eqnarray*}

% By numerically simulating a large number of cross-correlation matrices
% and solving for the eigenvalues of each simulated matrix, one can
% obtain the eigenvalue distribution corresponding to each value of
% $\phi$. Figure \ref{fig:WishartLevySpectraPositive} shows the
% distribution for a range of positive $\phi$ values, and figure
% \ref{fig:WishartLevySpectraNegative} shows those for a range of
% negative $\phi$ values.
% \begin{figure}[htb!]
%   \centering
%   \subfigure[]{
%     \includegraphics[scale=0.35, clip=true, trim=94 223 109
%     134]{../pics/WishartLevyEigDist.pdf}
%     \label{fig:WishartLevySpectraPositive}
%   }
%   \subfigure[]{
%     \includegraphics[scale=0.35, clip=true, trim=94 223 109
%     134]{../pics/WishartLevyNegEigDist.pdf}
%     \label{fig:WishartLevySpectraNegative}
%   }
%   \caption{\small \it Eigenvalue distribution (PDF) of Wishart-L\'evy
%     Matrices. Horizontal axis: eigenvalues ($\lambda$) on log-scale;
%     Vertical axis: PDF ($f_\phi(\lambda)$) of the eigenvalues on
%     log-scale. \ref{fig:WishartLevySpectraPositive}: $\phi > 0$;
%     \ref{fig:WishartLevySpectraNegative}: $\phi < 0$;  The $|\phi|$
%     values 0, 0.5, 0.8, and 0.99 correspond to $\tau$ (correlation
%     time) values 0, 1.0, 3.10, and 68.97.  The correlation time is
%     independent of the sign of $\phi$.}
% \end{figure}

% It is seen in these figures
% \begin{itemize}
% \item the eigenvalue distribution is identical for $\phi$ and
%   $-\phi$;
% \item the distribution is not bounded within a finite interval but rather
%   extends to infinity, in constrast to the Marcenko-Pastur law that
%   bounds the maximum eigenvalue at $\sigma^2(1 + \sqrt{N/T})^2$;
% \item as auto-correlation increases, the distribution flatens,
%   implying probability being transferred to large eigenvalues.
% \end{itemize}

% \section{Cross-Correlation Matrix of ARIMA Log-volatility
%   returns}\label{sec:XieWishart}
% In this section we study the cross-correlation matrix of return series
% specified as follows:
% \begin{equation}
%   \label{eq:CrossCorrSLV1}
%   \begin{aligned}
%     R_{it} &= \phi R_{i, t-1} + a_{it} \\
%     a_{it} &= e^{v_{it}} b_{it}
%   \end{aligned}
% \end{equation}
% where $v_{it}$ is typically described by an ARIMA model as discussed
% in \S\ref{sec:SLV_model}. The unconditional distribution of
% such returns is calculated in \S\ref{sec:XieCalc}, where it is shown
% that all the moments of the distribution are finite. Based on this
% observation, we expect the Marcenko-Pastur law to emerge for the
% eigenvalue distribution of the cross-correlation matrix C, when
% $\phi=0$, i.e. when no auto-correlations are absent. In this case C is
% built as
% \begin{eqnarray*}
%   C &=& {1 \over T} \sum_{t=1}^T (DR)(DR)' \\
% \end{eqnarray*}
% where $R'$ denotes the transpose of $R$ and
% \begin{eqnarray*}
%   D &=&
%   \begin{pmatrix}
%     1/\sigma_1 & 0 & \cdots & 0 \\
%     0 & 1/\sigma_2 & \cdots & 0 \\ 
%     \vdots & \vdots & \ddots & 0 \\
%     0 & 0 & \cdots & 1/\sigma_N
%   \end{pmatrix}
% \end{eqnarray*}
% $\sigma_i$ is the standard deviation of return series $i$. By
% numerical simulation as described above, the eigenvalue distribution
% of C is obtained. We plot it in figure \ref{fig:FatWishartSpectra}.
% \begin{figure}[htb!]
%   \centering
%   \vspace{-20mm}
%   \includegraphics[scale=0.5, clip=true, trim=91 229 116
%   137]{../pics/FatWishartEigDist.pdf}
%   \caption{\small \it Eigenvalue distribution (PDF) of a
%     cross-correlation matrix with returns described by
%     \ref{eq:CrossCorrSLV1}. $\phi$ = 0, 0.5, 0.8, 0.95 correspond to
%     correlation time $\tau$ = 0.00, 1.00, 3.11, 13.51}
%   \label{fig:FatWishartSpectra}
% \end{figure}
% As expected, the case with zero auto-correlation has an eigenvalue
% distribution governed by the Marcenko-Pastur law. As auto-correlations
% become stronger, the distribution deforms, but in a rather different
% way if compared to the case of Gaussian returns. Figure
% \ref{fig:FatWishartSpectra} shows that the minimum eigenvalue
% decreases with increasing $\phi$ in the present case of ARIMA
% log-volatility returns, while figure \ref{fig:Gaussian_mineig} shows
% that, in the case of Gaussian returns, the minimum eigenvalue actually
% increases with $\phi$. We leave this interesting comparison to future
% investigations.

% \section{Cross-Correlation Matrix of GARCH(1,1) returns}
% \label{sec:GarchWishart}
GARCH models and particularly GARCH(1,1) models are widely used to
model financial return series of various time scales, ranging from
daily and monthly returns that have been studied extensively in the
literature to intraday returns that we have selectively investigated
in chapter \ref{chp:PriceModels}. One particularly nice feature of
GARCH(1,1) models is that they have regularly varying tails (power-law
tails) \cite{Mikosch2009, mikosch2000} even when the innovations
(denoted $z_t$ in the following text) are normally distributed ---
something not shared by other classes of models, e.g. not by ARIMA
log-volatility models, as shown in \S\ref{sec:XieCalc}. However, what
this tail behavior implies for the covariance matrix is much less
understood, especially when the constituent return series are
auto-correlated.

So in this chapter we consider the covariance matrix of N
identically specified, possibly auto-correlated GARCH(1, 1)
processes:
\begin{eqnarray}
  r_{it} &=& \phi r_{i, t-1} + \epsilon_{it} \nonumber \\
  \epsilon_{it} &=& \sigma_{it} z_{it} \label{eq:garch_spec}
\end{eqnarray}
where $i=1,2,...,N$; $t=1,2,...,T$; $z_{it}$ is independent,
identically distributed, and
\begin{eqnarray*}
  \sigma_{it}^2 &=& \alpha_0 + \alpha_1 z_{i, t-1}^2 + \beta_1
  \sigma_{i,t-1}^2
\end{eqnarray*}
Mikosch et al showed in \cite{mikosch2000} that a GARCH(1,1) process
satisfying
\begin{eqnarray*}
  \alpha_0 &>& 0 \\
  \E(\alpha_1 Z^2 + \beta_1) &<& 0 \\
\end{eqnarray*}
and
\begin{eqnarray*}
  \E[(\alpha_1 z^2 + \beta_1)^{p/2}] &\geq& 1 \\
  \E|Z|^p \ln|Z| &\leq& \infty
\end{eqnarray*}
for some $p > 0$, is stationary and has regularly varying tails. The
tail exponent $\alpha$ is determined by:
\begin{equation}\label{eq:garch_alpha}
  \E[(\alpha_1 Z^2 + \beta_1)^{\alpha/2}] = 1
\end{equation}
Here $Z$ is a random variable that has the same distribution as
$z_{it}$. In our simulations described hereafter $z_{it}, Z \sim N(0,
1)$. In this section and the next, we first study situations where no
auto-correlations are present among the returns, i.e. $\phi = 0$; then
in \S\ref{sec:garch_nonzero_phi} we look at how auto-correlations
change the picture.

With regularly varying tails, the eigenvalue distribution of a
cross-correlation matrix built from GARCH(1,1) returns is expected to
differ from the Marcenko-Pastur law. In figure
\ref{fig:garch_ev_cii} we simulate N=50 independent GARCH(1,1)
returns series, each with identical parameters, namely $\alpha_0 =
2.3\times 10^{-6}$, $\alpha_1 = 0.15$, $\beta_1 = 0.84$, $\phi = 0$
and T=$8\times10^4$ time steps, then we build the cross-correlation
matrix as
\begin{eqnarray*}
  C &=& {1 \over T^{2/\alpha}} RR'
\end{eqnarray*}
where R is an $N\times T$ matrix, with elements $r_{it}$ specified by
\ref{eq:garch_spec}. The PDF of the eigenvalue distribution of C is
plotted in figure \ref{fig:garch_ev_cii_linear} and CDF of the
distribution is plotted on log-log scale in
\ref{fig:garch_ev_cii_loglog}.
\begin{figure}[htb!]
  \centering
  \subfigure[]{
    \includegraphics[scale=0.40, clip=true, trim=104 271 111
    216]{../pics/garch_ev_cii.pdf}
    \label{fig:garch_ev_cii_linear}
  }
  \subfigure[]{
    \includegraphics[scale=0.40, clip=true, trim=111 241 110
    134]{../pics/garch_ev_cii_loglog.pdf}
    \label{fig:garch_ev_cii_loglog}
  }
  \caption{\small \it \ref{fig:garch_ev_cii_linear}: Eigenvalues \&
    Diagonal elements' distribution of a cross-correlation matrix
    built from independent GARCH return series. Blue: PDF of
    eigenvalues; Green: PDF of diagonal elements; Red: PDF of a
    $\alpha$-stable distribution fitted to the diagonal
    elements. \ref{fig:garch_ev_cii_loglog}: CDF of the same
    quantities in 10-based log-log scale.}
  \label{fig:garch_ev_cii}
\end{figure}
Also plotted in the same figure is the distribution of the diagonal
elements of C. It is clear from the figure that the two PDFs coincide,
implying C is diagonal. This is further confirmed by figure
\ref{fig:garch_cij} which shows the distribution of the non-diagonal
elements of C. One can see the non-diagonal elements are distributed
symmetrically around 0 with a very small width in comparison to the
distribution of the diagonal elements - in fact, 1 order of magnitude
smaller ($2.14\times10^{-5}$ v.s. $7.31\times 10^{-4}$). Hence $C$ is
very close to a diagonal matrix.
\begin{figure}[htb!]
  \centering
    \includegraphics[scale=0.5, clip=true, trim=95 253 91
    223]{../pics/garch_cij.pdf}
  \caption{\small \it Distribution of the non-diagonal elements of
    the cross-correlation matrix. Blue: PDF of the non-diagonal
    elements; Green: $\alpha$-stable distribution fitted to the
    non-diagonal elements' PDF.}
  \label{fig:garch_cij}
\end{figure}

Figure \ref{fig:garch_ev_cii} also shows the two curves are well fitted
by an $\alpha$-stable distribution, with $\alpha \approx 1.38$. This
is really an expected result for the diagonal elements. Using
$\alpha_1 = 0.15$, $\beta_1 = 0.84$, which are the values used for
simulating the GARCH returns, one can obtain, by solving equation
\ref{eq:garch_alpha}, $\alpha=2.96$. Then according to Mikosch et al
\cite{mikosch2000}
\begin{eqnarray*}
  P(|r_t| > x) &\sim& {\E(z^\alpha) c_0 \over x^\alpha} \\
  P(|r_t|^2 > x) &\sim& {\E(z^\alpha) c_0 \over x^{\alpha/2}} \\
\end{eqnarray*}
for some constant $c_0$. Now that $P(|r_t|^2 > x)$ has power-law tail
behavior with power $\alpha/2 < 2$, one can deduce
\begin{equation}
  \label{eq:stable_CLT}
  \begin{aligned}
    \sum_{t=1}^T r_t^2 &\xrightarrow{d} S(\alpha/2,
    1, \gamma, \mu) \text{ as $T \to \infty$}
  \end{aligned}
\end{equation}
where $\xrightarrow{d}$ denotes convergence in distribution, and
$S(\alpha/2, 1, \gamma, \mu)$ denotes an $\alpha$-stable distribution
with parameters $(\alpha/2, 1, \gamma, \mu)$. Here $\alpha/2$ is the
L\'evy index, 1 is the asymmetry, $\gamma$ is the scale parameter
and $\mu$ is the mean value of the distribution. Asymmetry being 1
means a random variable so distributed only takes positive values
\cite{Bilik2008, Embrechts1997}.

The mean $\mu$ in equation \ref{eq:stable_CLT} is given by
\begin{eqnarray*}
  \mu &=& T \E(|r_t|^2) \\
  &=& T{\alpha_0 \over 1 - \alpha_1 - \beta_1}
\end{eqnarray*}
and the scale parameter $\gamma$ in \ref{eq:stable_CLT} is determined
by the limit \cite{Bilik2008}
\begin{eqnarray*}
  \lim_{T\to\infty} {T \E(z^{\alpha/2}) c_0 \over \gamma^{\alpha/2}}
  &=& C_{\alpha/2}
\end{eqnarray*}
where
\begin{eqnarray*}
  C_{\alpha/2} &=& \left( \int_0^\infty {\sin x \over x^{\alpha/2}} dx
  \right)^{-1} \\
  &\approx& {1 \over \sqrt{2 \pi}}
\end{eqnarray*}
Therefore
\begin{eqnarray*}
  \gamma^{\alpha/2} &=& \sqrt{2\pi} T \E\left(
    |z|^{\alpha/2}
  \right) c_0 \\
  \gamma &=& (2\pi)^{1/\alpha} T^{2/\alpha} \E\left(
    |z|^{\alpha/2}
  \right)^{2/\alpha} c_0^{2/\alpha}
\end{eqnarray*}
Here we note that an $\alpha$-stable distribution $S(\alpha, \beta,
\gamma, \mu)$ has characteristic function \cite{Guhr2007}
\begin{eqnarray*}
  \phi(k; \alpha, \beta, \gamma, \mu) &=& \exp\left[
    i\mu k - \gamma^\alpha |k|^\alpha \left(
      1 - i \beta {k \over |k|} \tan{\pi \alpha \over 2}
    \right) \right] \text{ for $\alpha \neq 1$}
\end{eqnarray*}
from which we see $\phi(ak; \alpha, \beta, \gamma, \mu) = \phi(k;
\alpha, \beta, a\gamma, a\mu)$, implying that, if $x \sim S(\alpha,
\beta, \gamma, \mu)$, then $ax \sim S(\alpha, \beta, a\gamma, a\mu)$.

Now that
\begin{eqnarray*}
  \sum_{t=1}^T r_t^2 &\xrightarrow{d} S(\alpha/2,
  1, \gamma, \mu) \text{ as $T \to \infty$}  
\end{eqnarray*}
we have
\begin{eqnarray*}
  C_{ii} &=& {1 \over T^{2/\alpha}}\sum_{t=1}^T r_{it}^2 \\
  &\xrightarrow{d}&
  S(\alpha/2, 1, \gamma_D, \mu_D) \text{ as $T \to \infty$}
\end{eqnarray*}
where
\begin{eqnarray}\label{eq:garch_wishart_cij_params}
  \gamma_D &=& (2\pi)^{1/\alpha} \E\left(|z|^{\alpha/2}
  \right)^{2/\alpha} c_0^{2/\alpha} \\
  \mu_D &=& {\alpha_0 \over 1 - \alpha_1 - \beta_1} T^{1 - 2/\alpha}
  \nonumber
\end{eqnarray}
So the diagonal elements of the cross-correlation matrix converge to
an $\alpha$-stable distribution with L\'evy index $\alpha/2 \approx
1.48$. This is consistent with the measured index 1.38 within
measurement errors.

Now we look at the distribution of the non-diagonal elements. At first
glance, one might be tempted to think that figure \ref{fig:garch_cij}
shows a Gaussian distribution, but in fact, it is not. This is
illustrated in the probability plot in figure
\ref{fig:garch_nondiag_probplot}. In comparison to the PDF of a
Gaussian, the PDF of the nondiagonal elements is significantly fatter
on the tails.
\begin{figure}[htb!]
  \centering
  \includegraphics[scale=0.5, clip=true, trim=81 229 114
    121]{../pics/garch_cij_prob.pdf}
    \caption{\small \it Probability plot of the non-diagonal
      elements. Blue: The accumulative probability function
      (CDF) of the non-diagonal elements. Black, dashed: CDF of the
      Gaussian distribution that has the same mean and variance as the
      sample. The graph is arranged on such a scale that the Gaussian
      CDF is a straight line.
    }
  \label{fig:garch_nondiag_probplot}
\end{figure}

The parameters of the non-diagonal elements' distribution $S(\alpha',
\beta', \gamma', \mu')$ can be obtained via fitting. The results have
been shown in the legend of figure \ref{fig:garch_cij}. In table
\ref{tab:garch_wishart_cij_params} we list them with a higher
precision.
\begin{table}[htb!]
  \centering
  \begin{tabular}{|c|c|c|c|}
    \hline
    $\alpha'$ & $\beta'$ & $\gamma'$ & $\mu'$ \\
    \hline
    1.9453 & 0.0018 & $2.1381 \times 10^{-5}$ & $2.0637\times 10^{-9}$ \\
    \hline
  \end{tabular}
  \caption{\small \it Parameters of the non-diagonal elements' distribution}
  \label{tab:garch_wishart_cij_params}
\end{table}
Since $r_{it}$ and $r_{jt}$ are independent of each other, $\beta'$
and $\mu'$ are expected to be 0 --- but with a finite T, some deviation
from 0 is not surprising.

Now that the parameters' values have been obtained, the way they scale
with T, i.e. the length of the return series, can be deduced. Consider
\begin{equation}\label{eq:garch_cij1}
  T^{2/\alpha} C_{ij} = \sum_{t=1}^T r_{it} r_{jt} \xrightarrow{d} S(\alpha',
  0, \gamma', 0)
\end{equation}
where the width $\gamma'$ is determined by \cite{Bilik2008},
\begin{eqnarray*}
  \lim_{T\to\infty} {T C \over \gamma'^{\alpha'}}
  &=& C_{\alpha'} \\
  C_{\alpha'} &=& \left( \int_0^\infty {\sin x \over x^{\alpha'}} dx
  \right)^{-1} \\
\end{eqnarray*}
Here the constant C is such that
\begin{equation*}
  P(|r_{it}r_{jt}| > x) \sim {C \over x^{\alpha'}} \text{ as $x \to \infty$}
\end{equation*}
So we have
\begin{eqnarray*}
  \gamma' &=& {\left( CT \over C_{\alpha'}\right)^{1/\alpha'}}
\end{eqnarray*}
Divide throughout equation \ref{eq:garch_cij1} by $T^{2/\alpha}$ then
gives
\begin{equation}\label{eq:garch_cij2}
  C_{ij} = {1 \over T^{2/\alpha}}\sum_{t=1}^T r_{it} r_{jt}
  \xrightarrow{d}
  S(\alpha', 0, \gamma_N, 0)
\end{equation}
where
\begin{eqnarray*}
  \gamma_N &=& {\left(C \over C_{\alpha'}\right)^{1/\alpha'}}
  T^{1/\alpha'-2/\alpha} \\
\end{eqnarray*}
So we see that distribution of the non-diagonal elements has a width
that scales with T as $T^{1/\alpha' - 2/\alpha} \approx T^{-1/6}$,
while the width of the diagonal elements' distribution, as shown in
equation \ref{eq:garch_wishart_cij_params}, does not scale with
T. This is to be compared with the Wishart case where the return
series have Gaussian distribution, and hence the asymptotic
distributions of both the diagonal and the non-diagonal elements of the
covariance matrix are Gaussian with a variance that scales as $1/T$
(c.f. \ref{sec:GCC-analytical}).

% We also notice that, given our particular choice of $\alpha_1$ and
% $\beta_1$, the covariance matrix's convergence to a diagonal matrix
% is slow, since the non-diagonal elements decay to zero only 

% In the Wishart case, the diagonal and the non-diagonal elements
% scale in the same way with T and thus have comparable sizes at large
% T --- so the matrix is not diagonal and the eigenvalue distribution has
% the Marcenko-Pastur law. In the GARCH case, the diagonal elements do
% not scale with T while the non-diagonal elements scale as $T^{-1/6}$,
% thus at large T, the non-diagonal elements are significantly smaller
% than the diagonal ones --- so a diagonal matrix arises.

\section{Implications of Finite Number of Observations}
In the last section we have discussed the limiting situation where the
return series of the covariance matrix have an infinite number of
observations ($T \to \infty$), and in the simulation studies we have
generated a large number of observations for each return series,
namely $T = 8 \times 10^4$. However, in practice, one often does not
have such a large number of observations available. Therefore, it is
useful to investigate situations where $T$ only has a modest size.

Figure \ref{fig:GarchCiiEig1} shows the diagonal elements' as well as
the eigenvalues' distributions when the number of return series (N) is
fixed at 250 and the number of observations in each series (T) is
increased from 3000 to 6000.
\begin{figure}[htb!]
  \centering
    \includegraphics[scale=0.5, clip=true, trim=81 248 75
    195]{../pics/GarchCiiEig1.pdf}
  \caption{\small \it The Diagonal elements' and the eigenvalues'
    distributions with modest T. Blue: eigenvalues' CDF;
    Green: Diagonal elements' CDF; Red: CDF of the $\alpha$-stable
    distribution fitted to the diagonal elements. All the curves are
    drawn on 10-based log-log scale.
  }
  \label{fig:GarchCiiEig1}
\end{figure}
It is seen from the figure that, compared to the earlier case where $T
= 8 \times 10^4$, the eigenvalue distribution and the diagonal
elements' distribution do not coincide as well but differ rather
siginificantly for small values. For large values, however, the two do
coincide and comply with the fitted $\alpha$-stable
distribution. Moreover, the difference between the eigenvalues'
distribution and the diagonal elements' distribution, as well as that
between the diagonal elements' distribution and the fitting
$\alpha$-stable distribution, is also seen to diminish as T
increases.

The convergence of the eigenvalues ($\lambda$) towards the diagonal
elements $C_{ii}$ as $\lambda \to \infty$ is really an anticipated
result. For convenience, we order the diagonal elements so that
\begin{eqnarray*}
  C_{11} < C_{22} < \cdots < C_{NN}
\end{eqnarray*}
where, as before, $N$ is the dimension of the covariance matrix
$C$. Then, as $x \to \infty$,
\begin{eqnarray*}
  P(C_{ii} > x) &\sim& {c \over x^{\alpha/2}} \\
  f(C_{ii}) &\sim& {c \over x^{\alpha/2 + 1}} \\
\end{eqnarray*}
where $f(\cdot)$ denotes the PDF of the diagonal elements and $c$ is
some constant. Thus, at the limit $C_{ii} \to \infty$, the distance
between two adjacent diagonal elements $C_{ii}$ and $C_{i+1, i+1}$ can
be expressed as
\begin{eqnarray*}
{1 \over N (C_{i+1, i+1} - C_{ii})} &=& f(C_{ii})\\
C_{i+1, i+1} - C_{ii} &\sim& {C_{ii}^{\alpha/2 + 1} \over c N}
\end{eqnarray*}

Thus as $C_{ii} \to \infty$, $C_{i+1, i+1} - C_{ii} \to \infty$ while
$C_{ij} \to 0$ ($i \neq j$). Now that the spacing between adjacent
diagonal elements become wider and the non-diagonal elements become
smaller, the eigenstates considered as a mixture of the basis states,
become more and more localized to a prominent basis state. This
localization can be measured by the size of the component in each
eigenvector that has the largest absolute value($|c|_\M$), provided
that the eigenvectors have been normalized.

Figure \ref{fig:garch_eigenvec_Cmax} shows how $|c|_\M$ changes
in response to increasing $\lambda$. It is seen that as T increases,
localization of the eigenstates proceeds from those with very
large eigenvalues towards those with relatively smaller
eigenvalues. At the same time, the minimum of $|c|_\M$ increases
and so does the minimum of the eigenvalues. The last point here is
further illustrated in figure \ref{fig:garch_eigenvec_Cmax_dist} where
the PDF of $|c|_\M$ is plotted. We see in this figure that an
increased value of T leads to advancement of $\min(|c|_\M)$ to larger
values as well as to an increased proportion of large $|c|_\M$. The
mean of $|c|_\M$ has apparently been increased too.
\begin{figure}[htb!]
  \centering
  \includegraphics[scale=0.5, clip=true, trim=14 199 31
  155]{../pics/garch_eigenvec_Cmax.pdf}
  \caption{\small \it largest component ($|c|_\M$) corresponding to
    the eigenvalue. N is fixed to 50.}
  \label{fig:garch_eigenvec_Cmax}
\end{figure}

\begin{figure}[htb!]
  \centering
  \includegraphics[scale=0.5, clip=true, trim=86 257 112
  136]{../pics/garch_eigenvec_Cmax_dist.pdf}
  \caption{\small \it PDF of the largest component ($|c|_\M$) of
    each eigenvector. N is fixed to 50.}
  \label{fig:garch_eigenvec_Cmax_dist}
\end{figure}

Another informative quantity that measures the localization is the
``Inverse Participation Ratio'' (IPR). For a given normalized eigenvector
$\vec{c}_i = (c_{1, i}, c_{2, i}, ..., c_{N, i})$, the IPR is defined
as \cite{Aberg2013}
\begin{eqnarray*}
  \text{IPR}(\vec{c}_i) &=& \sum_{k=1}^N c_{k,i}^4 \\
\end{eqnarray*}
Figure \ref{fig:garch_eigenvec_PR} shows ${1 \over
  N \cdot \text{IPR}(\vec{c}_i)}$ in correspondence to the
eigenvalues. This quantity is sometimes termed the
normalized participation ratio (PR) and measures the proportion of
basis vectors that contribute considerably to the eigenvector in
question. From this figure we see that, for all values of T, if an
eigenvalue is larger than $10^{-1.5} \approx 0.03$, its corresponding
participation ratio is less than $10^{-1.6} = 2.5\%$, meaning less
than $50 \times 0.025 = 1.26$ basis vectors contribute --- each of the
corresponding eigenvectors is localized to a single basis vector and
hence the distribution of such large eigenvalues is the same as the
diagonal elements' distribution.

Figure \ref{fig:garch_eigenvec_PR_dist} shows the PDF of the
normalized PR. We see that as T increases, the distribution of PR is
compressed towards 0, suggesting increased localization of the
eigenvectors.
\begin{figure}[htb!]
  \centering
  \includegraphics[scale=0.5, clip=true, trim=49 208 70
  162]{../pics/garch_eigenvec_PR.pdf}
  \caption{\small \it Normalized participation ratio (PR)
    versus eigenvalue.}.
  \label{fig:garch_eigenvec_PR}
\end{figure}

\begin{figure}[htb!]
  \centering
  \includegraphics[scale=0.5, clip=true, trim=97 259 113
  226]{../pics/garch_eigenvec_PR_dist.pdf}
  \caption{\small \it PDF of the normalized PR. N is fixed to
    50.}
  \label{fig:garch_eigenvec_PR_dist}
\end{figure}
In conclusion, localization of the eigenvectors, which implies 
coincident eigenvalue and diagonal elements' distributions, begins with
those associated to large eigenvalues. Increased observation points
lead to increased localization and hence increased coincident sections
of the eigenvalue and diagonal elements' distributions. However, this
increment with T is slow, because the diagonal elements mean $\mu_D$
increases only as a fractional power of $T$, namely $T^{1 -
  2/\alpha}$, and the non-diagonal elements' variance decreases only
as a fractional power too, namely $T^{1/\alpha' - 2/\alpha}$. These
have been detailed in equations \ref{eq:garch_wishart_cij_params} and
\ref{eq:garch_cij2}.

\section{Influence of auto-correlations}
\label{sec:garch_nonzero_phi}
In the previous two sections we have studied situations where
$\phi=0$ in the specification \ref{eq:garch_spec}, i.e. no
auto-correlation is in the returns. In this section we investigate how
auto-correlations change the picture.

% When auto-correlations are present among the returns, the
% cross-correlation matrix is expected to change. How exactly it changes
% is the subject of this section. Here we consider a model specified
% as follows:
% \begin{eqnarray*}
%   r_{it} &=& \phi r_{i, t-1} + \epsilon_{it} \\
%   \epsilon_{it} &=& \sigma_{it} z_{it} \\
%   \sigma_{it}^2 &=& \alpha_0 + \alpha_1 \epsilon_{i, t-1}^2 + \beta_1
%   \sigma_{i, t-1}^2 \\
%   z_{it} &\sim& N(0, 1)
% \end{eqnarray*}
% where $r_{it}$ is the element of the R matrix at the $i$-th row and the
% $t$-th column. In the simulations described below R has N=50 rows and
% $T = 8 \times 10^4$ columns. The parameters $\alpha_1$ and $\beta_1$
% are the same as in the previous case of zero auto-correlations, namely
% $\alpha_1 = 0.15$ and $\beta_1 = 0.84$. The cross-correlation matrix C
% is built from R using
% \begin{eqnarray*}
%   C &=& {1 \over T^{2/\alpha}} RR'
% \end{eqnarray*}
% where $\alpha$ is 2.96 as before.

Figure \ref{fig:GarchEigDiag1} shows the eigenvalue as well as the
diagonal elements' distribution when $\phi = 0.95$, i.e. $\tau =
13.51$. The values of N and T are 50 and $8\times 10^4$ as before. The
GARCH(1,1) parameters are also unchanged, namely $\alpha_0 = 2.3\times
10^{-6}$, $\alpha_1 = 0.15$, and $\beta_1 = 0.84$. Figure
\ref{fig:GarchNondiag1} shows the non-diagonal elements' distribution
in the same setup. Included in these plots are $50 \times 2000 =
1\times 10^5$ eigenvalues and diagonal elements, as well as ${50
  \choose 2} \times 2000 = 2,450,000$ non-diagonal elements. These
data come from 2000 simulated matrices.
\begin{figure}[htb!]
  \centering
  \subfigure[]{
    \includegraphics[scale=0.42, clip=true, trim=112 272 103
    217]{../pics/GarchEigDiag1.pdf}
    \label{fig:GarchEigDiag1}
  }
  \subfigure[]{
    \includegraphics[scale=0.42, clip=true, trim=105 272 101
    213]{../pics/GarchNondiag1.pdf}
    \label{fig:GarchNondiag1}
  }
  \caption{\small \it \ref{fig:GarchEigDiag1}: Eigenvalues' and
    diagonal elements' distribution when $\phi = 0.95$, i.e. $\tau$ =
    13.51; \ref{fig:GarchNondiag1}: Non-diagonal elements'
    distribution in the same situation.}
\end{figure}

From figure \ref{fig:GarchEigDiag1} we see that, as auto-correlations
become significant, the distribution of the eigenvalues no longer
coincides with the diagonal elements' distribution --- instead it
becomes wider and fatter on the tails. We also notice that the widths
of both the diagonal and the non-diagonal elements' PDF's have
increased.  In figure \ref{fig:garch_ev_cii} we see that, when no
auto-correlation is present, the PDF of the diagonal elements has
width ($\gamma$) $7.31 \times 10^{-4}$, while in figure
\ref{fig:GarchEigDiag1} we see that the width has become $7.91 \times
10^{-3}$ as $\tau$ becomes 13.51. Similarly the non-diagonal elements'
PDF has width $2.14 \times 10^{-5}$ when $\tau=0$, as shown in figure
\ref{fig:garch_cij}, and this width becomes $9.60 \times 10^{-4}$ when
$\tau = 13.51$, as shown in figure \ref{fig:GarchNondiag1}.

Figure \ref{fig:GarchSpectrumAutocorrelated} shows the eigenvalues'
distribution corresponding to a range of $\phi$ values. The number of
eigenvalues in each curve is the same as in figure
\ref{fig:GarchEigDiag1}.
From this figure one can see that, as auto-correlation strengthens,
\begin{itemize}
\item the PDF of the eigenvalue distribution flattens and widens;
\item the minimum as well as the maximum eigenvalues increase.
\end{itemize}

To find out more about this series of deformation, we first look at
how the largest component and the normalized participation ratio of
the eigenvectors change as $\phi$ takes on larger values. Figure
\ref{fig:garch_eigenvec_Cmax_corr} shows the largest eigenvector
component $|c|_\M$ in correspondence to the eigenvalue. Apparently, as
auto-correlation strengthens, the eigenvectors' composition
fractures, leading to a reduced degree of localization and even
reduced certainty of localization --- for a fixed eigenvalue, $|c|_\M$
now varies in a larger range than it does with smaller $\phi$.

The same story of reduced localization is also evident from the plot
of the normalized participation ratio (PR), shown in figure
\ref{fig:garch_eigenvec_PR_dist_corr}, and from the PDF of $|c|_\m$
shown in figure \ref{fig:garch_eigenvec_Cmax_dist_corr}. It is seen
in \ref{fig:garch_eigenvec_PR_dist_corr} that the peak at the left 
of the plot, representing the group of localized eigenvectors, falls
with increased auto-correlation, and essentially disappears when
$\phi$ reaches the extreme value 0.99. Figure
\ref{fig:garch_eigenvec_Cmax_dist_corr} shows the proportion of large
$|c|_\M$ values is severely reduced and the mean of $|c|_\M$ is pushed
to smaller values by increased auto-correlation.

It is also useful to look at how the fraction of localized eigenvectors
changes with the auto-correlation. For definiteness, we classify
an eigenvector as being localized when (1) its number of participating
basis vectors is less than 2, or (2) the largest of its components'
absolute values is larger than 0.9.

Figure \ref{fig:localization_ratio} shows how the ratio of localized
eigenvectors depends on the auto-correlation strength $\phi$. In
either way of classification, the ratio falls with $\phi$ in
accordance with a power law, the power exponent lying a bit below 2.
This is further confirmed in plot \ref{fig:localization_ratio2}, where
the ratios are plotted versus $\phi$ on log-log scale.

% \subsection{Correlation Matrix of a Factor Model}
%% Result of normalizing by 1/T
% In this section we consider the estimation of correlation matrix in
% the text of a factor model: the observed return series
% $\{r_{it}\}$ ($1 \leq i \leq N$, $1 \leq t \leq T$) are driven by a
% number of unobserved and uncorrelated random variables $z_{it}$
% (factors), each of which is an auto-correlated GARCH(1,1) process as
% specified by equation \ref{eq:garch_spec}. Factor GARCH models have
% been studied extensively in the literature. A good review is found in
% \cite{Mikosch2009}. Here we are interested in the impact of
% auto-correlations in $z_{it}$ on the cross-correlation matrix of
% $r_{it}$.

% To be specific we adopt the generalized orthogonal GARCH model by
% Weide \cite{Weide2002}:
% \begin{eqnarray*}
%   \vec{r_t} &=& \mtx{W}\vec{z_t}
% \end{eqnarray*}
% where $\mtx{W}$ is an $N \times N$ invertible matrix; $\vec{z_t}$ and
% $\vec{r_t}$ are $N \times 1$ column vectors. Moreover, to meet our
% interest, $\vec{z_t}$ are assumed to be auto-correlated:
% \begin{eqnarray*}
%   \vec{z}_t &=& \phi \vec{z}_{t-1} + \vec{y}_t \\
%   \vec{y}_t &=& \mtx{H}^y_t \vec{x}_t \\
% \end{eqnarray*}
% where $\vec{x_t} \sim N(\vec{0}, \mtx{I})$, and $\mtx{H}^y_t$,
% the conditional covariance matrix of $\vec{y}_t$, is given by
% \begin{eqnarray*}
%   \mtx{H}^y_t &=& (\mtx{I} - \mtx A - \mtx B) + \mtx A \odot
%   (\vec{r}_{t-1} \vec{r}'_{t-1}) + \mtx B
%   \mtx{H}^y_{t-1}
% \end{eqnarray*}
% Here $\mtx A$ and $\mtx B$ are diagonal $N \times N$ matrix and
% $\odot$ denotes element-wise multiplication. The form of the constant
% term, $\mtx{I} - \mtx A - \mtx B$, ensures that the unconditional
% variance of $\vec{y}_t$ is $\mtx I$. In the simulations described in
% the rest of this section, we use the parametrization $\mtx A =
% \alpha_1 \mtx I$, $\mtx B = \beta_1 \mtx I$, where $\alpha_1 = 0.15$,
% $\beta_1 = 0.84$. For tractability, we fix $N=4$ and consider one
% particular covariance matrix of $\vec{r}_{it}$:
% \begin{eqnarray*}
%   \mtx C &=& \text{cov}(\vec{r}_{it}) \\
%   &=& \begin{pmatrix}
%     1 & \rho & \rho & \rho \\
%     \rho & 1 & \rho & \rho \\
%     \rho & \rho & 1 & \rho \\
%     \rho & \rho & \rho & 1 \\
%   \end{pmatrix}
% \end{eqnarray*}
% where $\rho = 0.5$. The eigenvalue decomposition of $\mtx C$ is
% \begin{eqnarray*}
%   \mtx C &=& \mtx{U} \mtx{\Lambda} \mtx{U}' \\
%   \mtx U &=&
%   \begin{pmatrix}
%     -1/\sqrt{2} & -1/\sqrt{6} & -1/2\sqrt{3} & 1/2 \\
%     1/\sqrt{2} & -1/\sqrt{6} & -1/2\sqrt{3} & 1/2 \\
%     0 & \sqrt{2}/\sqrt{3} & -1/2\sqrt{3} & 1/2 \\
%     0 & 0 & \sqrt{3}/2 & 1/2 \\
%   \end{pmatrix} \\
%   \mtx \Lambda &=&
%   \begin{pmatrix}
%     1-\rho & 0 & 0 & 0 \\
%     0 & 1-\rho & 0 & 0 \\
%     0 & 0 & 1-\rho & 0 \\
%     0 & 0 & 0 & 1+ 3\rho \\
%   \end{pmatrix}
% \end{eqnarray*}

%% Abandoned.
% In the last section we have investigated what the measured
% covariance matrix looks like when the return series have
% auto-correlated GARCH(1,1) returns but are actually independent of
% each other. In this section we investigate the situation when the
% return series are truly correlated. In particular, we study the
% following case:
% \begin{equation}\label{eq:garch_correlated_returns}
%   \begin{aligned}
%     x_{1t} &= r_{1t} & \\
%     x_{it} &= \sqrt{1 - \rho^2} r_{it} + \rho r_{1t} & \text{i = 2, 3,
%       ..., N}\\
%   \end{aligned}
% \end{equation}
% where $x_{it}$ are the return series whose covariance matrix is the
% subject of the current study and $r_{it}$ are independent
% auto-correlated GARCH(1,1) processes as specified by equation
% \ref{eq:garch_spec}.

% The covariance matrix C of the returns $\{x_{it}\}_{t=1}^T$ can be trivially
% computed 
% \begin{eqnarray}
%   C_{ij} &=& {1 \over T^{2/\alpha}} \sum_{t=1}^T x_{it} x_{jt}
%   \nonumber \\
%   C &\xrightarrow{T \to \infty}&
%   \begin{pmatrix}
%     \sigma_1^2 & \rho \sigma_1^2 & \cdots & \rho \sigma_1^2 \\
%     \rho \sigma_1^2 & (1 - \rho^2) \sigma_2^2 + \rho^2 \sigma_1^2 & \cdots &
%     \rho^2 \sigma_1^2 \\
%     \vdots & \vdots & \ddots & \vdots \\
%     \rho \sigma_1^2 & \rho^2 \sigma_1^2 & \cdots & (1 - \rho^2)
%     \sigma_N^2 + \rho^2 \sigma_1^2 
%   \end{pmatrix} \label{eq:garch_cov}
% \end{eqnarray}
% where $\sigma_i$ is the standard deviation of the GARCH(1,1) series
% $\{r_{it}\}_{t=1}^T$:
% \begin{eqnarray*}
%   \sigma_i = {1 \over T^{2/\alpha}} \sum_{t=1}^T r_{it}^2
% \end{eqnarray*}

% We would like to find out, when the covariance matrix of the $x$
% series is estimated, how the auto-correlation in the $r$ series affects
% the result --- specifically, how the auto-correlation strength $\phi$
% (c.f. equation \ref{eq:garch_spec}) affects the distribution of
% $C_{ij}$ when T is not very large.

% Figure \ref{fig:garch_correlated_Cij} shows the distribution of
% $C_{1j} / C_{11}$ ($j \geq 2$) and $C_{i1} / C_{11}$ ($i \geq
% 2$). These rescaled elements, as seen from equation
% \ref{eq:garch_cov}, have mean $\rho$. This is exactly what one sees in
% figure \ref{fig:garch_correlated_Cij}. It is also clear from the
% figure that $\phi$, the strength of auto-correlation, does not
% introduce a bias but does increase the variance of the
% estimation. This is the same conclusion as drawn in
% \S\ref{sec:garch_nonzero_phi}.
% \begin{figure}[htb!]
%   \centering
%     \includegraphics[scale=0.5, clip=true, trim=96 260 112
%     227]{../pics/garch_correlated_Cij.pdf}
%   \caption{\small \it Probability density function (PDF) of $C_{1j} /
%     C_{11}$ ($j \geq 2$) and $C_{i1} / C_{11}$ ($i \geq 2$). The
%     covariance matrix is constructed from 50 return series
%     $\{x_{it}\}$, $i = 1, 2, ..., 50$, and $t=1, 2, ..., 600$, as
%     specified by equation \ref{eq:garch_correlated_returns}. $\rho =
%     0.5$. 4000 instances of the covariance matrix are generated for
%     each value of $\phi$.}
%   \label{fig:garch_correlated_Cij}
% \end{figure}
% One can also find support in the data that the aforementioned
% distribution has power-law tails. This is shown in figure
% \ref{fig:garch_mix_Cij_tail} where the power of the left
% and the right tail is found to be 3.74 and 3.40, respectively.
% \begin{figure}[htb!]
%   \centering
%     \includegraphics[scale=0.5, clip=true, trim=105 229 107
%     120]{../pics/garch_mix_Cij_tail.pdf}
%   \caption{\small \it Horizontal axis (x) : the elements $C_{1j} /
%     C_{11}$ ($j \geq 2$) and $C_{i1} / C_{11}$ ($i \geq 2$) shifted to
%     center around 0. The section with $0.15 < x < 0.35$ is
%     plotted. Vertical axis: See the legend. }
%   \label{fig:garch_mix_Cij_tail}
% \end{figure}

% It is also interesting to see how the eigenvalues and eigenvectors
% behave given the non-trivial cross-correlation and the
% auto-correlation in the returns. However, the cross-correlation matrix
% of equation \ref{eq:garch_cov} has rather complicated eigenvalues and
% eigenvectors. It has been useful in studying the non-diagonal matrix
% elements but becomes clumsy with regard to eigenvalues and
% eigenvectors. Therefore we look at the following return series
% instead:
% \begin{eqnarray}
%   \begin{pmatrix}
%     x_{1t} \\
%     x_{2t} \\
%   \end{pmatrix} &=&
%   \begin{pmatrix}
%     \cos\theta/\sigma_1 & -\sqrt{2} \sin\theta/\sigma_2  \\
%      \sin\theta/\sigma_1 & \sqrt{2} \cos\theta/\sigma_2 \\
%   \end{pmatrix}
%   \begin{pmatrix}
%     r_{1t} \\
%     r_{2t} \\
%   \end{pmatrix} \label{eq:x-r_transformation}
% \end{eqnarray}
% where $r_{it}$ are independent, auto-correlated GARCH(1,1) processes
% as specified by equation \ref{eq:garch_spec}. Each of them has
% standard deviation $\sigma_i$ ($i = 1, 2$). It is easy to
% find the covariance matrix C of the series $\{x_{it}\}_{t=1}^T$ ($i =
% 1,2$):
% \begin{eqnarray*}
%   C &=& 
%   \begin{pmatrix}
%     1 + \sin\theta^2 & -\sin\theta\cos\theta \\
%     -\sin\theta\cos\theta & 1 + \cos\theta^2 \\
%   \end{pmatrix}
% \end{eqnarray*}
% The eigenvalues and eigenvectors of $C$ are
% \begin{eqnarray*}
%   CX &=& X
%   \begin{pmatrix}
%     1 & \\
%     & 2 \\
%   \end{pmatrix} \\
%   X &=& 
%   \begin{pmatrix}
%     \cos\theta & -\sin\theta \\
%     \sin\theta & \cos\theta \\
%   \end{pmatrix}
% \end{eqnarray*}

% Figure \ref{fig:garch_rotated_Cii} shows the probability plot of the
% diagonal elements of $C$. Evidently, their distribution is very close
% to a Gaussian, although the Anderson-Darling test rejects Gaussianity
% at the 5\% significance level. Through equation
% \ref{eq:x-r_transformation} the differences in variance of the
% realizations of $\{r_{1t}\}_{t=1}^T$ and $\{r_{2t}\}_{t=1}^T$ are not
% carried over to $\{x_{1t}\}_{t=1}^T$ and $\{x_{2t}\}_{t=1}^T$, and
% therefore the differences in variance of the latter series are simply
% the results of numerical computation and hence expected to be small
% and Gaussian distributed. One can also see from figure
% \ref{fig:garch_rotated_Cii} that the different auto-correlations
% (different $\phi$ values) in the $\{r_{1t}\}_{t=1}^T$ and
% $\{r_{2t}\}_{t=1}^T$ series do not change the Gaussianity. The
% variances of these Gaussian distributions, however, are indeed
% increased by increased auto-correlation.
% \begin{figure}[htb!]
%   \centering
%     \includegraphics[scale=0.4, clip=true, trim=24 152 44
%     18]{../pics/garch_rotated_Cii.pdf}
%   \caption{\small \it Probability plot of $C_{11}$ corresponding to
%     different auto-correlation strength ($\phi$) in the
%     $r$-series. When $\phi = 0, 0.2, 0.4, 0.6$, the variance is $3.380
%     \times 10^{-4}$, $3.639 \times 10^{-4}$, $4.512 \times 10^{-4}$,
%     $7.506 \times 10^{-4}$, respectively.} 
%   \label{fig:garch_rotated_Cii}
% \end{figure}

% On the other hand, the non-diagonal elements of $C$ have a very
% different distribution from the diagonal ones, as shown in figure
% \ref{fig:garch_rotated_Cij}.
% \begin{figure}[htb!]
%   \centering
%     \includegraphics[scale=0.5, clip=true, trim=24 303 30
%     274]{../pics/garch_rotated_Cij.pdf}
%   \caption{\small \it Probability density function (PDF) of the
%     non-diagonal elements of C. Corresponding to $\phi = 0, 0.2, 0.4,
%     0.6, 0.8$, the variance of the distribution is
%     $6.77\times10^{-8}$, $1.529\times10^{-7}$, $3.896\times10^{-7}$,
%     $1.124\times10^{-6}$, $5.445\times10^{-6}$.}
%   \label{fig:garch_rotated_Cij}
% \end{figure}
% One can see that the distribution of the non-diagonal elements is
% left-skewed, regardless of the auto-correlation $\phi$. This is to be
% compared with the distributions shown in figure
% \ref{fig:garch_correlated_Cij}, where the skewness is small and
% changes sign for different $\phi$ values, whereas in figure
% \ref{fig:garch_rotated_Cij}, the skewness is consistently negative and
% takes comparable values as $\phi$ changes from 0 to 0.9. We list these
% skewness values in table \ref{tab:garch_rotated_var_skw}.
% \begin{table}[htb!]
%   \centering
%   \begin{tabular}{|c|c|c|c|c|c|}
%     \hline
%     $\phi$ & 0 & 0.1 & 0.2 & 0.3 & 0.4 \\
%     \hline 
%     variance& 6.770e-08 & 1.008e-07 & 1.529e-07 & 2.339e-07 & 3.896e-07 \\
%     \hline
%     skewness & -1.62 & -1.65 & -1.96 & -1.86 & -1.67 \\
%     \hline
%   \end{tabular}
%   \begin{tabular}{|c|c|c|c|c|c|}
%     \hline
%     $\phi$ & 0.5 & 0.6 & 0.7 & 0.8 & 0.9 \\
%     \hline
%     variance& 6.022e-07 & 1.124e-06 & 2.156e-06 & 5.445e-06 & 2.389e-05 \\
%     \hline
%     skewness & -1.62 & -1.65 & -1.96 & -1.86 & -1.67 \\
%     \hline
%   \end{tabular}
%   \caption{\small \it Variance and skewness of $C_{12}$ and $C_{21}$}
%   \label{tab:garch_rotated_var_skw}
% \end{table}
% From table \ref{tab:garch_rotated_var_skw} and figure
% \ref{fig:garch_rotated_Cij} one can also see that the
% variance steadily increases as $\phi$ increases --- a similar
% observation that has been seen in figure
% \ref{fig:garch_correlated_Cij}.

% \begin{figure}[htb!]
%   \centering
%   \subfigure[]{
%     \includegraphics[scale=0.35, clip=true, trim=107 278 104
%     240]{../pics/garch_rotated_eig1_dist.pdf}
%     \label{fig:garch_rotated_eig1_dist}
%   }
%   \subfigure[]{
%     \includegraphics[scale=0.35, clip=true, trim=103 270 99 
%     227]{../pics/garch_rotated_eig2_dist.pdf}
%     \label{fig:garch_rotated_eig2_dist}
%   }
%   \caption{\small \it Eigenvalue distribution}
%   \label{fig:garch_rotated_eig_dist}
% \end{figure}

% \begin{table}[htb!]
%   \centering
%   \begin{tabular}{|c|c|c|c|c|c|}
%     \hline
%     $\phi$ & 0 & 0.2 & 0.4 & 0.6 & 0.8 \\
%     \hline
%     variance& 3.338e-07 & 4.113e-07 & 8.415e-07 & 2.351e-06 & 9.580e-06 \\
%     \hline
%     skewness & -3.92 & -1.65 & -1.22 & -1.38 & -0.90 \\
%     \hline
%   \end{tabular}
%   \caption{variance and skewness of the distribution of $\lambda_1$}
%   \label{tab:garch_rotated_eig1_var_skw}
% \end{table}

% \begin{table}[htb!]
%   \centering
%   \begin{tabular}{|c|c|c|c|c|c|}
%     \hline
%     $\phi$ & 0 & 0.2 & 0.4 & 0.6 & 0.8 \\
%     \hline
%     variance& 4.947e-07 & 7.852e-07 & 1.767e-06 & 4.813e-06 & 2.258e-05 \\
%     \hline
%     skewness & 2.93 & 2.14 & 2.41 & 2.42 & 2.49 \\
%     \hline
%   \end{tabular}
%   \caption{variance and skewness of the distribution of $\lambda_2$}
%   \label{tab:garch_rotated_eig2_var_skw}
% \end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% To find out more about this series of deformation, we investigate 
% how the characteristic function changes in response to increasing
% auto-correlation (increasing $\phi$). When $\phi = 0$ and T is very
% large, like $8 \times 10^4$ in our simulation, we have found the
% eigenvalue distribution converges to an $\alpha$-stable distribution
% whose characteristic function is known in analytic form.

% The empirical characteristic function $\varphi(k)$ is computed from
% the simulated eigenvalues according to:
% \begin{eqnarray*}
%   \varphi(k) &=& \E(e^{ik\lambda}) \\
%   &=& {1 \over M}\sum_{n=1}^M e^{ik\lambda_n}
% \end{eqnarray*}
% where $M$, the total number of eigenvalues in the sample, is $1\times
% 10^5$ in our simulation, as mentioned earlier.

% \begin{eqnarray}\label{eq:PDF_fourier_expansion}
%   p(\lambda) &=& \sum_{n=-\infty}^\infty c_n \exp\left(
%     -i {2\pi n \lambda \over L}
%   \right) \\ \nonumber
%   &\approx& \sum_{n=-M}^M c_n \exp\left(
%     -i {2\pi n \lambda \over L}
%   \right) \\
%   L &=& \lambda_\M - \lambda_\m \nonumber
% \end{eqnarray}
% where $M$ is a large integer at which the series expansion is
% truncated. For the simulations described in this section, we choose M
% = 500.

% The Fourier coefficients $c_n$ used in the above formula are computed
% as
% \begin{eqnarray*}
%   c_n &=& {1 \over L} \int_{\lambda_\m}^{\lambda_\M} \exp\left(
%     i {2\pi n \lambda \over L}
%   \right) p(\lambda) d\lambda \\
%   &=& {1 \over L} \E \left[
%     \exp\left(
%       i {2\pi n \lambda \over L}
%     \right)
%   \right] \\
%   &=& {1 \over LS} \sum_{k=1}^S \exp\left(
%     i {2\pi n \lambda_k \over L}
%   \right)
% \end{eqnarray*}
% where $S = 1 \times 10^5$ is the aforementioned sample size of the
% simulated eigenvalue. Figure \ref{fig:GarchEigPDFFourierCoef}
% shows the Fourier coefficients computed according to the above
% formula.
% \begin{figure}[htb!]
%   \centering
%   \includegraphics[scale=0.56, clip=true, trim=10 203 6
%   89]{../pics/GarchEigPDFFourierCoef.pdf}
%   \caption{\small \it Fourier Coefficients of the empirical PDF of
%     the Eigenvalue distribution. Blue/Red: Real/imaginary parts of the
%     Fourier coefficients. The $\phi$ values 0, 0.2, 0.5, 0.8, 0.95, 0.96,
%     0.97, 0.98 correspond to correlation time $\tau$ = 0, 0.43, 1.00,
%     3.11, 13.51, 16.98, 22.76, 34.31.}
%   \label{fig:GarchEigPDFFourierCoef}
% \end{figure}

% Before we proceed to investigate the properties of the empirical PDF
% using the Fourier series expansion, we need to ensure the expansion
% does provide a sufficiently accurate approximation to the empirical
% PDF. However, to directly compare the Fourier series expansion
% with the empirical PDF, the empirical PDF will have to be evaluated
% --- the accuracy of the result will then largely depend on the number
% of bins to which one chooses to sort the eigenvalues so as to produce
% the empirical PDF --- a choice that is rather arbitrary and
% subjective. To avoid such a procedure, we can compare the empirical
% CDF with its Fourier expansion instead. Formally we integrate equation
% \ref{eq:PDF_fourier_expansion} to obtain
% \begin{equation}
%   \label{eq:CDF_fourier_expansion}
%   \begin{aligned}
%     F(\lambda) &= \int_{\lambda_\m}^{\lambda} p(x) dx \\
%     &= \sum_{n \in
%       \mathbb{Z}\backslash\{0\}} c_n i {L \over 2\pi n} \left[
%       \exp\left(
%         -i {2\pi n \lambda \over L}
%       \right) - 
%       \exp\left(
%         -i {2\pi n \lambda_\m \over L}
%       \right) \right] \\
%     & + (\lambda - \lambda_\m) c_0
%   \end{aligned}
% \end{equation}
% The CDF on the left-hand-side of equation
% \ref{eq:CDF_fourier_expansion} can be evaluated directly using the
% simulated eigenvalues; meanwhile, the right-hand-side can be evaluated
% for any $\lambda$ using $c_n$.

% Figure \ref{fig:CDF_fourier_expansion} shows how the two sides of
% equation \ref{eq:CDF_fourier_expansion} compare to each other.
% \begin{figure}[htb!]
%   \centering
%   \includegraphics[scale=0.6, clip=true, trim=20 240 15
%   200]{../pics/garch_cdf_fourier_expansion.pdf}
%   \caption{\small \it The empirical cummulative distribution
%     function (CDF) and its Fourier series expansion. Blue: empirical
%     CDF. Red: Fourier series expansion. The plots are on log-log
%     scale. The $\phi$ values 0, 0.2, 0.5, 0.8, 0.95, 0.96,
%     0.97, 0.98 correspond to correlation time $\tau$ = 0, 0.43, 1.00,
%     3.11, 13.51, 16.98, 22.76, 34.31.}
%   \label{fig:CDF_fourier_expansion}
% \end{figure}
% We see that the two curves match fairly well --- they differ from each
% other only at their lower ends, which are statistically insignificant
% due to the very small portion of data points in that region.

% Now that one is assured that the Fourier series expansion is
% sufficiently accurate, it makes sense to study the properties of the
% Fourier coefficients $c_n$. A few features immediately present
% themselves in figure \ref{fig:GarchEigPDFFourierCoef}:
% \begin{itemize}
% \item $\re c_n = \re c_{-n}$, where $\re$ denotes the real part of a
%   complex number;
% \item $\re c_n$ resembles a dumped cosine wave;
% \item $\re c_n$ and $\im c_n$ have the same frequency with a constant
%   phase difference. Here $\im$ denotes the imaginary part of $c_n$.
% \end{itemize}
% The last conjecture about the frequency and the phase difference can
% be verified by looking at the phase plot of $c_n$, which is shown in
% figure \ref{fig:garch_Cn_phase}.
% \begin{figure}[htb!]
%   \centering
%   \includegraphics[scale=0.6, clip=true, trim=21 248 14
%   198]{../pics/garch_Cn_phase.pdf}
%   \caption{\small \it Phases of the Fourier coefficients ($c_n$) of
%     the empirical eigenvalue PDF. Horizonal axis: $n$; Vertical Axis:
%     phases of $c_n$. The $\phi$ values 0, 0.2, 0.5, 0.8, 0.95, 0.96,
%     0.97, 0.98 correspond to correlation time $\tau$ = 0, 0.43, 1.00,
%     3.11, 13.51, 16.98, 22.76, 34.31.}
%   \label{fig:garch_Cn_phase}
% \end{figure}
% From figure \ref{fig:garch_Cn_phase} one can see that the phase of
% $c_n$ ($\ph c_n$) is approximately linear when $\phi$ is
% small --- however, as $\phi$ becomes larger and larger, the
% non-linearity of $\ph c_n$ becomes stronger and stronger.

% In the relatively simple situation of small $\phi$, i.e. weak
% auto-correlation, it lends some insight to consider the following
% approximate formula:
% \begin{eqnarray}
%   \re c_n &=& e^{
%     -u_r(\phi) - v_r(\phi) |n|
%   } \cos[\omega(\phi) n] \label{eq:garch_fourier_coef_real}\\
%   \im c_n &=& e^{
%     -u_i(\phi) - v_i(\phi) |n|
%   } \sin[\omega(\phi) n] \label{eq:garch_fourier_coef_imag}
% \end{eqnarray}
% Apparently, $\omega(\phi)$ can be directly read out from figure
% \ref{fig:garch_Cn_phase}. The results are listed in table
% \ref{tab:garch_phase_velocity}.
% \begin{table}[htb!]
%   \footnotesize
%   \centering
%   \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
%     \hline
%     $\phi$ & 0 & 0.2 & 0.5 & 0.8 & 0.95 & 0.96 & 0.97 & 0.98 \\
%     \hline
%     $\tau$ & 0 & 0.4307 & 1.0000 & 3.1063 &  13.5134 & 16.9797 &
%     22.7566 & 34.3096 \\
%     \hline
%     $\omega(\phi)$ & 0.0442 & 0.0538 & 0.0348 & 0.0414 & 0.0180 &
%     0.0320 & 0.0154 & 0.0242 \\
%     \hline
%   \end{tabular}
%   \caption{\small \it Phase velocity $\omega$ of the Fourier
%     coefficients of the eigenvalue PDF.}
%   \label{tab:garch_phase_velocity}
% \end{table}
% Using these values as initial estimate, the parameters in equation
% \ref{eq:garch_fourier_coef_real} and \ref{eq:garch_fourier_coef_imag},
% namely $u_r(\phi)$, $v_r(\phi)$ and $v_i(\phi)$ can be estimated by
% least-square-error methods. We use the Matlab function ``lsqcurvefit''
% for this purpose and list the estimated parameter values in table
% \ref{tab:garch_fourier_coef_real_param}.
% \begin{table}[htb!]
%   \footnotesize
%   \centering
%   \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
%     \hline
%     $\phi$ & 0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8\\
%     \hline
%     $\tau$ & 0 & 0.3010 & 0.4307 & 0.5757 & 0.7565 & 1.0000 & 1.3569 &
%     1.9434 & 3.1063 \\
%     \hline
%     $u_r(\phi)$ & 0.0938 & 0.0936 & -0.0812 & 0.1649 & 0.2627 & 0.6285 &
%     0.5443 & 0.5720 & 1.1610 \\
%     \hline
%     $v_r(\phi)$ & 0.0044 & 0.0045 & 0.0057 & 0.0046 & 0.0045 & 0.0034 &
%     0.0046 & 0.0059 & 0.0048 \\
%     \hline
%     $\omega(\phi)$ & 0.0420 & 0.0424 & 0.0511 & 0.0428 & 0.0421 &
%     0.0333 & 0.0416 & 0.0497 & 0.0394 \\
%     \hline
%   \end{tabular}
%   % \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
%   %   \hline
%   %   $\phi$ & 0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.9 \\
%   %   \hline
%   %   $\tau$ & 0 & 0.3010 & 0.4307 & 0.5757 & 0.7565 & 1.0000 & 1.3569 &
%   %   1.9434 & 3.1063 & 6.5788 \\
%   %   \hline
%   %   $u_r(\phi)$ & 0.0938 & 0.0936 & -0.0812 & 0.1649 & 0.2627 & 0.6285 &
%   %   0.5443 & 0.5720 & 1.1610 & 1.2347 \\
%   %   \hline
%   %   $v_r(\phi)$ & 0.0044 & 0.0045 & 0.0057 & 0.0046 & 0.0045 & 0.0034 &
%   %   0.0046 & 0.0059 & 0.0048 & 0.0098 \\
%   %   \hline
%   %   $\omega(\phi)$ & 0.0420 & 0.0424 & 0.0511 & 0.0428 & 0.0421 &
%   %   0.0333 & 0.0416 & 0.0497 & 0.0394 & 0.0667 \\
%   %   \hline
%   % \end{tabular}
%   \caption{\small \it Least-Square-Error estimate of parameter values of $\re
%     c_n$}
%   \label{tab:garch_fourier_coef_real_param}
% \end{table}
% As is said earlier, the equation \ref{eq:garch_fourier_coef_real} and
% \ref{eq:garch_fourier_coef_imag} only apply in the cases of small
% $\phi$, so the estimation has only been done for $\phi$ values up to
% 0.8. To obtain better statistics about the relation between $\phi$ and
% the parameters, a few more values of $\phi$ have been added. The
% corresponding Fourier coefficients have been computed and validated
% against the empirical CDF. Figure \ref{fig:garch_fourier_coef_real}
% shows how the equation \ref{eq:garch_fourier_coef_real} fits to $\re
% c_n$ with the estimated parameter values.
% \begin{figure}[htb!]
%   \centering
%   \includegraphics[scale=0.56, clip=true, trim=10 197 11
%   147]{../pics/garch_fourier_coef_real.pdf}
%   \caption{\small \it A dumped cosine function fit to $\re
%     c_n$. Blue: empirical Fourier coefficients; Green: Dumped cosine
%     function.}
%   \label{fig:garch_fourier_coef_real}
% \end{figure}
% We see that the two curves in figure \ref{fig:garch_fourier_coef_real}
% match fairly well. Obvious deviations are found only in regions of
% large n, where slight inaccuracy in $\omega(\phi)$ is magnified. This
% confirms the validity of formula \ref{eq:garch_fourier_coef_real}.

% % To find out the relation between $\phi$ and the parameters $u_r(\phi)$,
% % $v_r(\phi)$ and $\omega(\phi)$, we plot these parameters' values
% % against $\phi$ in figure \ref{fig:Cn_real_param}.
% % \begin{figure}[htb!]
% %   \centering
% %   \includegraphics[scale=0.56, clip=true, trim=6 311 0
% %   264]{../pics/Cn_real_param.pdf}
% %   \caption{\small \it Linear fit of the parameters in $\re
% %     c_n$. Blue circles: parameter values normalized by the sample
% %     mean; Green line: straight line fitted to the data points.}
% %   \label{fig:Cn_real_param}
% % \end{figure}
% % Instead of directly plotting the parameters' values, figure
% % \ref{fig:Cn_real_param} plots the ratio of these values over their
% % respective sample mean, so that one can see how much each
% % individual data point (blue circles) deviates from the sample mean as
% % well as from the fitted line. The title of each plot in figure
% % \ref{fig:Cn_real_param} is the equation of the fitted line.

% % From figure \ref{fig:Cn_real_param} we see that $u_r(\phi)$ is
% % approximately linear in $\phi$ --- the coefficient of the linear term
% % in its equation is an order of magnitude larger than the constant
% % term. In contrast, $v_r(\phi)$ and $\omega(\phi)$ are approximately
% % constant --- the coefficient of the linear term in their equation is
% % an order of magnitude smaller than the constant term. So, also
% % considering $0 < \phi < 1$ and that we have a rather small sample
% % size, we estimate $v_r(\phi)$ and $\omega(\phi)$ to be constant. In
% % summary
% % \begin{eqnarray*}
% %   u_r(\phi) &=& 1.2364 \phi - 0.1124 \\
% %   v_r &=& 4.7148 \times 10^{-3} \\
% %   \omega &=& 3.3476 \times 10^{-2} \\
% % \end{eqnarray*}
% % Now that $v_r(\phi)$ and $\omega(\phi)$ are constant, we shall write
% % them only as $v_r$ and $\omega$ to avoid confusion.

% Turning our attention to the imaginary part of $c_n$($\im c_n$) and
% applying the same methods, we find the corresponding parameter values
% for $\im c_n$ (table \ref{tab:garch_fourier_coef_imag_param}). Figure
% \ref{fig:garch_fourier_coef_imag} shows how these values together with
% equation \ref{eq:garch_fourier_coef_imag} fit to $\im c_n$. Comparing
% table \ref{tab:garch_fourier_coef_real_param} and
% \ref{tab:garch_fourier_coef_imag_param}, we see that $v_r$ and $v_i$
% are essentially equal, while $u_r(\phi)$ is consistently larger than
% $u_i(\phi)$.

% \begin{table}[htb!]
%   \footnotesize
%   \centering
%   \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
%     \hline
%     $\phi$ & 0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8\\
%     \hline
%     $\tau$ & 0 & 0.3010 & 0.4307 & 0.5757 & 0.7565 & 1.0000 & 1.3569 &
%     1.9434 & 3.1063 \\
%     \hline
%     $u_i(\phi)$ & 0.0555 & 0.0565 & -0.1105 & 0.1291 & 0.2236 &
%     0.6007 & 0.5031 & 0.5341 & 1.1273 \\
%     \hline
%     $v_i(\phi)$ & 0.0047 & 0.0047 & 0.0059 & 0.0048 & 0.0048 &
%     0.0035 & 0.0049 & 0.0062 & 0.0049 \\
%     \hline
%     $\omega(\phi)$ & 0.0420 & 0.0424 & 0.0511 & 0.0428 & 0.0421 &
%     0.0333 & 0.0416 & 0.0497 & 0.0394 \\
%     \hline
%   \end{tabular}
%   \caption{\small \it Least-Square-Error estimate of parameter values of $\im c_n$}
%   \label{tab:garch_fourier_coef_imag_param}
% \end{table}

% \begin{figure}[htb!]
%   \centering
%   \includegraphics[scale=0.56, clip=true, trim=18 221 19
%   171]{../pics/garch_fourier_coef_imag.pdf}
%   \caption{\small \it A dumped sine function fit to the imaginary
%     part of $c_n$ ($\im c_n$). Red: empirical Fourier coefficients;
%     Green: Dumped sine function.} 
%   \label{fig:garch_fourier_coef_imag}
% \end{figure}

%  As is done for $\re c_n$, we can also find an approximate
% linear expression for $u_i(\phi)$ by fitting a line to the data. The
% result is
% \begin{eqnarray*}
%   u_i(\phi) &=& 1.2356 \phi - 0.1480
% \end{eqnarray*}
% So, combining the results for $\re c_n$ and $\im c_n$, we may write
% \begin{eqnarray*}
%   c_n &=& e^{-v|n|}\left[
%     e^{-a_r\phi - b_r}\cos(\omega n) + ie^{-a_i\phi - b_i}\sin(\omega n)
%   \right] \\
% \end{eqnarray*}
% where
% \begin{equation}
%   \label{eq:garch_Cn_parameters}
%   \begin{aligned}
%     v &= 4.7148 \times 10^{-3} \\
%     a_r &= 1.2364 \\
%     b_r &= -0.1124 \\
%     a_i &= 1.2356 \\
%     b_i &= -0.1480 \\
%     \omega &= 3.3476 \times 10^{-2}
%   \end{aligned}
% \end{equation}
% It should be emphasized that the above parameters are only constants
% with respect to the GARCH parameters $\alpha_1$, $\beta_1$ and hence
% the resulting tail exponent $\alpha$. If the GARCH parameters are
% changed, the parameters in \ref{eq:garch_Cn_parameters} are expected
% to follow.


%% TODO:
% 1. find the power-exponent of the tail of the SLV (stochastic
% log-volatility) model. Normalize the returns using the sample
% standard deviation when computing the cross-correlation matrix.
% 
% 2. Hows does the power-law return distribution change the eigenvalue
% distribution in comparison to the Marcenko-Pastur law?
%
% 3. How do the maximum and the minimum eigenvalue change
% corresponding to different values of autocorrelation?
%
% 4. Use different combinations of q = N/T and investigate how the
% eigenvalue distribution changes in the case of the SLV model.

\input{summary}
\input{appendix}

\bibliographystyle{plain}
\bibliography{econophysics}
\end{document}


