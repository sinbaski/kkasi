\documentclass{article}
\usepackage{amssymb,amsmath,amsthm,bm}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{multirow}
% \usepackage{wrapfig}
\usepackage[usenames,dvipsnames]{color}
\usepackage{mathrsfs}
\usepackage{enumerate}
\usepackage[bookmarks=true]{hyperref}
\usepackage{bookmark}

\input{../physics_common.tex}
\DeclareGraphicsExtensions{.eps, .pdf}

\author{
  Prof. Sven \AA berg \\
  Xie Xiaolei}
\date{\today}
\title{Effect of Heteroscedasticity in Covariance Matrix}

\begin{document}
\maketitle

\section{Introduction}
In financial applications such as principle component analysis, it is
of major interest to identify the most influential factors in a
portfolio, whose variances are the largest eigenvalues of the
covariance matrix of the portfolio \cite{Potters2003}. For this
purpose, it is important to know the eigenvalue distribution of a
large random covariance matrix. This distribution constitutes a
background noise that must be filtered out before the true eigenvalues
can be reliablly identified.

Applying free probability thoery to the study of eigenvalue
distributions of large random matrices has acheived encouraging
results. M. Politi, E. Scalas and G. Germano \cite{politi2010} studied
the empirical covariance matrix of iid sequences having an
$\alpha$-stable distribution, and derived formulas for the spectral
density. Burda et al \cite{burda2011} investigated empirical
covariance matrices of not only iid sequences but also those with
auto- and cross-correlations. In the same paper, they also studied the
case of I-GARCH sequences ($\sigma^2_t = \alpha \sigma^2_{t-1} + (1 -
\alpha) r^2_{t-1}$) and derived formulas for the spectral density in
the limiting situation $T(1-\alpha) \to \text{const}$.

However, covariance matrices of sequences of a stochastic volatility
(SV) model, which often gives more accurate volatility forecast than
do GARCH and its variates, seem to have eluded researchers'
attention. In this paper, we present some results about the spectral
density and minimum/maximum eigenvalues of a covariance matrix of SV,
and in particular, lognormal-volatility return sequences where the
volatilities have autocorrelations.

Section \ref{sec:sep} summerizes the techniques of separating the
heteroscedasticity effect, which are also found in
\cite{biroli2007student} and \cite{burda2011}, and we argue that, in
the special case where the sequences always have the same
conditional volatilities, heteroscedasticity affects the spectral
properties of the covariance matrix only through the unconditional
distribution of the volatility.

Section \ref{sec:lognormal} presents some results in the case where
volatility is autocorrelated and conditionally lognormally
distributed, while the innovations are iid normal.

\section{Separation of the Heteroscedasticity Effect}\label{sec:sep}
Consider a covariance matrix constructed as
$$
C_{ij} = {1 \over T}\sum_{t=1}^T r_{it} r_{jt}
$$
where $r_{it}$ is the log-return of the $i$-th stock at time $t$. $i =
1, 2, \dots, N$, $t = 1,2,\dots,T$. Here and in the rest of the
text we adopt a stochastic volatility model for $r_{it}$, i.e.
$$
r_{it} = \sigma_{it} \eta_{it}
$$
where $\sigma_{it}$ symbolizes the volatility of the return while
$\eta_{it}$ is assumed a standard normal random variable.
So we can write
\begin{eqnarray}
C_{ij} &=& {1 \over T}\sum_{t=1}^T \sigma_{it} \eta_{it} \sigma_{jt}
\eta_{jt} \nonumber \\
\bm{C} &=& {1 \over T} (\bm{\sigma * \eta}) (\bm{\sigma *
  \eta})' \label{eq:C}
\end{eqnarray}
where the matrices $\bm{\sigma}$ and $\bm{\eta}$ have elements
$\sigma_{it}$ and $\eta_{it}$ respectively, and * denotes element-wise
multiplication. Due to the cyclic property of matrix trace, the
moment generating function of $\bm{C}$, $M_C(z)$, relates to $M_D(z)$,
the moment generating function of
\begin{eqnarray*}
  \bm{D} &=& {1 \over T} (\bm{\sigma * \eta})' (\bm{\sigma * \eta})
\end{eqnarray*}
through the equation
\begin{eqnarray*}
  M_C(z) &=& {1 \over q} M_D(z)
\end{eqnarray*}
where $q = N/T$. For later convenience, we make use of $T \times T$
matrices $\bm{\tilde{\sigma}}$ and $\bm{\tilde{\eta}}$, as well as a
projector
$$
\bm{P} = \text{diag}(\underbrace{1, \cdots, 1}_{\text{N 1's}}, 
\underbrace{0, \cdots, 0}_{\text{T-N 0's}})
$$
The first N rows of $\tilde{\bm{\sigma}}$ and $\bm{\tilde{\eta}}$ are
precisely those of $\bm{\sigma}$ and $\bm{\eta}$. This way, we have
\begin{eqnarray*}
\bm D &=& {1 \over T} (\bm{\sigma * \eta})' (\bm{\sigma * \eta}) \\
&=& {1 \over T} (\bm{\tilde{\sigma} * \tilde{\eta}})' \bm P'
\bm P (\bm{\tilde{\sigma} * \tilde{\eta}}) \\
&=& {1 \over T} (\bm{\tilde{\sigma} * \tilde{\eta}})'
\bm P (\bm{\tilde{\sigma} * \tilde{\eta}}) \\
\end{eqnarray*}
Again, by the cyclic property of matrix trace, the spectral properties
of the RHS of the last equation is equivalent to those of
\begin{eqnarray*}
  \bm E &=& {1 \over T} (\bm{\tilde{\sigma} * \tilde{\eta}}) (\bm{\tilde{\sigma}
    * \tilde{\eta}})' \bm P \\
\end{eqnarray*}
In the simple situation where $\sigma_{it} =
\sigma_{jt} = \sigma_t$ for some $\sigma_t$ and for all $i, j, t$,
\begin{eqnarray*}
  \bm{\tilde \sigma * \tilde \eta} &=& \bm{\tilde \eta}
  \begin{pmatrix}
    \sigma_1 &        & \\
        & \ddots & \\
        &        & \sigma_T
  \end{pmatrix} \\
  &=& \bm{\tilde \eta \bar \sigma}
\end{eqnarray*}
Thus
\begin{eqnarray}\label{eq:E_def}
  \bm E &=& {1 \over T}\bm{\tilde \eta \bar \sigma^2 \tilde \eta' P}
\end{eqnarray}
In the large T limit $\bm{\tilde \eta \bar \sigma^2 \tilde \eta'}$ and $P$ are
freely independent, therefore their S-transforms are multiplicative,
i.e.
\begin{eqnarray}
  S_E(z) &=& S_{\tilde \eta \bar \sigma^2 \tilde \eta'/T}(z) S_P(z)
  \nonumber \\
  &=& S_{\tilde \eta' \tilde \eta /T}(z) S_{\bar \sigma^2}(z) S_P(z) \label{eq:S_E}
\end{eqnarray}
For $S_{\tilde \eta' \tilde \eta /T}(z)$ there have been results when
the elements of $\bm{\tilde \eta}$ are identically distributed with finite
second moment \cite{burda2011} or alternatively, follow a stable
distribution \cite{politi2010}.

So our focus hereafter is on the spectral properties of $\bar{
\bm \sigma^2}$. The Green's function of $\bar{\bm \sigma^2}$
can be immediately written down:
\begin{eqnarray*}
G_{\bar \sigma^2} &=& \frac{1}{T}\sum_{t=1}^T \frac{1}{z - \sigma_t^2}
\end{eqnarray*}
Suppose the $\sigma_t^2$ process has correlation time $\tau$,
i.e. $\text{corr}(\sigma_t^2, \sigma^2_{t+\tau}) \approx 0$. Then we
have
\begin{eqnarray*}
G_{\bar \sigma^2} &=& {1 \over T}\sum_{n=0}^{\tau-1}\sum_{m=1}^{T/\tau}
\frac{1}{z - \sigma_{m\tau + n}^2} \\
&=& {1 \over \tau}\sum_{n=0}^{\tau-1} {1 \over T/\tau}
\sum_{m=1}^{T/\tau} \frac{1}{z - \sigma_{m\tau + n}^2} \\
&=& {1 \over \tau}\sum_{n=0}^{\tau-1}
\E_m\left(\frac{1}{z - \sigma_{m\tau + n}^2}\right)
\end{eqnarray*}
where $\E_m$ denotes averaging over the index $m$. On condition that
$\sigma_t^2$ is a stationary process, $\E_m[1 / (z -
    \sigma_{m\tau + n}^2)]$ is independent of $n$. Thus
\begin{eqnarray}\label{eq:greens}
G_{\bar \sigma^2} &=& \E_m\left(
  \frac{1}{z - \sigma_{m\tau + n}^2}
\right)
\end{eqnarray}
So autocorrelations of $\sigma_t^2$'s affect spectral properties
only through the stationary distribution of $\sigma_t^2$.
% Then the moment generating function $M_{\bar \sigma^2}$ is
% \begin{eqnarray*}
%   M_{\bar \sigma^2} &=& z G_{\bar \sigma^2} - 1 \\
%   &=& \frac{1}{T} \sum_{t=1}^T \frac{1}{1 - \sigma_t^2/z} - 1\\
%   &=& \frac{1}{T} \sum_{n=0}^{\infty} \frac{1}{z^n}\sum_{t=1}^T
%   \sigma_t^{2n} - 1\\
%   &=& \frac{1}{T} \sum_{n=1}^{\infty} \frac{1}{z^n}\sum_{t=1}^T
%   \sigma_t^{2n}
% \end{eqnarray*}

\section{Normal innovations \& lognormal volatilities with AR(1)
  autocorrelation}\label{sec:lognormal}
In this subsection we consider the case
\begin{eqnarray}
  \log \sigma_t &=& \phi\log \sigma_{t-1} + x_t \nonumber \\
  &=& \sum_{l = 0}^{\infty} \phi^l x_{t-l} \label{eq:AR}
\end{eqnarray}
where $x_t \sim N(0, \theta^2)$ and we have assumed that the $\log
\sigma_t$ process was started in the infinite past with $\log
\sigma_{-\infty} = x_{-\infty}$. In addition, we assume $\log
\sigma_t$ is a stationary process with $|\phi| < 1$. Clearly $\log
\sigma_t$ is normally distributed with mean 0 and variance ${\theta^2
  / (1 - \phi^2)}$. The auto-correlation function $\rho(n)$ decays
exponentially: $\rho_n = \phi^{n-1} = \exp[-(n-1)\ln(1/\phi)]$.
According to \eqref{eq:greens}, the spectral
density of $\bm E$ as appears in \eqref{eq:E_def} is the same as if
$\sigma_t^2$'s were uncorrelated but $\log \sigma_t \sim N(0,
\theta^2/(1 - \phi^2))$.

To find the spectral density of $\bm C$ with normally distributed $\bm
\eta$ and uncorrelated, log-normally distributed $\sigma_t$, we use
the result of Biroli et al in \cite{biroli2007student}, which gives
the Blue function of $\bm C$ as
\begin{eqnarray*}
  B(z) &=& \frac{1}{z} + \int \frac{\sigma ^2 P(s) ds}{1-q \sigma ^2 z}
\end{eqnarray*}
where $z$ is a complex argument $z = x + i y$, $q = N/T$ and $\sigma$
has the stationary distribution of $\sigma_t$, whose probability
density function is $P(s)$. Now we are specifically considering
lognormally distributed $\sigma_t$, so we rewrite the above equation as
\begin{eqnarray}
B(z) &=& \int_{-\infty }^{\infty } \frac{e^{2 s} \exp \left(-\frac{s^2}{2
      v}\right)}{\sqrt{2 \pi  v} \left(1-q e^{2 s} z\right)} \,
ds+\frac{1}{z} \label{eq:LognormalBlue}
\end{eqnarray}
where $v = \theta^2/(1 - \phi^2)$, reflecting the effect
of autocorrelation in $\log \sigma_t$'s. However, no closed form of
this integral is known to the author. To obtain the spectral density,
which is given by the Sokhotski-Weierstrass formula as
\begin{eqnarray}
  \rho(\lambda) &=& -{1 \over \pi} \lim_{\epsilon \to 0} \im G(\lambda +
  i\epsilon) \label{eq:Sokhotski-Weierstrass}
\end{eqnarray}
we equate the real part of RHS of \eqref{eq:LognormalBlue} to $\lambda \in
R$ and the imaginery part to 0, then we can solve \eqref{eq:LognormalBlue}
numerically and obtain $\rho(\lambda)$ via
\eqref{eq:Sokhotski-Weierstrass}. Specifically we solve the following
equation system:
\begin{eqnarray}
\lambda &=& \int_{-\infty}^{\infty} \frac{ds}{\sqrt{2\pi v}}
\frac{
  e^{-s^2/2v}(e^{-2s} - qx)
}{
  (e^{-2s} - qx)^2 + q^2 y^2  
} +
\frac{x}{x^2 + y^2} \label{eq:LognormalBlueReal}\\
0 &=& \int_{-\infty}^{\infty} \frac{ds}{\sqrt{2\pi v}}
\frac{
  e^{-s^2/2v} q y
}{
  (e^{-2s} - qx)^2 + q^2 y^2  
} -
\frac{y}{x^2 + y^2} \label{eq:LognormalBlueImag}
\end{eqnarray}
This is the method used by Biroli et al in
\cite{biroli2007student}. Figure \ref{fig:LognormalBlue}
shows the plot of the Blue function \eqref{eq:LognormalBlue}. $-\pi
\rho(\lambda)$ is the projection of the contour $\im B(z) = 0$ onto the $\im
G$ axis.
\begin{figure}[htb!]
  \centering
    \includegraphics[scale=0.45, clip=true, trim=66 0 64
    0]{../pics/LognormalBlue.eps}
  \caption{\small \it The Blue function $B$. q = 0.5, v = 0.5. The
    thick black curve in the left figure is the $\im B = 0$ curve. The
    smallest (largest) eigenvalue is achieved at the left (right) end
    of the curve.}
  \label{fig:LognormalBlue}
\end{figure}
The spectral density function $\rho(\lambda)$ obtained as numerical
solutions is shown in figure \ref{fig:LognormalSpectra} and compared
to the Marcenko-Pastur law with several values of $q = N/T$ and $v$.
\begin{figure}[htb!]
  \centering
  \subfigure[$q = 0.1$]{
    \includegraphics[scale=0.35]{../pics/Lognormal_q0.1.eps}
    \label{fig:Lognormal_q0.1}
  }
  \subfigure[$q = 0.2$]{
    \includegraphics[scale=0.35]{../pics/Lognormal_q0.2.eps}
    \label{fig:Lognormal_q0.2}
  }
  \subfigure[$q = 0.5$]{
    \includegraphics[scale=0.35]{../pics/Lognormal_q0.5.eps}
    \label{fig:Lognormal_q0.5}
  }
  \subfigure[$q = 1.0$]{
    \includegraphics[scale=0.35]{../pics/Lognormal_q1.0.eps}
    \label{fig:Lognormal_q1.0}
  }
  \caption{\small \it Spectra with lognormal volatilities. The
    empirical probability density functions are plotted as stairs. The
  density function given by the Marcenko-Pastur law is plotted as a
  thick red curve.}
  \label{fig:LognormalSpectra}
\end{figure}

When $v$ is small, the integrands in \eqref{eq:LognormalBlueReal} and
\eqref{eq:LognormalBlueImag} have sharp maxima at $s = 0$. Hence it is
useful to approximate them by Taylor expansion. Let's write
\begin{eqnarray*}
  \lambda &=& \int _{-\infty }^{\infty }\!g \left( s,v \right) F \left( s
  \right) {ds}+{\frac {x}{{x}^{2}+{y}^{2}}} \\
  0 &=& \int _{-\infty }^{\infty }\!g \left( s,v \right) G \left( s
  \right) {ds}-{\frac {y}{{x}^{2}+{y}^{2}}}
\end{eqnarray*}
where
\begin{eqnarray*}
g &=& \frac{1}{\sqrt{2\mathop{\rm  }\pi \mathop{\rm  }v }}e^{-\frac{s
    ^{2}}{2\mathop{\rm  }v }}\\
F \left(s \right) &=& \frac{\left(e^{-2\mathop{\rm  }s }-q \mathop{\rm
    }x \right)}{\left(e^{-2\mathop{\rm  }s }-q \mathop{\rm  }x
  \right)^{2}+q ^{2}\mathop{\rm  }y ^{2}}\mathop{\rm  } \\
G(s) &=& \frac{q \mathop{\rm  }y }{\left(e^{-2\mathop{\rm  }s }-q
    \mathop{\rm  }x \right)^{2}+q ^{2}\mathop{\rm  }y ^{2}}\\
\end{eqnarray*}

Taylor-expand $F(s)$ and $G(s)$ to 2nd order in $s$
\begin{eqnarray*}
\lambda &=& F(0) + {v \over 2}F^{''}(0) + O(v^2) \\
0 &=& G(0) + {v \over 2}G^{''}(0) + O(v^2) \\
\end{eqnarray*}
Plugging them into \eqref{eq:LognormalBlueReal} and
\eqref{eq:LognormalBlueImag} gives
\begin{eqnarray}
\lambda &=& {-q x+1 \over (-q x+1)^2+q^2 y^2} + {v \over 2}\left\{
{4 \over (-q x+1)^2+q^2 y^2} -{24 (-q x+1) \over
  [(-q x+1)^2+q^2 y^2]^2} \right. \nonumber \\
&& \left. + {32 (-q x+1)^3 \over [(-q x+1)^2+q^2 y^2]^3} -8 {(-q
    x+1)^2 \over [(-q x+1)^2+q^2 y^2]^2}
\right\} \label{eq:lambda_linear}\\
0 &=& {q y \over (-q x+1)^2+q^2 y^2} + {v \over 2} \left\{
  {32 q y (-q x+1)^2 \over [(-q x+1)^2+q^2 y^2]^3}
  -{8 q y \over [(-q x+1)^2+q^2 y^2]^2} \right. \nonumber \\
  && \left.
    -{8 q y (-q x+1) \over [(-q x+1)^2+q^2 y^2]^2}
\right\} \label{eq:constraint_linear}
\end{eqnarray}
When $v = 0$ and $\lambda = \lambda^{MP}_{\pm} = (1 \pm
\sqrt{q})^2$ the Marcenko-Pastur law is recovered, as seen in figure
\ref{fig:LognormalSpectra}. When $v > 0$, since we look for a solution
in the limit $y \to 0$, we set $y = 0$ in \eqref{eq:lambda_linear}; in
\eqref{eq:constraint_linear} we first cancel the factor $y$ and then
set $y = 0$. This yields
\begin{eqnarray}
  \lambda &=& {1 \over (-q x+1)} + {2 v (q x+1) \over (-q x+1)^3} + {1
    \over x} \label{eq:lambda_1} \\
  0 &=& {q \over (-q x+1)^2} + {4 v q(q x+2) \over (-q x+1)^4} - {1
    \over x^2} \label{eq:constraint_1}
\end{eqnarray}

Evaluating \eqref{eq:lambda_1} at $x = \lambda^{MP}_{+} = (1 +
\sqrt{q})^2$ gives a rough estimate about the expected maximum
eigenvalue, respectively. That is
\begin{eqnarray}
  \lambda_{+} &=& \lambda_{+}^{MP} [1+2v(2\sqrt q +
  1)] \label{eq:lambda_3} \\
  \lambda_{-} &=& \lambda_{-}^{MP} [1-2v(2\sqrt q -
  1)] \label{eq:lambda_4}
\end{eqnarray}
This result is checked against simulations, as shown in figure
\ref{fig:lambda_verified}. Clearly one can see that the linear
approximation \eqref{eq:lambda_3} fits fairly well to the simulated
$\E \lambda_\M$.
\begin{figure}[htb!]
  \centering
  \subfigure[$q = 0.1$]{
    \includegraphics[scale=0.35]{../pics/lambda_max_q0.1.eps}
    \label{fig:lambda_max_q0.1}
  }
  \subfigure[$q = 0.2$]{
    \includegraphics[scale=0.35]{../pics/lambda_max.eps}
    \label{fig:lambda_max_q0.2}
  }
  \subfigure[$q = 0.5$]{
    \includegraphics[scale=0.35]{../pics/lambda_max_q0.5.eps}
    \label{fig:lambda_max_q0.5}
  }
  \subfigure[$q = 1.0$]{
    \includegraphics[scale=0.35]{../pics/lambda_max_q1.0.eps}
    \label{fig:lambda_max_q1.0}
  }
  \caption{\small \it Simulated $\E \lambda_\M$ against its linear
    approximation.}
  \label{fig:lambda_verified}
\end{figure}

The same comparison for the smallest eigenvalue is shown in figure
\ref{fig:lambda_min_verified}.
\begin{figure}[htb!]
  \centering
  \subfigure[$q = 0.1$]{
    \includegraphics[scale=0.35]{../pics/lambda_min_q0.1.eps}
    \label{fig:lambda_min_q0.1}
  }
  \subfigure[$q = 0.2$]{
    \includegraphics[scale=0.35]{../pics/lambda_min_q0.2.eps}
    \label{fig:lambda_min_q0.2}
  }
  \subfigure[$q = 0.5$]{
    \includegraphics[scale=0.35]{../pics/lambda_min_q0.5.eps}
    \label{fig:lambda_min_q0.5}
  }
  \subfigure[$q = 1.0$]{
    \includegraphics[scale=0.35]{../pics/lambda_min_q1.0.eps}
    \label{fig:lambda_min_q1.0}
  }
  \caption{\small \it Simulated $\E \lambda_\m$ against its linear
    approximation. Green: $\E \lambda_\m$ of simulated
    eigenvalues. Red: linear approximation.}
  \label{fig:lambda_min_verified}
\end{figure}

QQ-plots of the smallest/largest eigenvalues are shown in
figure \ref{fig:eigmin_qq_plot} and \ref{fig:eigmax_qq_plot}. It is
clearly seen that normal distributions are fairly good approximation
to the distribution of the smallest eigenvalue in all cases, and the
largest eigenvalue in all but the last two values of $v$, namely $v =
0.05$ and $v = 0.08$.
\begin{figure}[htb!]
  \centering
  \includegraphics[scale=0.6]{../pics/eigmin_dist.eps}
  \caption{\small \it QQ-plot of the smallest eigenvalues}
  \label{fig:eigmin_qq_plot}
\end{figure}

\begin{figure}[htb!]
  \centering
  \includegraphics[scale=0.6]{../pics/eigmax_dist.eps}
  \caption{\small \it QQ-plot of the largest eigenvalues}
  \label{fig:eigmax_qq_plot}
\end{figure}


\bibliographystyle{unsrt}
\bibliography{econophysics}
\end{document}
