\chapter{Correlation Matrix of Gaussian Returns}\label{chp:Gaussian}
In this chapter we present some analytical and numerical results about
the cross-correlation matrix (cf. \S
\ref{sec:FundamentalConcepts}), in the special case where the involved
asset returns are assumed to be Gaussian distributed. This assumption
is of course an over-simplification, but nevertheless lends some
insight into the problem.

Our primary interest is in the influence of autocorrelations on the
cross-correlation matrix. \S \ref{sec:GCC-analytical} discusses the
distribution of the matrix entries and \S \ref{sec:GCC-numerical} the
distribution of the eigenvalues.

\section{Distribution of the Matrix Elements}\label{sec:GCC-analytical}
If the returns follow a zero-mean Gaussian distribution, 
i.e. $\vec{r}_t \sim N(0, \Sigma)$, and are not
auto-correlated, the L\'evy index $\alpha$ as appears in equation
\ref{eq:cross-correlation} is 2, and $RR'$ is a Wishart matrix with
the following distribution law \cite{Anderson2003}:
\begin{equation}
  \label{eq:WishartPDF}
  f_{RR'}(X) = { |\det X|^{(T-N-1)/2} \exp\left(-{1 \over 2}\tr
      \Sigma^{-1}X \right)
    \over
    2^{NT/2}\pi^{N(N-1)/4}|\det \Sigma|^{T/2}
    \prod_{i=1}^N \Gamma\left[(n+1-i)/2\right]
  }
\end{equation}
where $\Sigma$ is the true covariance matrix of the returns.

When $\vec{r}_t$ are indeed auto-correlated, $RR'$ no longer has the
Wishart distribution. The joint distribution function of its elements
can be expressed in terms of the Wishart PDF and the
auto-correlations. We show this in appendix
\ref{app:pdf_gaussian1}. In the rest of this section, we derive an
approximate expression for the asymptotic distribution of these matrix
elements.

In the following we denote the diagonal elements as $C_{ii}$ and the
non-diagonal elements as $C_{ij}$ ($i \neq j$). We assume that
$\var(a_{i,t}) = \sigma^2$ for any $i$ and $t$, and $a_{i,t}$ are not
autocorrelated, i.e. $\text{corr}(a_{i,t}, a_{i, t'}) = 0$ for any
$i$, $t$ and $t'$. Then we can write
\begin{eqnarray*}
  r_{i,t} r_{j, t} &=& \left[
    \phi_1 r_{i, t-1} + a_{i, t}
  \right] \left[
    \phi_1 r_{j, t-1} + a_{j, t}
  \right] \\
  C_{ij} &=& {1 \over T}\sum_{t=1}^T r_{i,t} r_{j, t} \\
  &=& \phi_1^2 {1 \over T}\sum_{t=1}^Tr_{i,t-1} r_{j, t-1} +
  \phi_1{1 \over T}\sum_{t=1}^T\left(
    r_{i, t-1} a_{j, t} + r_{j, t-1} a_{i, t}
  \right) + {1 \over T}\sum_{t=1}^T a_{i,t} a_{j,t}
\end{eqnarray*}
We note that the two sums $\sum_{t=1}^T r_{i,t} r_{j, t}$ and
$\sum_{t=1}^T r_{i,t-1} r_{j, t-1}$ only differ by the first and the
last addend, which is negligible for sufficiently large T. Thus we
have
\begin{equation}\label{eq:Offdiag1}
  \begin{aligned}
    (1 - \phi_1^2)C_{ij} &\approx& \phi_1 {1 \over T} \sum_{t=1}^T
    \left(r_{i, t-1} a_{j, t} + r_{j, t-1} a_{i, t} \right)
    + {1 \over T}\sum_{t=1}^T a_{i,t} a_{j,t}
  \end{aligned}
\end{equation}
Now we write the AR(1) process $r_{i,t}$ as an infinite moving-average
process:
\begin{eqnarray*}
  r_{i, t} &=& \phi_1 r_{i, t-1} + a_{i,t} \\
  (1 - \phi_1 B) r_{i, t} &=& a_{i,t} \\
\end{eqnarray*}
where $B$ is the back-shift operator. Then it follows from the above
equation
\begin{eqnarray*}
  r_{i,t} &=& {1 \over 1 - \phi_1 B} a_{i,t} \\
  &=& \sum_{k=0}^\infty \phi_1^k B^k a_{i,t} \\
  &=& \sum_{k=0}^\infty \phi_1^k a_{i,t-k} \\
\end{eqnarray*}
where it is left implicit that $a_{i, t}$ with $t \leq 0$ is zero (in
words, this implies that the $r_{i,t}$ process is not affected at all
by events before $t = 1$).

Substituting this into eq.\ref{eq:Offdiag1} for $r_{i,t-1}$ yields
\begin{equation}
  \label{eq:Offdiag2}
  \begin{aligned}
  (1 - \phi_1^2)C_{ij} &\approx
  {1 \over T} \sum_{t=1}^T \sum_{k=0}^\infty\phi_1^{k+1} (a_{i, t-k-1}
  a_{j, t} + a_{j, t-k-1} a_{i, t})
  + {1 \over T} \sum_{t=1}^T a_{i,t} a_{j,t} \\
  &=
  {1 \over T} \sum_{t=1}^{T} \sum_{k=1}^{t-1}\phi_1^k (a_{i, t-k}
  a_{j, t} + a_{j, t-k} a_{i, t})
  + {1 \over T} \sum_{t=1}^{T} a_{i,t} a_{j,t}\\
  \end{aligned}
\end{equation}

Given two Gaussian random variables $x$ and $y$ with zero mean and
covariance matrix 
\begin{equation*}
  \Sigma =
  \begin{pmatrix}
    1 & \rho \\
    \rho & 1 \\
  \end{pmatrix}
\end{equation*}

The PDF of $xy$ can be found by considering $P(xy < z)$:
\begin{eqnarray*}
  P(xy < z) &=& \left(\int_0^\infty dx \int_0^{z/x} dy
    +\int_{-\infty}^0 dx \int_{z/x}^\infty dy \right)
    {1 \over 2\pi \sqrt{1 - \rho^2}} \\
    &&
    \exp\left[
      -{x^2 -2\rho xy + y^2
        \over
        2\sigma^2 (1 - \rho^2)} 
    \right] \\
  f(z; \sigma, \rho) &=& {d \over dz} P(xy < z)\\
  &=& {1 \over \pi \sqrt{1 - \rho^2}} \exp\left[
    {\rho z \over \sigma^2 (1 - \rho^2)}\right] K_0\left[
    {|z| \over \sigma^2 (1 - \rho^2)}
  \right]
\end{eqnarray*}
where $K_n(z')$ is the modified Bessel function of the second
kind. It is worth taking note that, when $\rho \neq 0$,
$f(z; \sigma, \rho)$ is not symmetric with respect to
$z$. As a result, if $\rho > 0$, the mean of $f(z;
\sigma, \rho)$ is positive, and vice versa.

Secondly, because $|\rho| < 1$ and
\begin{equation*}
  K_0(z) \sim \sqrt{\pi \over 2z} e^{-z}
\end{equation*}
as $z \to \infty$ \cite{Olver:2010:NHMF}, all the moments of
$f(z; \sigma, \rho)$ are finite, implying the
applicability of the Lyapunov central limit theorem provided that $T$
is large, which is very often the case and what we assume here.

With this in mind, we observe that $\phi_1$ only affects the first sum in
\ref{eq:Offdiag2}. If $a_{i,t-k}$ and $a_{j,t}$ are not correlated for
non-zero $k$, $\phi_1$ will not affect the mean of
$C_{ij}$. Furthermore, it is also clear from equation
\ref{eq:Offdiag2} that the variance of $C_{ij}$ is
always increased by a non-zero $\phi_1$, regardless of the sign of
$\phi_1$. We compute this increment in the following.

In light of the above expression for $f(z; \sigma,
\rho)$, we rewrite equation \ref{eq:Offdiag2} as
\begin{equation}\label{eq:Cij_dist}
  \begin{aligned}
    (1 - \phi_1^2)C_{ij} \approx &
    {1 \over T}
    \sum_{t=1}^{T} \sum_{k=1}^{t-1}\phi_1^k (a_{i, t-k} a_{j, t} + a_{j,
      t-k} a_{i, t}) \\
    & + {1 \over T} \sum_{t=1}^{T} \sigma^2 (1 - \rho^2) {a_{i,t} \over
      \sigma \sqrt{1 - \rho^2}} {a_{j,t} \over \sigma \sqrt{1 - \rho^2}} \\
  \end{aligned}
\end{equation}
In addition, we assume
\begin{equation*}
  \text{corr}(a_{i, t-k}, a_{j, t}) = \left\{
    \begin{array}{l l}
      1 & \text{ if $i=j$ and $k = 0$ }\\
      \rho & -1 < \rho < 1. \text{ if $i \neq j$ and $k = 0$ }\\
      0 & \text{otherwise}
    \end{array}
  \right.
\end{equation*}
Then, because $a_{i, t-k}$ and $ a_{j, t}$ with $i \neq j$ and $k
> 0$ are not correlated, the mean of $a_{i, t-k} a_{j, t}$ is 0
($f(z; \sigma, 0)$ is symmetric), and the variance of it
can be found to be $\sigma^6$ using formula (10.43.19) of \cite{NIST:DLMF}:
\begin{equation*}
  \int_0^\infty dt K_\nu(t) t^{\mu-1} = 2^{\mu-2}
  \Gamma\left(
    {\mu + \nu \over 2}
  \right) \Gamma\left(
    {\mu - \nu \over 2}
  \right)
\end{equation*}
On the other hand, $a_{i,t}/\sigma \sqrt{1 - \rho^2}$ and
$a_{j,t}/\sigma \sqrt{1 - \rho^2}$ have variance $1/(1 - \rho^2)$
and are correlated - $\text{corr}(a_{i,t}, a_{j,t}) = \rho$. The mean
of $a_{i,t}a_{j,t}/\sigma^2 (1 - \rho^2)$ can be found using formula
(10.43.22) of \cite{NIST:DLMF}, given that $-1 < \rho < 1$:
\begin{eqnarray*}
  \int_0^\infty t^{\mu - 1} e^{-at} K_\nu(t) dt &=& (\pi/2)^{1/2}
  \Gamma(\mu + \nu) \Gamma(\mu - \nu)(1 - a^2)^{-\mu/2 + 1/4} \times\\
  && P^{-\mu+1/2}_{\nu-1/2} (a)
\end{eqnarray*}
where $P^\mu_\nu(\cdot)$ is Ferrers function of the first
kind\footnote{ Ferrers function of the first kind is defined through
  the hypergeometric functon $F(a, b; c; z)$ \cite{NIST:DLMF}:
  \begin{equation}
    \label{eq:Ferrers_1st}
    \mathop{\mathsf{P}^{\mu}_{\nu}\/}\nolimits\!\left(x\right)=\left(\frac{1+x}{1-
        x}\right)^{\mu/2}\mathop{\mathbf{F}\/}\nolimits\!\left(\nu+1,-\nu;1-\mu;\tfrac
      {1}{2}-\tfrac{1}{2}x\right).
  \end{equation}
}. The result is
\begin{eqnarray*}
  \E\left[{a_{i,t}a_{j,t} \over \sigma^2 (1 - \rho^2)}\right]
  &=&
  {1 \over \sqrt{2\pi} (1 - \rho^2)^{5/4}} \left[
    P^{-3/2}_{-1/2}(-\rho) - P^{-3/2}_{-1/2}(\rho)
  \right]
\end{eqnarray*}
Similarly, the variance of $a_{i,t}a_{j,t}/\sigma^2 (1 - \rho^2)$ is
found to be
\begin{eqnarray*}
  v^2(\rho) &=&
  {4 \over \sqrt{2\pi} (1 - \rho^2)^{7/4}} \left[
    P^{-5/2}_{-1/2}(\rho) + P^{-5/2}_{-1/2}(-\rho)
  \right] \\
  && - \E^2\left[{a_{i,t}a_{j,t} \over
      \sigma^2 (1 - \rho^2)}\right]
\end{eqnarray*}
Now we can apply the Lyapunov central limit theorem
\cite{Billingsley1995} to the sum in equation \ref{eq:Cij_dist}
and write down the asymptotic Gaussian distribution of $C_{ij}$:
\begin{equation*}
C_{ij} \sim N(\mu'_X, \sigma'_X)
\end{equation*}
where
\begin{eqnarray}
  \mu'_X &=& {\sigma^2 \over \sqrt{2\pi} (1 - \phi_1^2)(1 -
    \rho^2)^{1/4}} \left[ P^{-3/2}_{-1/2}(-\rho) -
    P^{-3/2}_{-1/2}(\rho)
  \right] \label{eq:gaussian_mean}\\
  \sigma'^2_X &=& {1 \over (1 - \phi_1^2)^2}\left[
    \sum_{t=1}^T \sum_{k=1}^{t-1} 2\left(
      \phi_1^k \over T
    \right)^2 \sigma^6 + \sum_{t=1}^T
    {\sigma^4 (1 - \rho^2)^2 \over T^2} v^2(\rho)
  \right] \nonumber \\
  &=& {2 \sigma^6 \over T (1 - \phi_1^2)^2} \left[
    {\phi_1^2 \over 1 - \phi_1^2} -
    {\phi_1^2 (1 - \phi_1^{2T}) \over
      T(1 - \phi_1^2)}
  \right] + {\sigma^4 (1 - \rho^2)^2 v^2(\rho) \over
    T (1 - \phi_1^2)^2} \nonumber \\
  &\approx& {2 \sigma^6 \phi_1^2 \over T (1 - \phi_1^2)^3}
  + {\sigma^4 (1 - \rho^2)^2 v^2(\rho) \over
    T (1 - \phi_1^2)^2} \label{eq:gaussian_variance}
\end{eqnarray}
Equation \ref{eq:gaussian_mean} tells that, if two return series $i$
and $j$ are not correlated, auto-correlation in the returns does not
introduce a bias into the estimation of the cross-correlation; if,
however, the return series are indeed correlated, auto-correlation
in the returns rescales the cross-correlation through a multiplicative
factor $1/(1 - \phi_1^2)$.

In addition, equation \ref{eq:gaussian_variance} tells that
auto-correlation in the returns always makes the cross-correlation
estimation more noisy. Auto-correlation not only rescales the variance
of the no-autocorrelation estimation by $1/(1 - \phi_1^2)$ but even
adds an extra term ${2 \sigma^6 \phi_1^2 \over T (1 - \phi_1^2)^3}$.

% What we have obtained is a quantitative description of what has been
% said before: The mean of the non-diagonal entries of the
% cross-correlation matrix is not affected by autocorrelations in the
% returns, given that innovations $a_{i,t}$ of different assets and
% different times are un-correlated; the variance, however, is always
% increased by autocorrelations no matter the autocorrelation is
% positive or nagative. The increment is in the form of an additive term
% that scales as $\phi_1^2 / (1 - \phi_1^2)^3$.

We may apply a similar treatment to the diagonal elements of the
covariance matrix, which we denote as $C_{ii}$ here:
\begin{eqnarray*}
  C_{ii} &=& {1 \over T} \sum_{t=1}^T r^2_{it} \\
  &=& {1 \over T} \sum_{t=1}^T \sum_{l=0}^{t-1}\phi_1^l a_{i, t-l}
  \sum_{k=0}^{t-1}\phi_1^k a_{i, t-k} \\
  &=& {1 \over T} \sum_{t=1}^T \left[
    \sum_{k=0}^{t-1} \phi_1^{2k} a^2_{i, t-k} +
    \sum_{k,l = 0}^{t-1} \phi_1^{k+l} a_{i, t-k} a_{i, t-l}
  \right]
\end{eqnarray*}
By the Lyapunov central limit theorem under the assumption of large T,
the asymptotic distribution of $C_{ii}$ is Gaussian, the mean and
variance being
\begin{eqnarray}
  \E(C_{ii}) &=& {1 \over T}\left[
    \sum_{k=0}^{t-1} \phi_1^{2k} \sigma^2
  \right] \nonumber \\
  &=& {\sigma^2 \over (1 - \phi_1^2) T} \left[
    T - {\phi_1^2 (1 - \phi_1^{2T}) \over 1 - \phi_1^2}
  \right] \nonumber \\
  & \approx & {\sigma^2 \over 1 - \phi_1^2} \left[
    1 - {\phi_1^2 \over T}
  \right] \label{eq:gaussian_cii_mean}
\end{eqnarray}
and
\begin{eqnarray}
  \var(C_{ii}) &=& \sum_{t=1}^T \left[
    \sum_{k=0}^{t-1} {\phi_1^{4k} \sigma^4 \over T^2} 2 +
    \sum_{k,l=0}^{t-1} {\phi_1^{2(k+l)} \over T^2} \sigma^6
  \right] \nonumber \\
  &=& \sum_{t=1}^T \left[
    {2 \sigma^4 \over T^2} {1 - \phi_1^{4t} \over 1 - \phi_1^4} +
    {\sigma^6 \over T^2} \left(
      {1 - \phi_1^{2t} \over 1- \phi_1^2}
    \right)^2 \right] \nonumber \\
  &=& {2 \sigma^4 \over T (1 - \phi_1^4)} -
  {2 \sigma^4 \phi_1^4 (1 - \phi_1^{4T}) \over T^2(1 - \phi_1^4)^2} +
  \nonumber \\
  && {\sigma^6 \over T (1 -\phi_1^2)^2} -
  {2 \sigma^6 \phi_1^2 (1 - \phi_1^{2T}) \over T^2 (1 - \phi_1^2)^3} +
  {\sigma^6 \phi_1^4 (1 - \phi_1^{4T}) \over T^2 (1 - \phi_1^2)^2 (1 -
    \phi_1^4)} \nonumber \\
  &\approx& {2 \sigma^4 \over T (1 - \phi_1^4)} + {\sigma^6 \over T (1
    -\phi_1^2)^2} \label{eq:gaussian_cii_variance}
\end{eqnarray}
From equation \ref{eq:gaussian_cii_mean} we see that auto-correlation
in the returns increases the variance of the return series; and from
equation \ref{eq:gaussian_cii_variance} we see that the variance of
that variance estimation is also increased by
auto-correlations. Moreover, we note that $\var(C_{ii})$ scales with T
approximately as $1/T$, similar to the behavior of
$\var(C_{ij})$. This is to be compared with the case of GARCH returns
discussed in chapter \ref{chp:CrossCorrelationFat}.

% As for the diagonal elements of the cross-correlation matrix,
% i.e. $\var(r_{it})$, from the representation of $r_{it}$ as an
% inifinite moving average process, one can deduce immediately
% \begin{equation*}
%   \text{var}(r_{i,t}) \approx {\sigma^2 \over 1 - \phi^2}
% \end{equation*}
% as is mentioned earlier in equation \ref{eq:gaussian_Cii_variance}.

% However, to write $f_A(RMM'R')$ as a function of the eigen
% values of $RMM'R'$ is very involved and no theoretical results are
% known.

% In the more general case
% \begin{eqnarray*}
%   a_{i,t} &=& \left(
%     1 - \sum_{k=1}^p \phi_{i,k} B^k
%   \right) r_{i,t} \\
%   &=& \Phi_i(B)} r_{i,t
% \end{eqnarray*}
% where $B$ denotes the back shift operator acting on subscript $t$, we
% can write
% \begin{eqnarray*}
%   A &=&
%   \begin{pmatrix}
%     \Phi_1(B) & &\\
%     & \ddots & \\
%     && \Phi_N(B)
%   \end{pmatrix} R \\
%   &=& \Phi(B) R
% \end{eqnarray*}

% Following the same reasoning, $RR' \sim W(\Phi^{-1}(B)
% \Sigma \Phi'^{-1}(B), T)$. Substituting $\Phi^{-1}(B)
% \Sigma \Phi'^{-1}(B)$ for $\Sigma$ in the Wishart
% probability density function, we get
% \begin{eqnarray*}
%   && \tr\left\{\left[\Phi^{-1}(B)\Sigma \Phi'^{-1}(B)\right]^{-1}
%     RR'\right\}\\
%   &=& \tr \left\{\Phi'(B)\Sigma^{-1} \Phi(B) RR'\right\}\\
%   &=& \tr\left\{\Sigma^{-1}AA'\right\}\\
% \end{eqnarray*}
% As to $\det \Phi^{-1}(B)\Sigma \Phi'^{-1}(B)$, we note that $\Sigma$
% is constant over time, and thus a function of $B$ has no effect on
% it. Therefore $\det \Phi^{-1}(B)\Sigma \Phi'^{-1}(B) = \det \Sigma$.

\section{Distribution of the Eigenvalues}\label{sec:GCC-numerical}
For the Wishart matrix, theoretical results are available for the
eigenvalue distribution \cite{Chiani2012}. In the simplest case where
$\Sigma = I$, i.e. no cross-correlations are present, we have
\begin{eqnarray*}
  f_{\lambda}(x_1, \cdots, x_N) = K \prod_{i=1}^N e^{-x_i/2}
  x_i^{(T-N-1)/2} \prod_{i<j}^N (x_i - x_j)
\end{eqnarray*}
where the eigenvalues have been indexed in descending order: $x_1 \geq
x_2 \geq \cdots \geq x_N$. The normalization constant K is given by
\begin{equation*}
  K = {\pi^{N^2/2} \over 2^{NT/2}\Gamma_N(T/2) \Gamma_N(N/2)}
\end{equation*}
where the function $\Gamma_m(a)$ is defined as
\begin{equation*}
  \Gamma_m(a) = \pi^{m(m-1)/4} \prod_{k=1}^m \Gamma\left(a - {k-1
      \over 2}\right)
\end{equation*}
However, as detailed in the derivation leading to equation
\ref{eq:cross-corr-matrix-PDF}, the distribution of $RR'$ is not
Wishart when the columns of $R$ are correlated. Deriving the
eigenvalue distribution analytically in this case is difficult and
beyond the scope of this thesis. Instead we resort to numerical
methods.

As before we consider the AR(1) process (see \S
\ref{sec:FundamentalConcepts}):
\begin{equation*}
  \vec{r}_t = \phi \vec{r}_{t-1} + \vec{a}_{t}
\end{equation*}
where $\vec{a}_t \sim N(0, I)$, i.e. the elements of $\vec{a}_t$ are
independent Gaussian random variable with zero mean and unit
variance.

Now we investigate how the eigenvalue distribution depends on
$\phi$. Figure \ref{fig:GaussianMarkovSpectrumPDF} shows the results
of the simulation.
% \begin{wrapfigure}{r}{0.5\textwidth}
\begin{figure}[htb!]
  \begin{center}
    \includegraphics[scale=0.5, clip=true, trim=90 228 115
    226]{../pics/GaussianMarkovSpectrumPDF.pdf}
  \end{center}
  % \vspace{-10mm}
  \caption{\small \it
      Eigenvalue distribution with $\tau$ ranging from 0 to
      3. The 1st blue line, which is shown as stairs, is the
      theoretical eigenvalue distribution according to the
      Marcenko-Pastur law: $f(\lambda) = \sqrt{(\lambda_+/\lambda - 1)(1
        - \lambda/\lambda_-)}/2\pi q \sigma^2$, where $\lambda_{\pm} =
      \sigma^2(1 \pm \sqrt{q})^2$ In the simulation we have
      chosen $q = N/T = 50/1000 = 0.05$ and $\sigma=1$. For each value
      of $\tau$ we generate 2000 instances of $N \times T$ random matrix
      $R$, and compute C as $C=RR'/T$. Hence each curve in the figure is
      constructed from 2000 sets of eigenvalues.
    }
  \label{fig:GaussianMarkovSpectrumPDF}
\end{figure}
It is clear from the figure that the maximum eigenvalue moves
consistently to the right as the value of $\phi$ increases. The change
of the minimum eigenvalue remains unclear from the figure, so we plot
in \ref{fig:Gaussian_mineig} their mean for each value of
$\phi$. There one can see that the minimum eigenvalue actually
increases with $\phi$.
\begin{figure}[htb!]
  \centering
  \includegraphics[scale=0.5, clip=true, trim=100 226 116
  133]{../pics/Gaussian_mineig.pdf}
  \caption{\small \it The the minimum eigenvalue versus
    auto-correlation strength $\phi$. For each value of $\phi$ 2000
    random matrices are generated and their eigenvalues are
    calculated. The minimum eigenvalue of each random matrix is noted
    and the mean of the 2000 such minimum eigenvalues are plotted
    against the chosen value of $\phi$. 20 values of $\phi$ are
    included in the plot, ranging from 0 to 0.95 with step size 0.05.}
  \label{fig:Gaussian_mineig}
\end{figure}

K. Johnsson \cite{Johnsson2000} and I. Johnstone \cite{Johnstone2001}
showed that, at the {\it absence} of autocorrelations and in the
asymptotic limit $N, T \to \infty$, $N/T \to q < \infty$, the maximum
eigenvalue  $\lambda_1$ follows the Tracy-Widom distribution (denote
$\mathscr{TW}_1$ here) when properly relocated and rescaled:
\begin{equation*}
  {\lambda_1 - \mu_{NT} \over \sigma_{NT}} \sim \mathscr{TW}_1
\end{equation*}
where $\mu_{NT}$ and $\sigma_{NT}$ are given by
\begin{eqnarray*}
  \mu_{NT} &=& \left(
    \sqrt{N-1/2} + \sqrt{T - 1/2}
  \right)^2 \\
  \sigma_{NT} &=& \sqrt{\mu_{NT}} \left(
    {1 \over \sqrt{N-1/2}} + {1 \over \sqrt{T-1/2}}
  \right)^{1/3}
\end{eqnarray*}
So in the following we focus on the maximum eigenvalue distribution
and leave the minimum eigenvalue for future studies. The
$\mathscr{TW}_1$ distribution has the following cummulative
distribution function (CDF) \cite{Chiani2012}
\begin{equation*}
  F_1(x) = \exp\left[
    -{1 \over 2} \int_x^\infty dy \left(q(y) + (y-x)q^2(y)\right)
  \right]
\end{equation*}
where $q(y)$ is defined as the solution to the Painlev\'e II differential
equation
\begin{equation*}
  q''(y) = yq(y) + 2q^3(y)
\end{equation*}
which is unique when imposing the condition
\begin{equation*}
  q(y) \sim \text{Ai}(y) \text{ as } y \to \infty
\end{equation*}

Then Marco Chiani showed recently that the $\mathscr{TW}_1$ distribution
can be well approximated by a gamma distribution based on his proof
that the exact distribution of the maximum eigenvalue is a mixture of
gamma distributions. Specifically,
\begin{equation}\label{eq:TracyWidom-Gamma}
  {\lambda_1 - \mu_{NT} \over \sigma_{NT}} + \alpha \sim
  \mathscr{G}(k ,\theta)
\end{equation}
where $\mathscr{G}(k, \theta)$ denotes the Gamma distribution with
parameters $k$ and $\theta$ \cite{Chiani2012}. The probability
density function (PDF) of the gamma distribution is given by
\begin{equation*}
  f_\gamma(x; k, \theta) = {1 \over \Gamma(k) \theta^k} x^{k-1}
  e^{-x/\theta}
\end{equation*}
Moreover, the first 3 moments of the distribution are simple:
\begin{eqnarray*}
  \text{mean} &=& k\theta \\
  \text{variance} &=& k\theta^2 \\
  \text{skewness} &=& 2/\sqrt{k}
\end{eqnarray*}

In the following we first verify these results and extend them to the
case {\it with} autocorrelations and then study how the
distribution changes as $\phi$ increases. Figure
\ref{fig:GaussianMarkov005MaxEigCDF_loglog} compares the empirical
maximum eigenvalue CDF with the CDF of a gamma distribution. Each pair
of CDFs correspond to a fixed autocorrelation strength ($\phi$). The
parameters $k$ and $\theta$ of the gamma distribution are fit to
data by matching the 2nd and the 3rd moments of the gamma distribution
to the corresponding moments of the empirical distribution. Then the
parameter $\alpha$ in equation \ref{eq:TracyWidom-Gamma} is chosen to
be
\begin{equation*}
  \alpha = k\theta - \E\left({\lambda_1 - \mu_{NT} \over \sigma_{NT}}\right)
\end{equation*}

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.5, clip=true, trim=0 238 0
    197]{../pics/GaussianMarkov005MaxEigCDF_loglog.pdf}
  \end{center}
  \caption{\small \it Cummulative distribution function (CDF) of the maximum
    eigenvalue ($\lambda_1$). Blue: empirical CDF. Red: CDF of the
    fitting gamma distribution. The curves are plotted on log-log
    scale.}
  \label{fig:GaussianMarkov005MaxEigCDF_loglog}
  %\vspace{-10mm}
\end{figure}

We can see in figure \ref{fig:GaussianMarkov005MaxEigCDF_loglog} that
the gamma distribution fits fairly well to its corresponding maximum
eigenvalue distribution. So we conclude that a gamma distribution not
only approximates the maximum eigenvalue distribution at the absence of
autocorrelations but does so even at the {\it presence} of
autocorrelations. Since the maximum eigenvalue distribution is
characterized by the parmaters $k$, $\theta$, and  $\alpha$, the
influence of the autocorrelations can be characterized by the
dependence of $k$, $\theta$, and  $\alpha$ on $\phi$. In the following
we study these dependences in more details.

Figure \ref{fig:param1_phi_dep} shows how the mean, variance,
and skewness of the fitted gamma distribution, as well as the
relocation parameter $\alpha$ denpend on $\phi$.
\begin{figure}[htb!]
  \centering
    \includegraphics[scale=0.5, clip=true, trim=37 240 50
    202]{../pics/param1_phi_dep.pdf}
  \caption{\small \it  the  mean, variance, and skewness of the fitted
    gamma distribution as well as the relocation parameter $\alpha$
    are plotted against the autocorrelation strength $\phi$. 20 values
    of $\phi$ are included in the plot, ranging from 0 to 0.95 with
    step size 0.05.}
  \label{fig:param1_phi_dep}
\end{figure}
Despite the monotonic behavior shown by 3 of these curves, the
relation between each of the aforementioned quantities and $\phi$ is
more complicated than it apears. However, the mean, i.e. $k \theta$ is
an exception: Good support in the data can be found for the following
approximate relation:
\begin{eqnarray}
  k\theta &=& a \tan^2{\pi \phi \over 2} + b\tan{\pi \phi \over 2} +
  c \label{eq:k_theta-phi}
\end{eqnarray}
To verify this relation, we first fit a 2nd order polynomial and
obtain the coefficients $a$, $b$, $c$; then for each data point
$k_n\theta_n$ we solve the quadratic equation
\begin{eqnarray}
  a \tan^2{\pi \phi'_n \over 2} + b\tan{\pi \phi'_n \over 2} + c -
  k_n\theta_n &=& 0\label{eq:k_theta-phi_2}
\end{eqnarray}
for $\tan{\pi \phi'_n \over 2}$. If relation \ref{eq:k_theta-phi} is a
good approximation, a close match between $\tan{\pi \phi'_n \over 2}$
and $\tan{\pi \phi_n \over 2}$ is expected. Figure
\ref{fig:phi_and_roots} plots $\tan{\pi \phi'_n \over 2}$ against
$\tan{\pi \phi_n \over 2}$ and fits a straight line to the data
points. It is seen that the data points lie fairly close to the fitted
line and the line has a slope very close to 1 and an intercept close to
0. This strongly suggests $\tan{\pi \phi'_n \over 2} = \tan{\pi \phi_n
  \over 2}$ and supports the relation \ref{eq:k_theta-phi}.
\begin{figure}[htb!]
  \centering
    \includegraphics[scale=0.5, clip=true, trim=37 217 39
    170]{../pics/phi_and_roots.pdf}
  \caption{\small \it Upper plot: $\tan{\pi \phi'_n \over 2}$ against
    $\tan{\pi \phi_n \over 2}$. The fitted line has equation $y_n =
    0.995 \tan{\pi \phi_n \over 2} + 0.0146$. Lower plot: Residuals of
    the linear fit, i.e. $\tan{\pi \phi'_n \over 2} - y_n$. 20 values
    of $\phi$ are included in the plot, ranging from 0 to 0.95 with
    step size 0.05.}
  \label{fig:phi_and_roots}
\end{figure}

% Figure \ref{fig:GaussianMarkovMaxEig_k-phi} shows how the mean of the
% fitted gamma distribution, i.e. $k\theta$, varies as the
% autocorrelation strengthens ($\phi$ increases). 
% \begin{figure}[htb!]
%   %\vspace{-10mm}
%   \centering
%     \includegraphics[scale=0.5, clip=true, trim=100 223 112
%     141]{../pics/GaussianMarkov05MaxEig_k-phi.pdf}
%     \caption{\small \it The parameter $k$ against autocorrelation strength
%       $\phi$. Blue crosses: Empirical values of $k$. Red: best fitting
%       line in terms of {\it Least Square Errors}.}
%   \label{fig:GaussianMarkovMaxEig_k-phi}
% \end{figure}
% From the equation of the fitting line we can directly read out
% \begin{eqnarray*}
%   k &=& a\phi + b \\
%   &=& a\left(1 \over 2\right)^{1/\tau} + b
% \end{eqnarray*}
% where $a = -26$ and $b = 47$.

% For the parameter $\theta$, its behavior is more conveniently
% described in terms of the correlation time $\tau$. Figure
% \ref{fig:GaussianMarkovMaxEig_theta-tau} plots the values of $\theta$
% against those of $\tau$ together with a fitting quadratic
% function. Higher order polynomials provide a slightly better
% fit, but coefficients of the 3rd order and above are less than 1/1000
% times the coefficients of the 2nd and the 1st order. Therefore a 2nd
% order polynomial has been chosen.
% \begin{figure}[htb!]
%   \vspace{-15mm}
%   \centering
%   \includegraphics[scale=0.5, clip=true, trim=99 230 114
%   139]{../pics/GaussianMarkov05MaxEig_theta-tau.pdf} 
%   \caption{\small \it Vertical axis: $\theta$; Horizontal axis:
%     correlation time $\tau$. Blue: empirical values of $\theta$. Cyan:
%     Fitting quadratic function.}
%   \label{fig:GaussianMarkovMaxEig_theta-tau}
% \end{figure}
% The equation of this polynomial is
% \begin{eqnarray*}
%   \theta &=& A\tau^2 + B\tau + C
% \end{eqnarray*}
% where $A = 2.1\times 10^{-4}$, $B = -1.1\times 10^{-4}$, $C =
% 3.2\times 10^{-4}$.

% The parameter $\alpha$ shifts the gamma distribution $\mathscr{G}(k,
% \theta)$ to match the mean of ${(\lambda_1 -
%   \mu_{NT})/\sigma_{NT}}$. The behavior of $\alpha$ with respect to
% changing autocorrelation is shown in figure
% \ref{fig:GaussianMarkovMaxEig_alpha-tau}.
% \begin{figure}[htb!]
%   \vspace{-15mm}
%   \centering
%   \includegraphics[scale=0.5, clip=true, trim=104 229 114
%   139]{../pics/GaussianMarkovMaxEig_alpha-tau.pdf}
%   \caption{\small \it The mean-shift parameter $\alpha$ against
%     correlation time $\tau$. Blue: empirical values of $\alpha$. Cyan:
%     quadratic fit.}
%   \label{fig:GaussianMarkovMaxEig_alpha-tau}
% \end{figure}
% The equation of $\alpha$ is then inferred from the fitting line:
% \begin{equation*}
%   \alpha = D\tau^2 + E\tau + F
% \end{equation*}
% where $D = -3.4\times 10^{-3}$, $E = -4.6\times 10^{-2}$ and $F = 69$.

% So combining the results of $\alpha$, $k$ and $\theta$ we can express
% the moments of the Tracy-Widom variate ${(\lambda_1 -
%   \mu_{NT})/\sigma_{NT}}$:
% \begin{equation}\label{eq:lambda1_MeanVariance}
%   \begin{aligned}
%     \E({\lambda_1 - \mu_{NT} \over \sigma_{NT}}) &= k\theta -
%     \alpha \\
%     &= \left[-a\left(1 \over 2\right)^{1/\tau} + b\right](A\tau^2 +
%     B\tau - C) - (D\tau^2 + E\tau + F) \\
%     \var({\lambda_1 - \mu_{NT} \over \sigma_{NT}}) &=
%     k\theta^2 \\
%     &= \left[-a\left(1 \over 2\right)^{1/\tau} + b\right](A\tau^2 +
%     B\tau - C)^2 \\
%   \end{aligned}
% \end{equation}
% Figure \ref{fig:GaussianMarkovMaxEig_tw_moments} shows the empirical
% moments of ${(\lambda_1 - \mu_{NT})/\sigma_{NT}}$ together with the
% corresponding values computed using the above formulas.
% \begin{figure}[htb!]
%   \vspace{-15mm}
%   \centering
%   \includegraphics[scale=0.46, clip=true, trim=48 243 6
%   134]{../pics/GaussianMarkovMaxEig_tw_moments.pdf}
%   \caption{\small \it empirical moments of ${(\lambda_1 -
%       \mu_{NT})/\sigma_{NT}}$ against their theoretical
%     counterparts. Left: Empirical/theoretical mean against $\tau$;
%     Right: Empirical/theoretical variance against $\tau$. Blue
%     crosses: empirical values; Red line: fitting curves. Horizontal
%     axis: correlation time $\tau$.}
% \label{fig:GaussianMarkovMaxEig_tw_moments}
% \end{figure}
% The good fitness shown in figure
% \ref{fig:GaussianMarkovMaxEig_tw_moments} allows us to conclude 
% that, within the range of the correlation time that we have studied,
% namely $\tau \in [0, 13.51]$, the $k$ parameter is a linear function
% of $\phi = 2^{-1/\tau}$ while $\theta$ and $\alpha$ are quadratic
% functions of $\tau$. The moments of the transformed maximum
% eigenvalue, namely ${(\lambda_1 - \mu_{NT})/\sigma_{NT}}$, are thus
% expressed as functions of the correlation time $\tau$ via the
% parameters $k$, $\theta$ and $\alpha$.

% Figure \ref{fig:GaussianMarkovMaxEigPDF_original} shows the PDF of the
% maximum eigenvalue ($\lambda_1$) for a range of values of the
% correlation time $\tau$. One can clearly see that the mean of the
% distribution moves to the right and the width of the distribution
% increases as $\tau$ takes on larger and larger values. However, one
% must not forget that the behavior of the mean and the width is random
% in nature. Eq. \ref{eq:lambda1_MeanVariance} describes such behavior in
% the asymptotic limit, i.e. if an infinite number of random matrices
% are generated and their respective maximum eigenvalues computed for
% each and every value of correlation time $\tau$.
% \begin{figure}[htb!]
%   \begin{center}
%     \includegraphics[scale=0.5, clip=true, trim=98 228 112
%     221]{../pics/GaussianMarkovMaxEigPDF_original.pdf}
%     \caption{\small \it Probability density function of the maximum eigenvalue
%       ($\lambda_1$).}
%   \end{center}
%   \label{fig:GaussianMarkovMaxEigPDF_original}
% \end{figure}

