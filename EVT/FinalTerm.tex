\documentclass{report}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{enumerate}
% \usepackage{bookmark}

\input{../physics_common.tex}
\author{Xie Xiaolei}
\date{\today}
\title{Solutions to the Final Term Test}
\begin{document}
\maketitle

\begin{enumerate}[1.]
\item
  \begin{enumerate}[(a)]
  \item We consider the probability
    \begin{eqnarray*}
      && P(X_i < x) \\
      &=& P(\bigcap_{j=1}^k \{Y_{i+j} \le x\}) \\
    \end{eqnarray*}
    Since the $(Y_i)$ are iid with df. $F^{1/k}$, we have
    \begin{eqnarray*}
      && P(\bigcap_{j=1}^k \{Y_{i+j} \le x\}) \\
      &=& [F^{1/k}(x)]^k \\
      &=& F(x) \\
    \end{eqnarray*}
    The distribution function of $X_i$ is $F(x)$.

  \item 
    \begin{proof}
      We want to prove, $\forall n \ge 1, \forall h \ge 1$, $(X_1,
      \cdots, X_n) \ed (X_{1+h}, \cdots, X_{n+h})$.

      When $n = 1$, this is already proven in the solution to the last
      question since $X_i \sim F$, for any $i$.

      Now suppose $(X_1, \cdots, X_n) \ed (X_{1+h}, \cdots, X_{n+h})$
      for some $n \ge 1$ and $\forall h \ge 1$. In the following we
      prove 
      \[
      (X_1, \cdots, X_{n+1}) \ed (X_{1+h}, \cdots, X_{n+1+h})
      \]
      
      In fact we only need to prove the statement for $h=1$. Because
      $(Y_i)$ is an iid sequence, we can always remove the first element
      and relabel the sequence so that the old $Y_{i+1}$ is the new
      $Y_{i}$. Applying the statement for $h=1$ gives $(X_2, \cdots,
      X_{n+1}) \ed (X_{3}, \cdots, X_{n+2})$ with the old indices,
      thus establishing the statement for $h=2$. This procedure can be
      repeated, proving the statement for all values of $h$.

      In the case $h=1$, we need to prove
      \begin{eqnarray*}
        && P\left[(X_1 \le a_1, \cdots, X_{n+1} \le a_{n+1}) \le (a_1,
          \cdots, a_{n+1}) \right] \\
        &=& P\left[(X_2 \le a_1, \cdots, X_{n+2} \le a_{n+1}) \right]
      \end{eqnarray*}
      We may write
      \begin{eqnarray*}
        && P(X_1 \le a_1, \cdots, X_{n+1} \le a_{n+1}) \\
        &=& P(X_{n+1} \le a_{n+1} | X_1 \le a_1, \cdots, X_{n} \le
        a_{n}) \cdot \\
        && P(X_1 \le a_1, \cdots, X_{n} \le a_{n})
      \end{eqnarray*}
      \begin{eqnarray*}
        && P(X_{n+1} \le a_{n+1} | X_1 \le a_1, \cdots, X_{n} \le
        a_{n}) \\
        &=& P(Y_{n+2} \le a_{n+1}, \cdots, Y_{n+k+1} \le a_{n+1} | Y_2
        \le b_1, \cdots, Y_{n+k} \le b_{n+k-1})
      \end{eqnarray*}
      where $b_1, \cdots, b_{n+k-1}$ are constants depending on $a_1,
      \cdots, a_n$. Because $(Y_i)$ are iid, the above conditional
      probability can be factorized
      \begin{eqnarray*}
        && P(Y_{n+2} \le a_{n+1}, \cdots, Y_{n+k+1} \le a_{n+1} | Y_2
        \le b_1, \cdots, Y_{n+k} \le b_{n+k-1}) \\
        &=& P(Y_{n+2} \le a_{n+1} | Y_{n+2} \le b_{n+1}) \cdots
      P(Y_{n+k} \le a_{n+1} | Y_{n+k} \le b_{n+k-1}) \cdot \\
      && P(Y_{n+k+1} \le a_{n+1}) \\
      &=& {F^{1/k}(a_{n+1} \wedge b_{n+1}) \over F^{1/k}(b_{n+1})} \cdots
      {F^{1/k}(a_{n+1} \wedge b_{n+k-1}) \over F^{1/k}(b_{n+k-1})}
      F^{1/k}(a_{n+1})
    \end{eqnarray*}
    
    In comparison,
    \begin{eqnarray*}
      && P(X_2 \le a_1, \cdots, X_{n+2} \le a_{n+1}) \\
      &=& P(X_{n+2} \le a_{n+1} | X_2 \le a_1, \cdots, X_{n+1} \le
      a_{n}) \cdot \\
      && P(X_2 \le a_1, \cdots, X_{n+1} \le a_{n})
    \end{eqnarray*}
    By assumption, $(X_2, \cdots, X_{n+1}) \ed (X_1, \cdots, X_{n})$,
    meaning
    \[
    P(X_2 \le a_1, \cdots, X_{n+1} \le a_{n}) = P(X_1 \le
    a_1, \cdots, X_{n} \le a_{n})
    \]
    It remains to establish
    \begin{eqnarray*}
    && P(X_{n+2} \le a_{n+1} | X_2 \le a_1, \cdots, X_{n+1} \le
    a_{n}) \\
    &=& P(X_{n+1} \le a_{n+1} | X_1 \le a_1, \cdots, X_{n} \le
    a_{n})
    \end{eqnarray*}
    Analogous to the calculation before
    \begin{eqnarray*}
      && P(X_{n+2} \le a_{n+1} | X_2 \le a_1, \cdots, X_{n+1} \le
      a_{n})      \\
      &=& P(Y_{n+3} \le a_{n+1}, \cdots, Y_{n+k+2} \le a_{n+1} | Y_3
      \le b_1, \cdots, Y_{n+k+1} \le b_{n+k-1}) \\
      &=& P(Y_{n+3} \le a_{n+1} | Y_{n+3} \le b_{n+1}) \cdots
      P(Y_{n+k+1} \le a_{n+1} | Y_{n+k+1} \le b_{n+k-1}) \cdot \\
      && P(Y_{n+k+2} \le a_{n+1}) \\
      &=& {F^{1/k}(a_{n+1} \wedge b_{n+1}) \over F^{1/k}(b_{n+1})} \cdots
      {F^{1/k}(a_{n+1} \wedge b_{n+k-1}) \over F^{1/k}(b_{n+k-1})}
      F^{1/k}(a_{n+1}) \\
      &=& P(X_{n+2} \le a_{n+1} | X_2 \le a_1, \cdots, X_{n+1} \le
      a_{n})\\
    \end{eqnarray*}
  \end{proof}

  \item Notice that $M_n=\max(X_1, \cdots, X_n) = \max(Y_2, \cdots,
    Y_{n+k})$. Since $(Y_i)$ is an iid sequence
    \[
    P(M_n \le x) = F^{n+k-1 \over k}(x)
    \]
    Because
    \[
    \lim_{n \to \infty} n \bar{F}(u_n) = \tau
    \]
    by Poisson approximation,
    \begin{eqnarray*}
      \lim_{n \to \infty} F^n(u_n) &=& e^{-\tau} \\
      \lim_{n \to \infty} (F^n)^{\frac{n+k-1}{k} \frac{1}{n}} (u_n) &=&
      \lim_{n \to \infty} (e^{-\tau})^{\frac{n+k-1}{k} \frac{1}{n}} =
      e^{-\tau / k} \\
    \end{eqnarray*}
    That is
    \[
    \lim_{n \to \infty} P(M_n \le u_n) = \lim_{n \to \infty} F^{n+k-1 \over k}(u_n) = e^{-\tau / k}
    \]
  \item It is straight-forward to calculate
    \begin{eqnarray*}
      N_n &=& \E \sum_{i=1}^{n-1} I_{\{X_i = X_{i+1}\}} \\
      &=& \sum_{i=1}^{n-1} P(X_i = X_{i+1}) \\
    \end{eqnarray*}
    where
    \begin{eqnarray*}
      && P(X_i = X_{i+1}) \\
      &=& P[Y_{i+1} \ne \max(Y_{i+1}, \cdots,
      Y_{i+k}), Y_{i+k+1} \ne \max(Y_{i+2}, \cdots, Y_{i+k+1})]
    \end{eqnarray*}
    Because $Y_i$ is an iid sequence, the two events of the above
    joint probability are independent. Thus we have
    \begin{eqnarray*}
      P(X_i = X_{i+1}) &=& \left({k-1 \over k}\right)^2
    \end{eqnarray*}
    and
    \begin{eqnarray*}
      N_n &=& (n-1) \left({k-1 \over k}\right)^2 \\
      &\ge& {n-1 \over 4}
    \end{eqnarray*}
    The expected number of equal pairs is at least 1/4 of the number
    of pairs.
  \end{enumerate}

\item
  \begin{enumerate}[(a)]
  \item Proof:
    First of all, we show that, if
    \begin{eqnarray}
      \lim_{x \to \infty} \frac{\bar{F}(x - y)}{\bar{F}(x)} &=&
      e^{\gamma y} \label{eq:property}
    \end{eqnarray}
    holds for $y > 0$, it also holds for $y < 0$.
    
    Clearly
    \begin{eqnarray*}
      \lim_{x \to \infty} \frac{\bar{F}(x)}{\bar{F}(x - y)} &=&
      e^{-\gamma y}      
    \end{eqnarray*}
    Because $x - y \to \infty$ as $x \to \infty$, we may replace $x -
    y$ by $x'$ and write
    \begin{eqnarray*}
      \lim_{x \to \infty} \frac{\bar{F}(x' + y)}{\bar{F}(x')} &=&
      e^{-\gamma y}      
    \end{eqnarray*}
    Let $y' = -y < 0$, then we see \eqref{eq:property} also holds for
    $y < 0$.

    Now we assume \eqref{eq:property} holds for all $y \in
    R$. $e_F(u)$ can be easier evaluated by changing the variable of
    integration:
    \begin{eqnarray*}
      e_F(u) &=& \int_{u}^{\infty} \frac{\bar{F}(y)}{\bar{F}(u)} dy
    \end{eqnarray*}
    Let $y = \ln y' + u$. Then we have
    \begin{eqnarray*}
      e_F(u) &=& \int_{1}^{\infty} \frac{\bar{F}(\ln y' +
        u)}{\bar{F}(u)}{1 \over y'} dy' \\
    \end{eqnarray*}
    Applying \eqref{eq:property} to the integrand gives
    \begin{eqnarray*}
      \lim_{u \to \infty} e_F(u) &=& \lim_{u \to \infty}
      \int_{1}^{\infty} \frac{\bar{F}(\ln y' + u)}{\bar{F}(u)}{1 \over
        y'} dy' \\
      &=& \int_{1}^{\infty} e^{-\gamma \ln y'}{1 \over
        y'} dy' \\
      &=& \int_{1}^{\infty} {1 \over y'^{\gamma + 1}} dy' \\
    \end{eqnarray*}
    By Karamata's theorem we have
    \begin{eqnarray*}
      \lim_{u \to \infty} e_F(u) &=& - {1 \over \gamma} \left[{1 \over
          y'^\gamma}\right]_{y'=1}^{\infty} = {1 \over \gamma}\\
    \end{eqnarray*}
    % \begin{eqnarray*}
    %   e_F(u) &=& \int_{u}^{\infty} {\bar{F}(y) \over \bar{F}(u)} dy
    % \end{eqnarray*}
    % Let $y = \ln z + u$ and change the variable of integration to
    % $z$. We get
    % \begin{eqnarray*}
    %   e_F(u) &=& \int_{1}^{\infty} {\bar{F}(\ln z + ) \over \bar{F}(u)} dz
    % \end{eqnarray*}
    % Due to the assumed property we have
    % \begin{eqnarray*}
    %   \lim_{z \to \infty} {\bar{F}(u+z) \over \bar{F}(u)} &=&
    %   e^{-\gamma z}
    % \end{eqnarray*}
    % So
    % \begin{eqnarray*}
    %   \lim_{u \to \infty} e_F(u) &=& \int_{0}^{\infty} e^{-\gamma z}
    %   dz \\
    %   &=& {1 \over \gamma}
    % \end{eqnarray*}

  \item
    \begin{enumerate}[(i)]
    \item Since $\bar{F}$ is regularly varying, it is an
      subexponential function, implying
      \[
      \lim_{x \to \infty} {\bar{F}(x - y) \over \bar{F}(x)} = 1
      \]
      according to lemma 1.3.5 of EKM. This means $\gamma =
      0$. Then it follows $\lim_{u \to \infty} e(u) = \infty$.

    \item For $\text{Exp}(\lambda)$, $\bar{F}(x) = \lambda e^{-\lambda
        x}$, hence
      \begin{eqnarray*}
        \lim_{x \to \infty} {\bar{F}(x - y) \over \bar{F}(x)} &=&
        e^{-\lambda (x - y + x)} \\
        &=& e^{\lambda y}
      \end{eqnarray*}
      So it follows $\lim_{u \to \infty} e_F(u) = 1/\lambda$.

    \item When $F$ is standard normal,
      \begin{eqnarray*}
        \lim_{x \to \infty} {\bar{F}(x - y) \over \bar{F}(x)} &=&
        \lim_{x \to \infty} \frac{
          \int_{x-y}^{\infty} e^{z^2/2} {dz \over \sqrt{2\pi}}
        }{
          \int_{x}^{\infty} e^{z^2/2} {dz \over \sqrt{2\pi}}
        }
      \end{eqnarray*}
      Applying l'Hopital's rule, we have
      \begin{eqnarray*}
        \lim_{x \to \infty} {\bar{F}(x - y) \over \bar{F}(x)} &=&
        \lim_{x \to \infty} {e^{-(x-y)^2/2} \over e^{-x^2/2}} = \infty\\
      \end{eqnarray*}
      That is, $\gamma = \infty$ and $\lim_{u \to \infty} e_F(u) =
      0$.
    \end{enumerate}

  \item The Cauchy distribution has regularly varying tail with index
    -1, which we prove below, hence it belongs to
    $\text{MDA}(\Phi_{-1})$.
    \begin{proof}
      \begin{eqnarray*}
        \lim_{x \to \infty} {\bar{F}(ax) \over \bar{F}(x)} &=&
        \lim_{x \to \infty}
        {
          1/2 - {1 \over \pi} \arctan(ax)
          \over
          1/2 - {1 \over \pi} \arctan(x)
        }
      \end{eqnarray*}
      Using l'Hopital's rule, we have
      \begin{eqnarray*}
        \lim_{x \to \infty}
        {
          1/2 - {1 \over \pi} \arctan(ax)
          \over
          1/2 - {1 \over \pi} \arctan(x)
        } &=&
        \lim_{x \to \infty} {
          a(x^2 + 1)
          \over
          a^2 x^2 + 1
        } = {1 \over a}\\
      \end{eqnarray*}
    \end{proof}
  \end{enumerate}
\end{enumerate}
\end{document}