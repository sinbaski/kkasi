\title{Notes from Jeffrey Collamore's Paper}
\author{
  Xie Xiaolei
}
\date{\today}

\documentclass[12pt]{article}
\input{../physics_common.tex}
% \usepackage{amsmath}
% \usepackage{amsfonts}
% \usepackage{mathrsfs}
% \usepackage[bookmarks=true]{hyperref}
% \usepackage{bookmark}
% \usepackage{dsfont}
% \usepackage{enumerate}

% \newcommand{\E}{
%         \mathbb{E}
% }
% \newcommand{\p}{
%         \mathbb{P}
% }
% \newcommandf166{\I}[1]{
%         \mathbf 1_{#1}
% }
% \newcommand{\dom}{
%         \text{dom}
% }
% \newcommand{\inn}[1]{
%   \langle#1\rangle
% }

\begin{document}
\maketitle

\section{Borel-Cantelli Lemma}
\subsection{The 1st Borel-Cantelli lemma}
The first Borel-Cantelli lemma asserts that, given an infinite
sequence of events, $E_n$, $n = 1, 2, \dots$,
\[
\sum_{n=1}^\infty \P(E_n) < \infty
\]
implies
\[
\P(\bigcap_{n=1}^\infty \bigcup_{i=n}^\infty E_i) = 0
\]
$\bigcap_{n=1}^\infty \bigcup_{i=n}^\infty E_i$, which is sometimes
denoted $E_n\;i.o.$ is the event that infinitely many of $E_n$
happen.

\subsection{The 2nd Borel-Cantelli lemma}
This lemma states that if
\[
\sum_{n=1}^\infty \P(E_n) = \infty
\]
and the events $E_n$ are pairwise independent, then
\[
\P(\bigcap_{n=1}^\infty \bigcup_{i=n}^\infty E_i) = 1
\]

\section{Letac's Model E}
Define a recurrence sequence
\[
V_n = A_n \max(V_{n-1}, D_n) + B_n
\]
where $\forall n \geq 0$, each and every element of the sequence $A_n$
are iid. So are the sequences $B_n$ and $D_n$. Moreover,
\begin{eqnarray*}
  A_n &>& 1 \\
  B_n, D_n &\in& R
\end{eqnarray*}
Let
\[
V = \lim_{n \to \infty} V_n
\]
and require
\[
V  \overset{d}{=} A \max(V, D) + B
\]
Here $A \overset{d}{=} A_1$, $B \overset{d}{=} B_1$ and $D \overset{d}{=} D_1$.
\section{Aperiodic, irreducible Markov Chain \& Minorization}
Assume a markov chain on a general state space $\mathbb S, \mathcal S$
is aperiodic, irreducible with respect to its maximal irreducibility
measure $\varphi$, and in addition countably generated. Then it satisfies
a minorization: $\exists k \in \mathbb Z_+, \forall x \in \mathbb S
\forall E \in \mathcal S$

\begin{equation}\label{eq:M}
\exists \mathcal C \exists \nu \exists \delta > 0\text{: }
\delta \I{\mathcal C}(x) \nu(E) \leq P^k(x, E)
\end{equation}

where $P(x, E)$ is the transition kernel of the Markov chain, $\nu$
is a probability measure on $\mathbb S, \mathcal S$, and $\mathcal C$
is some set with $\varphi(\mathcal C) > 0$.

\subsection{Collamore's first lemma}
If a Markov chain $V_n$ satisfies the minorization condition, it has a
regeneration structure: There exists a sequence of random times $K_0,
K_1, \dots, K_i, \dots$ such that
\begin{enumerate}
\item the inter-regeneration times $K_1 - K_0, K_2 - K_1, \dots$ are iid.
\item the blocks $V_{K_i}, \dots, V_{K_{i+1}-1}$, $i=0, 1, 2, \dots$ are
independent of each other.

\item $\P(V_{K_i} \in E | \mathcal F_{K_i - 1}) = \nu(E)$, where $\mathcal
F_{K_i - 1}$ is the $\sigma$-field generated by $V_0, V_1, \dots, V_{K_i-1}$.
\end{enumerate}

{\bf Remark. } $K_i-1$ are the times of the sequence $(V_n, \eta_n)$, where
$\eta_n$ is a Bernoulli r.v. with $\P(\eta_n = 1) = \delta$, returning to
the set $\mathcal C \times \{1\}$. Equavalently, the chain $V_n$ regenerates
with probability $\delta$ upon each return to the set $\mathcal C$.

\subsection{Collamore's second lemma}
Assume that a Markov chain $V_n$ satisfies the minorization condition, and
\[
        V = \lim_{n \to \infty} V_n
\]
Then
\[
\P(V > u) = {\E N_u \over \E \tau}
\]
where
\[
        N_u = \sum_{i=0}^{\tau - 1} \I{V_i > u}
\]
and $\tau \overset{d}{=} \tau_i = K_i - K_{i-1}$.

\section{Collamore's Main Result}
{\bf Associate perpetuity sequence}
Define $Z^{(p)}_n$ as the backward recursion generated by
\[
Z^{(p)}_n = F^{(p)}_{Y_0} \circ F^{(p)}_{Y_1} \circ \cdots
\circ F^{(p)}_{Y_n}(0)
\]
where $Y_i = (\log A_i, B_i, D_i)$, and
\[
F^{(p)}_{Y_i}(x) = {x \over A_i} + {B_i \over A_i}
\]

In addition, define the conjugate sequence $Z^{(c)}_n$ as
the backward recursion generated by
\[
Z^{(c)}_n = F^{(c)}_{Y_0} \circ F^{(c)}_{Y_1} \circ \cdots
\circ F^{(c)}_{Y_n}(0)
\]
where
\[
F^{(c)}_{Y_i}(x) = {\min(\check D_i, x) \over A_i} + {B_i \over A_i}
\]
Here $\check D_i := -A_i D_i - B_i$.

Use the following notations
\begin{eqnarray*}
\lambda(\alpha) &=& \E A^\alpha \\
\Lambda(\alpha) &=& \log \E A^\alpha
\end{eqnarray*}
In other words, $\lambda(\alpha)$ denotes the cummulant generating
function of $\log A$, and $\Lambda(\alpha) = \log \lambda(\alpha)$.

{\bf Hypothesis}
\begin{enumerate}
\item The rv. $A$ is absolutely continuous with a nontrivial
      density in a neighbourhood of $R$.
\item $\exists \xi \in (0, \infty) \cap \text{dom}(\Lambda')$ such that
      $\Lambda(\xi) = 0$.
\item $\E|B|^\xi < \infty$ and $\E(A|D|)^\xi < \infty$.
\item $\P(A > 1, B > 0) > 0$ or $\P(A > 1, B \geq 0, D > 0) > 0$.
\end{enumerate}

{\bf Collamore's theorem}
Under the above hypothesis, Collamore's theorem states that, for
Letac's model E, the following holds true:
\[
\lim_{u \to \infty} u^\xi \P(V > u) = C
\]
where $C$ is a finite positive constant
\begin{eqnarray*}
  C &=& \lim_{n \to \infty} C_n \\
  C_n &=&     {1 \over \xi \lambda'(\xi) \E(\tau)}
    \E_\xi \left[
    \left(
    (Z^{(p)}_n - Z^{(c)}_n)^+
    \right)^\xi \I{\tau > n}
    \right]
\end{eqnarray*}
Here $\E_{\xi}$ is the expectation taken w.r.t the $\xi$ shifted
measure $\mu_\xi$:
\begin{eqnarray*}
  \mu_\xi(E) &=& \int_E e^{\xi x} d\mu(x, y, z)
\end{eqnarray*}
where $E \in \mathcal B(R^3)$ and $\mu(x, y, z) = \P(\log A \leq x, B
\leq y, D \leq z)$. In addition, $C - C_n = o(e^{\epsilon n})$ for
some $\epsilon > 0$. This $\epsilon$ is related to the tail
distribution of $\tau$. It is shown that $V_n$ is geometrically
recurrent, i.e. $\exists \gamma > 0$ such that
\[
\P(\tau > n) = o(e^{-\gamma n}) \text{, } n \to \infty
\]
Then $\epsilon = \gamma/\xi$.


\section{Additive Markov Chains}
Given a Markov chain $X_t$ in a general state space $(\mathds E,
\mathscr E)$, adjoin to it an additive component $S_t$,
which takes values in $R^d$ such that $(X_t, S_t) $ is a Markov chain
and
\begin{eqnarray*}
&& \P[(X_t, S_t) \in A \times (\Gamma + s) | (X_0, S_0) = (x, s)] \\
&=& \P[(X_t, S_t) \in A \times \Gamma | (X_0, S_0) = (x, 0)] \\
&=& P_t(x, A \times \Gamma)
\end{eqnarray*}
The Markov chain $(X_t, S_t)$ is called an additive Markov process and
$P_t(x, A \times \Gamma)$ its transition kernel.
When $t = n \in \{0, 1, 2, \dots\}$, $S_n$ can be represented in the
form
\[
S_n = S_0 + \sum_{i=1}^n \xi_i
\]
where $\xi_i \in \Gamma$.

{\bf The Recurrence Hypothesis}
Given an additive Markov chain $(X_t, S_t)$, several results have been
obtained under the recurrence hypothesis (condition {\bf R}): There
exists $m \in \mathds Z_+$, a probability measure $\nu$ on $\mathscr E
\times \mathscr R^d$ and constants $0 < a \leq b < \infty$ such that
\[
a \nu(A \times \Gamma) \leq P^m(x, A \times \Gamma) \leq b \nu(A
\times \Gamma)
\]
for all $x \in E$, $A \in \mathscr E$ and $\Gamma \in \mathscr
R^d$. Here $\mathscr E$ and $\mathscr R^d$ are the $\sigma$-fields
generated by $E$ and $R^d$, respectively.

\section{Renewal theorem of markov Chains}
{\bf Lemma 6.1 of Escoe, Ney and Nummelin 1985}
Assume that the lower bound of the recurrence condition holds, then
there exist random variables $0 \leq T_0 \leq T_1 \leq \dots$ such
that the following is true:
\begin{enumerate}[(i)]
\item $T_0, T_1 - T_0, T_2 - T_1, \dots$ are iid, and $\P(T_i -
  T_{i-1} > k) = O(\rho^k)$, $\P(T_0 > k) = O(\rho^k)$ for some $0 <
  \rho < 1$.
\item $(X_{T_i}, \dots, X_{T_{i+1} - 1}, \xi_{T_i}, \dots,
  \xi_{T_{i+1} - 1})$, $i=0, 1, 2, \dots$ are independent
\item $\P(X_{T_i} \in A, \xi_{T_i} \in \Gamma | \mathscr F_{T_i-1}) =
  \nu(A \times \Gamma)$, where $\mathscr F_{T_i-1}$ is the
  $\sigma$-field generated by $X_0, X_1, X_2, \dots, X_{T_i-1}, \xi_1,
  \xi_2, \dots, \xi_{T_i-1}$.
\end{enumerate}
Use the following notations:
\begin{eqnarray*}
  \tau_i &\overset{d}{=}& T_{i+1} - T_i \\
  Y_i &\overset{d}{=}& \sum_{l=T_i}^{T_{i+1}-1} \xi_l
\end{eqnarray*}
and, given $X_0 = x$
\begin{eqnarray*}
  Y_{0, x} &\overset{d}{=}& \xi_1 + \xi_2 + \cdots + \xi_{T_0}
\end{eqnarray*}
In addition
\begin{eqnarray*}
  |Y_i| &\overset{d}{=}& \sum_{l=T_i}^{T_{i+1}-1} |\xi_l|
\end{eqnarray*}
Let $\tau$ and $Y$ be typical of $\tau_i$ and $Y_i$
\begin{eqnarray*}
  \psi(\alpha, \zeta) &=& \E_{\nu} e^{\inn{\alpha, Y} - \zeta \tau} \\  
  N_n &=& \sup\{k \geq 0: T_k \leq n\} \\
  W_n &=& \sum_{i=T_{N_n}}^{T_{N_{n+1}} - 1} \xi_i
\end{eqnarray*}

{\bf Lemma 6.2 of Escoe, Ney and Nummelin 1985, Renewal theorem}


% Here $Z^{(p)}_n$ is the perpetuity sequence:
% \begin{eqnarray*}
%   Z^{(p)}_n &=& F_{Y_0}^{(p)} \circ F_{Y_1}^{(p)} \circ \cdots
%   F_{Y_n}^{(p)}(0) \\
%   F_{Y_i}^{(p)} (x) &=& {x + B_i \over A_i}
% \end{eqnarray*}
% and $Z^{(c)}_n$ is the conjugate sequence:
% \begin{eqnarray*}
%   Z^{(c)}_n &=& F_{Y_0}^{(c)} \circ F_{Y_1}^{(c)} \circ \cdots
%   F_{Y_n}^{(c)}(0) \\
%   F_{Y_i}^{(c)} (x) &=& {\min(x, \check D_i) + B_i \over A_i}
% \end{eqnarray*}

\section{The problem of Multivariate markov Chains}
Let $M_1, M_2, \dots$ be iid matrices. Let
\begin{eqnarray*}
  W_n &=& M_n \cdots M_1 \vec v  \\
  V_n &=& M_n V_{n-1} + R_n
\end{eqnarray*}
Ultimately we want to find out
\[
\P\left(
  \| V \| > u, {V \over |V|}  \in E
\right)
\]
But first of all we need to find out
\[
\P\left(
  \| W \| > u, {W \over |W|} \in E
\right)
\]
Note that
\[
X_n = {W_n \over \|W_n\|}
\]
is a Markov chain, and an additive Markov chain can be defined as
$(X_n, S_n)$, where
\begin{eqnarray*}
  S_n &=& \sum_{i=1}^n \xi_i = \log(\|W_n\|) \\
  \xi_i &=& \log(\|W_i\|) - \log(\|W_{i-1}\|)
\end{eqnarray*}
$(X_n, S_n)$ is a Markov additive process:
\begin{eqnarray*}
  && \P\left[
    (X_n, S_n) \in A \times (\Gamma + s) |
    (X_0, S_0) = (x_0, s_0)
  \right] \\
  &=& \P\left(
    X_n \in A, \log\|M_n \cdots M_1 v\| \in \Gamma + s_0 \left|
    \frac{v}{\|v\|} = x_0, \log \|v\|=s_0 \right.
  \right) \\
  &=& \P\left(
    X_n \in A, \log\|M_n \cdots M_1 \frac{v}{\|v\|}\| \in \Gamma \left|
    \frac{v}{\|v\|}=x_0, \log \|v\|=s_0 \right.
  \right) \\
  &=& \P\left(
    X_n \in A, \log\|M_n \cdots M_1 \frac{v}{\|v\|}\| \in \Gamma \left|
    \frac{v}{\|v\|}=x_0 \right.
  \right) \\
  &=& \P\left[
    (X_n, S_n) \in A \times \Gamma |
    (X_0, S_0) = (x_0, 0)
  \right]
\end{eqnarray*}
Then by theorem 5.1 of Iscoe, Ney and Nummelin \cite{NeyNummelin1985},
the MA process $(X_n, S_n)$ satisfy the large deviation principle,
i.e. for a closed set $F \subseteq (-\infty, 0)$ and an open set $G
\subseteq (-\infty, 0)$,
\begin{eqnarray*}
  \limsup_{n \to \infty} {1 \over n} P^n (x, S_n \in n F) &\leq&
  -\inf_{v \in F} \sup_{\alpha \in \mathbb R}\{v \alpha - \log
  \lambda(\alpha)\} \\
  \liminf_{n \to \infty} {1 \over n} P^n (x, S_n \in n G) &\geq&
  -\inf_{v \in G} \sup_{\alpha \in \mathbb R}\{v \alpha - \log
  \lambda(\alpha)\}
\end{eqnarray*}
\[
\]

\bibliographystyle{unsrt}
\bibliography{../thesis/econophysics}
\end{document}
